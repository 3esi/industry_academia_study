Domain,Knowledge-seeking vs. Eval,Nerd factor/zu spezifisch,Validation Nerd Factor,Distinguished,Bucket ID,Paper Name,Research Questions (max. 4),URL,Abstract
"AI and software engineering, Auto-coding",Knowledge-seeking,-1,1,FALSE,-,Towards Understanding Fairness and its Composition in Ensemble Machine Learning,"RQ1: What are the fairness measures of various ensemble techniques? 
• RQ2: How does fairness compose in ensemble models? 
• RQ3: Can ensemble-related hyperparameters be chosen to design fair ensemble models?",https://arxiv.org/abs/2212.04593,"['Machine Learning (ML) software has been widely adopted in modern society, with reported fairness implications for minority groups based on race, sex, age, etc. Many recent works have proposed methods to measure and mitigate algorithmic bias in ML models. The existing approaches focus on single classifier-based ML models. However, real-world ML models are often composed of multiple independent or dependent learners in an ensemble (e.g., Random Forest), where the fairness composes in a non-trivial way. How does fairness compose in ensembles? What are the fairness impacts of the learners on the ultimate fairness of the ensemble? Can fair learners result in an unfair ensemble? Furthermore, studies have shown that hyperparameters influence the fairness of ML models. Ensemble hyperparameters are more complex since they affect how learners are combined in different categories of ensembles. Understanding the impact of ensemble hyperparameters on fairness will help programmers design fair ensembles. Today, we do not understand these fully for different ensemble algorithms. In this paper, we comprehensively study popular real-world ensembles: bagging, boosting, stacking and voting. We have developed a benchmark of 168 ensemble models collected from Kaggle on four popular fairness datasets. We use existing fairness metrics to understand the composition of fairness. Our results show that ensembles can be designed to be fairer without using mitigation techniques. We also identify the interplay between fairness composition and data characteristics to guide fair ensemble design. Finally, our benchmark can be leveraged for further research on fair ensembles. To the best of our knowledge, this is one of the first and largest studies on fairness composition in ensembles yet presented in the literature.']"
"AI and software engineering, Auto-coding",Evaluation,-1 (NLL?),1,FALSE,-,An Empirical Study on Noisy Label Learning for Program Understanding,"RQ1:Howdodifferenttypes of synthetic label noises in program classification affect the performance of deep learning models when NLL is not introduced? • 
RQ2:Howdoexisting NLL approaches perform on different synthetic noises in program classification? 
• RQ3:HowdoNLLapproaches perform on program understanding tasks with real-world noises?",https://arxiv.org/abs/2307.08990,"['Recently, deep learning models have been widely applied in program understanding tasks, and these models achieve state-of-the-art results on many benchmark datasets. A major challenge of deep learning for program understanding is that the effectiveness of these approaches depends on the quality of their datasets, and these datasets often contain noisy data samples. A typical kind of noise in program understanding datasets is label noise, which means that the target outputs for some inputs are incorrect. Researchers have proposed various approaches to alleviate the negative impact of noisy labels, and formed a new research topic: noisy label learning (NLL). In this paper, we conduct an empirical study on the effectiveness of noisy label learning on deep learning for program understanding datasets. We evaluate various NLL approaches and deep learning models on three tasks: program classification, vulnerability detection, and code summarization. From the evaluation results, we come to the following findings: 1) small trained-from-scratch models are prone to label noises in program understanding, while large pre-trained models are highly robust against them. 2) NLL approaches significantly improve the program classification accuracies for small models on noisy training sets, but they only slightly benefit large pre-trained models in classification accuracies. 3) NLL can effectively detect synthetic noises in program understanding, but struggle in detecting real-world noises. We believe our findings can provide insights on the abilities of NLL in program understanding, and shed light on future works in tackling noises in software engineering datasets. We have released our code at this https URL .']"
"AI and software engineering, Auto-coding",Knowledge-seeking,0,0,FALSE,B1,Development in times of hype: How freelancers explore Generative AI?,RQ: What challenges do freelancers experience when developing solutions based on generative AI?,https://doi.org/10.1145/3597503.3639111,"['The rise of generative AI has led many companies to hire freelancers to harness its potential. However, this technology presents unique challenges to developers who have not previously engaged with it. Freelancers may find these challenges daunting due to the absence of organizational support and their reliance on positive client feedback. In a study involving 52 freelance developers, we identified multiple challenges associated with developing solutions based on generative AI. Freelancers often struggle with aspects they perceive as unique to generative AI such as unpredictability of its output, the occurrence of hallucinations, and the inconsistent effort required due to trial-and-error prompting cycles. Further, the limitations of specific frameworks, such as token limits and long response times, add to the complexity. Hype-related issues, such as inflated client expectations and a rapidly evolving technological ecosystem, further exacerbate the difficulties. To address these issues, we propose Software Engineering for Generative AI (SE4GenAI) and Hype-Induced Software Engineering (HypeSE) as areas where the software engineering community can provide effective guidance. This support is essential for freelancers working with generative AI and other emerging technologies.']"
"AI and software engineering, Auto-coding",Knowledge-seeking,0,0,FALSE,B1,Using an LLM to Help With Code Understanding,"RQ1: To what extent does GILT affect developers’ understanding, task completion time, and task completion rates when faced with unfamiliar code? 
• RQ2: How do developers interact with GILT, and to what extent does that differ between the participants? 
• RQ3: Howdodevelopers perceive the usefulness of GILT?",https://dl.acm.org/doi/abs/10.1145/3597503.3639187,"[""Understanding code is challenging, especially when working in new and complex development environments. Code comments and documentation can help, but are typically scarce or hard to navigate. Large language models (LLMs) are revolutionizing the process of writing code. Can they do the same for helping understand it? In this study, we provide a first investigation of an LLM-based conversational UI built directly in the IDE that is geared towards code understanding. Our IDE plugin queries OpenAI's GPT-3.5-turbo model with four high-level requestswithoutthe user having to write explicit prompts: to explain a highlighted section of code, provide details of API calls used in the code, explain key domain-specific terms, and provide usage examples for an API. The plugin also allows for open-ended prompts, which are automatically contextualized to the LLM with the program being edited. We evaluate this system in a user study with 32 participants, which confirms that using our plugin can aid task completion more than web search. We additionally provide a thorough analysis of the ways developers use, and perceive the usefulness of, our system, among others finding that the usage and benefits differ between students and professionals. We conclude that in-IDE prompt-less interaction with LLMs is a promising future direction for tool builders.""]"
"AI and software engineering, Auto-coding",Knowledge-seeking,-1,0,TRUE,B1,Can Machine Learning Pipelines Be Better Configured?,"• RQ1 (Common ML Library Combinations): What combinations of ML libraries do developers commonly use? To answer
RQ1, we collect 11,363 published ML pipelines and analyze their
library usage to identify common library combinations.
• RQ2 (Impacts of Different ML Library Version Combinations): Do different version combinations of ML libraries
affect pipelines’ performances? To answer RQ2, for each ML
pipeline, we generate a series of variants with different ML library
version combinations to inspect their performance inconsistencies. In particular, we define the generation rules of ML library
version combinations, to systematically explore their impacts on
pipelines’ performances.
• RQ3 (Root Causes of Performance Inconsistencies): What
are the root causes of pipelines’ performance inconsistencies when adopting different version combinations of ML
libraries? To answer RQ3, we consider the pipeline variants that
induce (1) significant performance inconsistencies, (2) crashes
and (3) NaN bugs as subjects, to analyze the corresponding root
causes and triggering conditions.",https://dl.acm.org/doi/abs/10.1145/3611643.3616352,"['A Machine Learning (ML) pipeline configures the workflow of a learning task using the APIs provided by ML libraries. However, a pipeline’s performance can vary significantly across different configurations of ML library versions. Misconfigured pipelines can result in inferior performance, such as poor execution time and memory usage, numeric errors and even crashes. A pipeline is subject to misconfiguration if it exhibits significantly inconsistent performance upon changes in the versions of its configured libraries or the combination of these libraries. We refer to such performance inconsistency as a pipeline configuration (PLC) issue.', 'There is no prior systematic study on the pervasiveness, impact and root causes of PLC issues. A systematic understanding of these issues helps configure effective ML pipelines and identify misconfigured ones. In this paper, we conduct the first empirical study of PLC issues. To better dig into the problem, we propose Piecer, an infrastructure that automatically generates a set of pipeline variants by varying different version combinations of ML libraries and compares their performance inconsistencies. We apply Piecer to the 3,380 pipelines that can be deployed out of the 11,363 ML pipelines collected from multiple ML competitions at Kaggle platform. The empirical study results show that 1,092 (32.3']"
"AI and software engineering, Auto-coding",Evaluation,0,0,FALSE,B2,Reusing Deep Neural Network Models through Model Re-engineering,"RQ1: How effective is our model re-engineering approach
in reusing trained models?
• RQ2: Does reusing a re-engineered model incur less overhead than reusing the original model?
• RQ3: Does reusing the re-engineered model mitigate the
defect inheritance?",https://ieeexplore.ieee.org/abstract/document/10172769,"[Training deep neural network (DNN) models, which has become an important task in today's software development, is often costly in terms of computational resources and time. With the inspiration of software reuse, building DNN models through reusing existing ones has gained increasing attention recently. Prior approaches to DNN model reuse have two main limitations: 1) reusing the entire model, while only a small part of the model's functionalities (labels) are required, would cause much overhead (e.g., computational and time costs for inference), and 2) model reuse would inherit the defects and weaknesses of the reused model, and hence put the new system under threats of security attack. To solve the above problem, we propose SeaM, a tool that re-engineers a trained DNN model to improve its reusability. Specifically, given a target problem and a trained model, SeaM utilizes a gradient-based search method to search for the model's weights that are relevant to the target problem. The re-engineered model that only retains the relevant weights is then reused to solve the target problem. Evaluation results on widely-used models show that the re-engineered models produced by SeaM only contain 10.11% weights of the original models, resulting 42.41% reduction in terms of inference time. For the target problem, the re-engineered models even outperform the original models in classification accuracy by 5.85%. Moreover, reusing the re-engineered models inherits an average of 57% fewer defects than reusing the entire model. We believe our approach to reducing reuse overhead and defect inheritance is one important step forward for practical model reuse.]"
"AI and software engineering, Auto-coding",Evaluation,0,0,FALSE,B2,Flexible and Optimal Dependency Management via Max-SMT,"RQ1: Can MAXNPM find better solutions than NPM when
given different optimization objectives?
RQ2: Do MAXNPM’s solutions pass existing test suites?
RQ3: Does MAXNPM successfully solve packages that NPM
solves?
RQ4: Does using MAXNPM substantially increase solving
time?",https://arxiv.org/abs/2203.13737,"[""Package managers such as NPM have become essential for software development. The NPM repository hosts over 2 million packages and serves over 43 billion downloads every week. Unfortunately, the NPM dependency solver has several shortcomings. 1) NPM is greedy and often fails to install the newest versions of dependencies; 2) NPM's algorithm leads to duplicated dependencies and bloated code, which is particularly bad for web applications that need to minimize code size; 3) NPM's vulnerability fixing algorithm is also greedy, and can even introduce new vulnerabilities; and 4) NPM's ability to duplicate dependencies can break stateful frameworks and requires a lot of care to workaround. Although existing tools try to address these problems they are either brittle, rely on post hoc changes to the dependency tree, do not guarantee optimality, or are not composable. We present PacSolve, a unifying framework and implementation for dependency solving which allows for customizable constraints and optimization goals. We use PacSolve to build MaxNPM, a complete, drop-in replacement for NPM, which empowers developers to combine multiple objectives when installing dependencies. We evaluate MaxNPM with a large sample of packages from the NPM ecosystem and show that it can: 1) reduce more vulnerabilities in dependencies than NPM's auditing tool in 33% of cases; 2) chooses newer dependencies than NPM in 14% of cases; and 3) chooses fewer dependencies than NPM in 21% of cases. All our code and data is open and available.""]"
"AI and software engineering, Auto-coding",Evaluation,0,0,TRUE,B2,Improving the Learning of Code Review Successive Tasks with Cross-Task Knowledge Distillation,"RQ1: Accuracy on code refinement Howdoesthe coderefinementmodel,guidedbythe qualityestimationmodel, perform compared to baseline models? 
• RQ2: Accuracy on comment generation How does the comment generation model, guided by the code refinement model (and the embedding alignment), perform compared to baseline models? 
• RQ3: Effect of the embedding alignment objective Considering that the inclusion of the embedding has an effect on the training cost, do we have similar results without considering the embedding? 
• RQ4: Training time of DISCOREV What is the additional time of the joint training compared to baseline models?",https://dl.acm.org/doi/abs/10.1145/3643775,"['Code review is a fundamental process in software development that plays a pivotal role in ensuring code quality and reducing the likelihood of errors and bugs. However, code review can be complex, subjective, and time-consuming.Quality estimation,comment generation, andcode refinementconstitute the three key tasks of this process, and their automation has traditionally been addressed separately in the literature using different approaches. In particular, recent efforts have focused on fine-tuning pre-trained language models to aid in code review tasks, with each task being considered in isolation. We believe that these tasks are interconnected, and their fine-tuning should consider this interconnection. In this paper, we introduce a novel deep-learning architecture, named DISCOREV, which employs cross-task knowledge distillation to address these tasks simultaneously. In our approach, we utilize a cascade of models to enhance bothcomment generationandcode refinementmodels. The fine-tuning of thecomment generationmodel is guided by thecode refinementmodel, while the fine-tuning of thecode refinementmodel is guided by thequality estimationmodel. We implement this guidance using two strategies: a feedback-based learning objective and an embedding alignment objective. We evaluate DISCOREV by comparing it to state-of-the-art methods based on independent training and fine-tuning. Our results show that our approach generates better review comments, as measured by theBLEUscore, as well as more accuratecode refinementaccording to theCodeBLEUscore.']"
Analytics,Knowledge-seeking,-1,1,FALSE,-,When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference,"RQ1:For different inputs, how many layers of the LCM are indispensable to yield correct predictions? 
RQ2:Is it advantageous to continue code completion after a wrong token has been generated?",https://arxiv.org/pdf/2401.09964.pdf,"[Leveraging recent advancements in large language models, modern neural code completion models have demonstrated the capability to generate highly accurate code suggestions. However, their massive size poses challenges in terms of computational costs and environmental impact, hindering their widespread adoption in practical scenarios. Dynamic inference emerges as a promising solution, as it allocates minimal computation during inference while maintaining the model’sperformance.Inthisresearch,weexploredynamicinference within the context of code completion. Initially, we conducted an empirical investigation on GPT-2, focusing on the inference capabilities of intermediate layers for code completion. We found that 54.4% of tokens can be accurately generated using just the first layer, signifying significant computational savings potential. Moreover, despite using all layers, the model still fails to predict 14.5% of tokens correctly, and the subsequent completions continued from them are rarely considered helpful, with only a 4.2% Acceptance Rate. These findings motivate our exploration of dynamic inference in code completion and inspire us to enhance it with a decisionmaking mechanism that stops the generation of incorrect code. We thus proposeanoveldynamicinferencemethodspecificallytailored for code completion models. This method aims not only to produce correct predictions with largely reduced computation but also to prevent incorrect predictions proactively. Our extensive evaluation shows that it can averagely skip 1.7 layers out of 16 layers in the models, leading to an 11.2% speedup with only a marginal 1.1% reduction in ROUGE-L.]"
Analytics,Evaluation,"-1 (CNN spezifisch; ich kann mir ungefährt vorstellen, was mit Modularisierung gemeint ist, aber nicht genug um eine RQ zu benatworten)",1,TRUE,-,Modularizing while Training: a New Paradigm for Modularizing DNN Models,"RQ1:HoweffectiveisMwTintrainingandmodularizingCNN models? 
•RQ2:HowefficientisMwTintrainingandmodularizingCNN models? 
•RQ3:HoweffectiveisMwTinreusingCNNmodules? 
•RQ4:Howdothemajorhyper-parametersinfluencetheperformanceofMwT?",https://arxiv.org/pdf/2306.09376.pdf,"[Deep neural network (DNN) models have become increasingly crucial components of intelligent software systems. However, training a DNN model is typically expensive in terms of both time and computational resources. To address this issue, recent research has focused on reusing existing DNNmodels-borrowingtheconceptof software reuse in software engineering. However, reusing an entire model could cause extra overhead or inherit the weaknesses from the undesired functionalities. Hence, existing work proposes to decompose an already trained model into modules, i.e., modularizingafter-training, to enable module reuse. Since the trained models are not built for modularization, modularizing-after-training may incur huge overhead and model accuracy loss. In this paper, we propose a novel approach that incorporates modularization into the modeltraining process, i.e., modularizing-while-training (MwT). We train a model to be structurally modular through two loss functions that optimize intra-module cohesion and inter-module coupling. We have implemented the proposed approach for modularizing Convolutional Neural Network (CNN) models. The evaluation results on representative models demonstrate that MwT outperforms the existing state-of-the-art modularizing-after-training approach. Specifically, the accuracy loss caused by MwT is only 1.13 percentage points, which is less than that of the existing approach. The kernel retention rate of the modules generated by MwT is only 14.58%, with a reduction of 74.31% over the existing approach. Furthermore, the total time cost required for training and modularizing is only 108 minutes, which is half the time required by the existing approach. Our work demonstrates that MwT is a new and more effective paradigm for realizing DNN model modularization, offering a fresh perspective on achieving model reuse.]"
Analytics,Knowledge-Seeking,0,0,FALSE,B5,The Smelly Eight: An Empirical Study on the Prevalence of Code Smells in Quantum Computing,"RQ1: How do practitioners perceive quantum-specific
code smells?
RQ2: What is the prevalence of quantum-specific code
smells in quantum programs?",https://ieeexplore.ieee.org/abstract/document/10172808,"[Quantum Computing (QC) is a fast-growing field that has enhanced the emergence of new programming languages and frameworks. Furthermore, the increased availability of computational resources has also contributed to an influx in the development of quantum programs. Given that classical and QC are significantly different due to the intrinsic nature of quantum programs, several aspects of QC (e.g., performance, bugs) have been investigated, and novel approaches have been proposed. However, from a purely quantum perspective, maintenance, one of the major steps in a software development life-cycle, has not been considered by researchers yet. In this paper, we fill this gap and investigate the prevalence of code smells in quantum programs as an indicator of maintenance issues. We defined eight quantum-specific smells and validated them through a survey with 35 quantum developers. Since no tool specifically aims to detect quantum smells, we developed a tool called QSmell that supports the proposed quantum-specific smells. Finally, we conducted an empirical investigation to analyze the prevalence of quantum-specific smells in 15 open-source quantum programs. Our results showed that 11 programs (73.33%) contain at least one smell and, on average, a program has three smells. Furthermore, the long circuit is the most prevalent smell present in 53.33% of the programs.]"
Analytics,Knowledge-seeking,0,0,FALSE,B5,CodeGen4Libs: A Two-Stage Approach for Library-Oriented Code Generation,"RQ1: How much do developers prefer specific third-party
libraries when coding?
RQ2: To what extent are developers familiar with the
contextual intricacies of third-party libraries when coding?
RQ3: How effective are current code generation models
at generating code for specific third-party libraries without
specific fine-tuning on library-related data?",https://ieeexplore.ieee.org/abstract/document/10298327,"[Automated code generation has been extensively studied in recent literature. In this work, we first survey 66 participants to motivate a more pragmatic code generation scenario, i.e., library-oriented code generation, where the generated code should implement the functionally of the natural language query with the given library. We then revisit existing learning-based code generation techniques and find they have limited effectiveness in such a library-oriented code generation scenario. To address this limitation, we propose a novel library-oriented code generation technique, CodeGen4Libs, which incorporates two stages: import generation and code generation. The import generation stage generates import statements for the natural language query with the given third-party libraries, while the code generation stage generates concrete code based on the generated imports and the query. To evaluate the effectiveness of our approach, we conduct extensive experiments on a dataset of 403,780 data items. Our results demonstrate that CodeGen4Libs outperforms baseline models in both import generation and code generation stages, achieving improvements of up to 97.4% on EM (Exact Match), 54.5% on BLEU, and 53.5% on Hit@All. Overall, our proposed CodeGen4Libs approach shows promising results in generating high-quality code with specific third-party libraries, which can improve the efficiency and effectiveness of software development.]"
Analytics,Evaluation,0,0,FALSE,B6,SkCoder: A Sketch-based Approach for Automatic Code Generation,"RQ1: How does SKCODER perform compared to SOTA
baselines? 
RQ2: What are the contributions of different modules in
our approach? 
RQ3: What is the better design choice of the sketcher?",https://arxiv.org/abs/2302.06144,"['Recently, deep learning techniques have shown great success in automatic code generation. Inspired by the code reuse, some researchers propose copy-based approaches that can copy the content from similar code snippets to obtain better performance. Practically, human developers recognize the content in the similar code that is relevant to their needs, which can be viewed as a code sketch. The sketch is further edited to the desired code. However, existing copy-based approaches ignore the code sketches and tend to repeat the similar code without necessary modifications, which leads to generating wrong results. In this paper, we propose a sketch-based code generation approach named SkCoder to mimic developers\' code reuse behavior. Given a natural language requirement, SkCoder retrieves a similar code snippet, extracts relevant parts as a code sketch, and edits the sketch into the desired code. Our motivations are that the extracted sketch provides a well-formed pattern for telling models ""how to write"". The post-editing further adds requirement-specific details to the sketch and outputs the complete code. We conduct experiments on two public datasets and a new dataset collected by this work. We compare our approach to 20 baselines using 5 widely used metrics. Experimental results show that (1) SkCoder can generate more correct programs, and outperforms the state-of-the-art - CodeT5-base by 30.30%, 35.39%, and 29.62% on three datasets. (2) Our approach is effective to multiple code generation models and improves them by up to 120.1% in Pass@1. (3) We investigate three plausible code sketches and discuss the importance of sketches. (4) We manually evaluate the generated code and prove the superiority of our SkCoder in three aspects.']"
Analytics,Evaluation,0,0,FALSE,B6,Studying and Understanding the Tradeoffs Between Generality and Reduction in Software Debloating,"RQ1:Howdotheapproaches considered compare in terms of reduction, c-generality, and their tradeoff? 
• RQ2:Howdotheapproaches considered compare in terms of reduction, r-generality, and their tradeoff? 
• RQ3: How do the approaches considered perform when an increasing amount of inputs is used for debloating? 
• RQ4:Howefficient are the approaches?",https://dl.acm.org/doi/abs/10.1145/3551349.3556970,"['Existing approaches for program debloating often use a usage profile, typically provided as a set of inputs, for identifying the features of a program to be preserved. Specifically, given a program and a set of inputs, these techniques produce a reduced program that behaves correctly for these inputs. Focusing only on reduction, however, would typically result in programs that are overfitted to the inputs used for debloating. For this reason, another important factor to consider in the context of debloating is generality, which measures the extent to which a debloated program behaves correctly also for inputs that were not in the initial usage profile. Unfortunately, most evaluations of existing debloating approaches only consider reduction, thus providing partial information on the effectiveness of these approaches. To address this limitation, we perform an empirical evaluation of the reduction and generality of 4 debloating techniques, 3 state-of-the-art ones, and a baseline, on a set of 25 programs and different sets of inputs for these programs. Our results show that these approaches can indeed produce programs that are overfitted to the inputs used and have low generality. Based on these results, we also propose two new augmentation approaches and evaluate their effectiveness. The results of this additional evaluation show that these two approaches can help improve program generality without significantly affecting size reduction. Finally, because different approaches have different strengths and weaknesses, we also provide guidelines to help users choose the most suitable approach based on their specific needs and context.']"
Analytics,Evaluation,0,0,TRUE,B6,Generative Type Inference for Python,"RQ1: How effective is TYPEGEN in type inference
compared with existing approaches?
• RQ2: How capable is TYPEGEN in language models with
different parameter sizes?
• RQ3: What are the impacts of different parts in the
prompt design of TYPEGEN?
• RQ4: What are the impacts of different examples in
TYPEGEN?",https://arxiv.org/abs/2307.09163,"['Python is a popular dynamic programming language, evidenced by its ranking as the second most commonly used language on GitHub. However, its dynamic type system can lead to potential type errors, leading researchers to explore automatic type inference approaches for Python programs. The rule-based type inference approaches can ensure the accuracy of predicted variable types, but they suffer from low coverage problems. Supervised type inference approaches, while feature-agnostic, require large, high-quality annotated datasets and are limited to pre-defined types. As zero-shot approaches, the cloze-style approaches reformulate the type inference problem into a fill-in-the-blank problem. However, their performance is limited. This paper introduces TypeGen, a few-shot generative type inference approach that incorporates static domain knowledge from static analysis. TypeGen creates chain-of-thought (COT) prompts by translating the type inference steps of static analysis into prompts based on the type dependency graphs (TDGs), enabling language models to learn from how static analysis infers types. By combining COT prompts with code slices and type hints, TypeGen constructs example prompts from human annotations. TypeGen only requires very few annotated examples to teach language models to generate similar COT prompts via in-context learning. Moreover, TypeGen enhances the interpretability of results through the use of the input-explanation-output strategy. Experiments show that TypeGen outperforms the best baseline Type4Py by 10.0% for argument type prediction and 22.5% in return value type prediction in terms of top-1 Exact Match by using only five examples. Furthermore, TypeGen achieves substantial improvements of 27% to 84% compared to the zero-shot performance of large language models with parameter sizes ranging from 1.3B to 175B in terms of top-1 Exact Match.']"
Dependability and Security,Knowledge-seeking,-1 (SAST Hintergrundwissen notwendig?),1,TRUE,-,Static Application Security Testing (SAST) Tools for Smart Contracts: How Far Are We?,"RQ1:CoverageAnalysis. To what extent do existing SAST tools support different vulnerability types?
RQ2: Effectiveness Analysis. How effective are these SAST tools in detecting vulnerabilities on our benchmark?
RQ3: Consistency Analysis. This research question focuses on two consistency analyses: 1) Are the detection results consistent among these tools in terms of the detected vulnerability categories? 2) How e ective are these SAST tools when combining their detecting results?
RQ4: E iciency Analysis. How e cient are these SAST tools to perform an analysis?",https://arxiv.org/abs/2404.18186,"['In recent years, the importance of smart contract security has been heightened by the increasing number of attacks against them. To address this issue, a multitude of static application security testing (SAST) tools have been proposed for detecting vulnerabilities in smart contracts. However, objectively comparing these tools to determine their effectiveness remains challenging. Existing studies often fall short due to the taxonomies and benchmarks only covering a coarse and potentially outdated set of vulnerability types, which leads to evaluations that are not entirely comprehensive and may display bias. In this paper, we fill this gap by proposing an up-to-date and fine-grained taxonomy that includes 45 unique vulnerability types for smart contracts. Taking it as a baseline, we develop an extensive benchmark that covers 40 distinct types and includes a diverse range of code characteristics, vulnerability patterns, and application scenarios. Based on them, we evaluated 8 SAST tools using this benchmark, which comprises 788 smart contract files and 10,394 vulnerabilities. Our results reveal that the existing SAST tools fail to detect around 50% of vulnerabilities in our benchmark and suffer from high false positives, with precision not surpassing 10%. We also discover that by combining the results of multiple tools, the false negative rate can be reduced effectively, at the expense of flagging 36.77 percentage points more functions. Nevertheless, many vulnerabilities, especially those beyond Access Control and Reentrancy vulnerabilities, remain undetected. We finally highlight the valuable insights from our study, hoping to provide guidance on tool development, enhancement, evaluation, and selection for developers, researchers, and practitioners.']"
Dependability and Security,Evaluation,0,1,TRUE,-,Lejacon: A Lightweight and Efficient Approach to Java Confidential Computing on SGX,"RQ1 Can Lejacon support Java confidential computing
effectively?
• RQ2 Can Java applications achieve competitive performance on Lejacon, compared with some state-of-the-art
runtime system(s)?
• RQ3 Can Lejacon be practically used in running Java
confidential computing tasks?",https://ieeexplore.ieee.org/abstract/document/10172889,"[Intel's SGX is a confidential computing technique. It allows key functionalities of C/C++/native applications to be confidentially executed in hardware enclaves. However, numerous cloud applications are written in Java. For supporting their confidential computing, state-of-the-art approaches deploy Java Virtual Machines (JVMs) in enclaves and perform confidential computing on JVMs. Meanwhile, these JVM-in-enclave solutions still suffer from serious limitations, such as heavy overheads of running JVMs in enclaves, large attack surfaces, and deep computation stacks. To mitigate the above limitations, we for-malize a Secure Closed-World (SCW) principle and then propose Lejacon, a lightweight and efficient approach to Java confidential computing. The key idea is, given a Java application, to (1) separately compile its confidential computing tasks into a bundle of Native Confidential Computing (NCC) services; (2) run the NCC services in enclaves on the Trusted Execution Environment (TEE) side, and meanwhile run the non-confidential code on a JVM on the Rich Execution Environment (REE) side. The two sides interact with each other, protecting confidential computing tasks and as well keeping the Trusted Computing Base (TCB) size small. We implement Lejacon and evaluate it against OcclumJ (a state-of-the-art JVM-in-enclave solution) on a set of benchmarks using the BouncyCastle cryptography library. The evaluation results clearly show the strengths of Lejacon: it achieves compet-itive performance in running Java confidential code in enclaves; compared with OcclumJ, Lejacon achieves speedups by up to 16.2x in running confidential code and also reduces the TCB sizes by 90+% on average.]"
Dependability and Security,Knowledge-seeking,0,0,FALSE,B13,Measuring Secure Coding Practice and Culture: A Finger Pointing at the Moon is not the Moon,"Research Question 1: What are the secure-coding characteristics of our sample group? 
Research Question 2: What are the security culture characteristics of our sample group? 
Research Question 3: Do secure coding practice and culture correlate, and if not, what lessons can we learn to help support the development of secure coding?",https://ieeexplore.ieee.org/abstract/document/10172883,"[Software security research has a core problem: it is impossible to prove the security of complex software. A low number of known defects may simply indicate that the software has not been attacked yet, or that successful attacks have not been detected. A high defect count may be the result of white-hat hacker targeting, or of a successful bug bounty program which prevented insecurities from persisting in the wild. This makes it difficult to measure the security of non-trivial software. Researchers instead usually measure effort directed towards ensuring software security. However, different researchers use their own tailored measures, usually devised from industry secure coding guidelines. Not only is there no agreed way to measure effort, there is also no agreement on what effort entails. Qualitative studies emphasise the importance of security culture in an organisation. Where software security practices are introduced solely to ensure compliance with legislative or industry standards, a box-ticking attitude to security may result. The security culture may be weak or non-existent, making it likely that precautions not explicitly mentioned in the standards will be missed. Thus, researchers need both a way to assess software security practice and a way to measure software security culture. To assess security practice, we converted the empirically-established 12 most common software security activities into questions. To assess security culture, we devised a number of questions grounded in prior literature. We ran a secure development survey with both sets of questions, obtaining organic responses from 1,100 software coders in 59 countries. We used proven common activities to assess security practice, and made a first attempt to quantitatively assess aspects of security culture in the broad developer population. Our results show that some coders still work in environments where there is little to no attempt to ensure code security. Security practice and culture do not always correlate, and some organisations with strong secure coding practice have weak secure coding culture. This may lead to problems in defect prevention and sustained software security effort.]"
Dependability and Security,Knowledge-seeking,0,0,FALSE,B13,An Empirical Study of Deep Learning Models for Vulnerability Detection,"RQ1 Do models agree on the vulnerability detection results? What are the variabilities across different runs of a model and across different models? 
• RQ2 Are certain types of vulnerabilities easier to detect? Should we build models for each type of vulnerabilities or should we build one model that can detect all the vulnerabilities? 
• RQ3 Are programs with certain code features harder to be predicted correctly by current models, and if so, what are those code features?
• RQ4 Can increasing the dataset size help improve the
model performance for vulnerability detection?",https://doi.org/10.48550/arXiv.2212.08109,"['Deep learning (DL) models of code have recently reported great progress for vulnerability detection. In some cases, DL-based models have outperformed static analysis tools. Although many great models have been proposed, we do not yet have a good understanding of these models. This limits the further advancement of model robustness, debugging, and deployment for the vulnerability detection. In this paper, we surveyed and reproduced 9 state-of-the-art (SOTA) deep learning models on 2 widely used vulnerability detection datasets: Devign and MSR. We investigated 6 research questions in three areas, namely model capabilities, training data, and model interpretation. We experimentally demonstrated the variability between different runs of a model and the low agreement among different models\' outputs. We investigated models trained for specific types of vulnerabilities compared to a model that is trained on all the vulnerabilities at once. We explored the types of programs DL may consider ""hard"" to handle. We investigated the relations of training data sizes and training data composition with model performance. Finally, we studied model interpretations and analyzed important features that the models used to make predictions. We believe that our findings can help better understand model results, provide guidance on preparing training data, and improve the robustness of the models. All of our datasets, code, and results are available at this https URL .']"
Dependability and Security,Knowledge-seeking,0,0,FALSE,B13,What Challenges Do Developers Face About Checked-in Secrets in Software Artifacts?,"• RQ1: What are the technical challenges faced by developers related to checked-in secrets?
RQ1.1 What are the questions developers ask about
checked-in secrets?
RQ1.2 Which questions related to checked-in secrets
exhibit more unsatisfactory answers?
• RQ1.3 Which questions are the most
• RQ2: What solutions do developers get for mitigating
checked-in secrets?
",https://arxiv.org/pdf/2301.12377.pdf,"[Throughout 2021, GitGuardian’s monitoring of public GitHub repositories revealed a two-fold increase in the number of secrets (database credentials, API keys, and other credentials) exposed compared to 2020, accumulating more than six million secrets. To our knowledge, the challenges developers face to avoid checked-in secrets are not yet characterized. The goal of our paper is to aid researchers and tool developers in understanding and prioritizing opportunities for future research and tool automation for mitigating checked-in secrets through an empirical investigation of challenges and solutions related to checked-in secrets. We extract 779 questions related to checkedin secrets on Stack Exchange and apply qualitative analysis to determine the challenges and the solutions posed by others for each of the challenges. We identify 27 challenges and 13 solutions. The four most common challenges, in ranked order, are: (i) store/version of secrets during deployment; (ii) store/version of secrets in source code; (iii) ignore/hide of secrets in source code; and (iv) sanitize VCS history. The three most common solutions, in ranked order, are: (i) move secrets out of source code/version control and use template config file; (ii) secret management in deployment; and (iii) use local environment variables. Our findings indicate that the same solution has been mentioned to mitigate multiple challenges. However, our findings also identify an increasing trend in questions lacking accepted solutions substantiating the need for future research and tool automation on managing secrets.]"
Dependability and Security,Evaluation,-1,0,FALSE,B14,Silent Vulnerable Dependency Alert Prediction with Vulnerability Key Aspect Explanation,"• RQ1: How is the accuracy of silent dependency alert classification?
• RQ2: How is the accuracy of explainable vulnerability key
aspect generation?
• RQ3: How useful is our explainable silent dependency alert
prediction compared with only binary patch classification?",https://ieeexplore.ieee.org/abstract/document/10172824,"[Due to convenience, open-source software is widely used. For beneficial reasons, open-source maintainers often fix the vulnerabilities silently, exposing their users unaware of the updates to threats. Previous works all focus on black-box binary detection of the silent dependency alerts that suffer from high false-positive rates. Open-source software users need to analyze and explain AI prediction themselves. Explainable AI becomes remarkable as a complementary of black-box AI models, providing details in various forms to explain AI decisions. Noticing there is still no technique that can discover silent dependency alert on time, in this work, we propose a framework using an encoder-decoder model with a binary detector to provide explainable silent dependency alert prediction. Our model generates 4 types of vulnerability key aspects including vulnerability type, root cause, attack vector, and impact to enhance the trustworthiness and users' acceptance to alert prediction. By experiments with several models and inputs, we confirm CodeBERT with both commit messages and code changes achieves the best results. Our user study shows that explainable alert predictions can help users find silent dependency alert more easily than black-box predictions. To the best of our knowledge, this is the first research work on the application of Explainable AI in silent dependency alert prediction, which opens the door of the related domains.]"
Dependability and Security,Evaluation,-1 (detaillierteres Wissen zu Web APIs notwendig?),0,FALSE,B14,Automated Black-box Testing of Mass Assignment Vulnerabilities in RESTful APIs,"RQ1: What is the accuracy of the automated identification of operations CRUD semantics, resource types, and resource-id parameters? Second, the main objective of our approach is to reveal mass assignment vulnerabilities in REST APIs, this is investigated by the following research question.             
RQ2: What is the accuracy in revealing mass assignment vulnerabilities in REST APIs? Finally, our approach should be able to deal with real-world production-ready REST services, that can be complex and large in size. Hence, the last research question investigates the scalability of the approach on large REST APIs. 
RQ3: Does the proposed approach to detect mass assignment vulnerabilities scale to large REST APIs?",https://arxiv.org/abs/2301.01261,"['Mass assignment is one of the most prominent vulnerabilities in RESTful APIs. This vulnerability originates from a misconfiguration in common web frameworks, such that naming convention and automatic binding can be exploited by an attacker to craft malicious requests writing confidential resources and (massively) overriding data, that should be read-only and/or confidential. In this paper, we adopt a black-box testing perspective to automatically detect mass assignment vulnerabilities in RESTful APIs. Execution scenarios are generated purely based on the OpenAPI specification, that lists the available operations and their message format. Clustering is used to group similar operations and reveal read-only fields, the latter are candidate for mass assignment. Then, interaction sequences are automatically generated by instantiating abstract testing templates, trying to exploit the potential vulnerabilities. Finally, test cases are run, and their execution is assessed by a specific oracle, in order to reveal whether the vulnerability could be successfully exploited. The proposed novel approach has been implemented and evaluated on a set of case studies written in different programming languages. The evaluation highlights that the approach is quite effective in detecting seeded vulnerabilities, with a remarkably high accuracy.']"
Dependability and Security,Evaluation,-1,0,TRUE,B14,Compatible Remediation on Vulnerabilities from Third-Party Libraries for Java Projects,"RQ1: How is CORAL compared with other cutting-edge
remediation tools regarding security and compatibility?
RQ2: How effectively does CORAL resolve the challenge of
global optimization by subgraph partitioning?
RQ3: How many vulnerabilities CAN/CANNOT be fixed
without breaking the projects in the Maven ecosystem?",https://ieeexplore.ieee.org/abstract/document/10172542,"[With the increasing disclosure of vulnerabilities in open-source software, software composition analysis (SCA) has been widely applied to reveal third-party libraries and the associated vulnerabilities in software projects. Beyond the revelation, SCA tools adopt various remediation strategies to fix vulnerabilities, the quality of which varies substantially. However, ineffective remediation could induce side effects, such as compi-lation failures, which impede acceptance by users. According to our studies, existing SCA tools could not correctly handle the concerns of users regarding the compatibility of remediated projects. To this end, we propose Compatible Remediation of Third-party libraries (CORAL) for Maven projects to fix vulnerabilities without breaking the projects. The evaluation proved that Coralnot only fixed 87.56% of vulnerabilities which outperformed other tools (best 75.32%) and achieved a 98.67% successful compilation rate and a 92.96% successful unit test rate. Furthermore, we found that 78.45% of vulnerabilities in popular Maven projects could be fixed without breaking the compilation, and the rest of the vulnerabilities (21.55%) could either be fixed by upgrades that break the compilations or even be impossible to fix by upgrading.]"
Evolution,Evaluation,"-1 (SMT Selection?, Vorwissen zu GNNs notwendig?)",-1,TRUE,-,Sibyl: Improving Software Engineering Tools with SMT Selection,"RQ1: For each domain, what portion of queries is the overall
fastest SMT solver the fastest? Depending on the domain, the
overall fastest solver is the fastest on 2.6% to 38.9% of the
queries.
RQ2: How does Sibyl’s predictions compare to other algorithm selectors on software engineering domains? Sibyl’s
predictions are 37.6% to 159.7% better than existing selectors.
RQ3: How does Sibyl’s overhead affect its performance?
Sibyl’s overhead is non-negligible, but there is evidence to
suggest a more efficient implementation will greatly reduce it.
RQ4: How do the components of Sibyl’s GNN contribute to
its performance? When combined, the core GNN components
of Sibyl – GAT layers and a jumping knowledge layer –
improve Sibyl’s performance substantially",https://ieeexplore.ieee.org/abstract/document/10172504,"[SMT solvers are often used in the back end of different software engineering tools─e.g., program verifiers, test generators, or program synthesizers. There are a plethora of algorithmic techniques for solving SMT queries. Among the available SMT solvers, each employs its own combination of algorithmic techniques that are optimized for different fragments of logics and problem types. The most efficient solver can change with small changes in the SMT query, which makes it nontrivial to decide which solver to use. Consequently, designers of software engineering tools often select a single solver, based on familiarity or convenience, and tailor their tool towards it. Choosing an SMT solver at design time misses the opportunity to optimize query solve times and, for tools where SMT solving is a bottleneck, the performance loss can be significant. In this work, we present Sibyl, an automated SMT selector based on graph neural networks (GNNs). Sibyl creates a graph representation of a given SMT query and uses GNNs to predict how each solver in a suite of SMT solvers would perform on said query. Sibyl learns to predict based on features of SMT queries that are specific to the population on which it is trained - avoiding the need for manual feature engineering. Once trained, Sibyl makes fast and accurate predictions which can substantially reduce the time needed to solve a set of SMT queries. We evaluate Sibyl in four scenarios in which SMT solvers are used: in competition, in a symbolic execution engine, in a bounded model checker, and in a program synthesis tool. We find that Sibyl improves upon the state of the art in nearly every case and provide evidence that it generalizes better than existing techniques. Further, we evaluate Sibyl's overhead and demonstrate that it has the potential to speedup a variety of different software engineering tools.]"
Evolution,Knowledge-seeking,0,0,FALSE,B7,Repeated Builds During Code Review: An Empirical Study of the OpenStack Community,"RQ1. How often are failing CI jobs rechecked?
RQ2. How often do CI outcomes change after a recheck?
RQ3. How much overhead is generated by rechecking
builds?",https://ieeexplore.ieee.org/abstract/document/10298533,"[Code review is a popular practice where developers critique each others' changes. Since automated builds can identify low-level issues (e.g., syntactic errors, regression bugs), it is not uncommon for software organizations to incorporate automated builds in the code review process. In such code review deployment scenarios, submitted change sets must be approved for integration by both peer code reviewers and automated build bots. Since automated builds may produce an unreliable signal of the status of a change set (e.g., due to “flaky” or non-deterministic execution behaviour), code review tools, such as Gerrit, allow developers to request a “recheck”, which repeats the build process without updating the change set. We conjecture that an unconstrained recheck command will waste time and resources if it is not applied judiciously. To explore how the recheck command is applied in a practical setting, in this paper, we conduct an empirical study of 66,932 code reviews from the OpenStack community. We quantitatively analyze (i) how often build failures are rechecked; (ii) the extent to which invoking recheck changes build failure outcomes; and (iii) how much waste is generated by invoking recheck. We observe that (i) 55% of code reviews invoke the recheck command after a failing build is reported; (ii) invoking the recheck command only changes the outcome of a failing build in 42% of the cases; and (iii) invoking the recheck command increases review waiting time by an average of 2,200% and equates to 187.4 compute years of waste-enough compute resources to compete with the oldest land living animal on earth. Our observations indicate that the recheck command is frequently used after the builds fail, but does not achieve a high likelihood of build success. Based on a developer survey and our history-based quantitative findings, we encourage reviewer teams to think twice before rechecking and be considerate of waste. While recheck currently generates plenty of wasted computational resources and bloats waiting times, it also presents exciting future opportunities for researchers and tool builders to propose solutions that can reduce waste.]"
Evolution,Knowledge-seeking,-1 (kein Go Vorwissen; breaking changes?),0,FALSE,B7,A Large-Scale Empirical Study on Semantic Versioning in Golang Ecosystem,"RQ1: How are semantic versioning compliance applied
in the Go ecosystem in terms of breaking changes?
• RQ2: How much adherence to semantic versioning compliance has increased over time?
• RQ3: What about the impact of breaking changes on
client programs?",https://arxiv.org/abs/2309.02894,"['Third-party libraries (TPLs) have become an essential component of software, accelerating development and reducing maintenance costs. However, breaking changes often occur during the upgrades of TPLs and prevent client programs from moving forward. Semantic versioning (SemVer) has been applied to standardize the versions of releases according to compatibility, but not all releases follow SemVer compliance. Lots of work focuses on SemVer compliance in ecosystems such as Java and JavaScript beyond Golang (Go for short). Due to the lack of tools to detect breaking changes and dataset for Go, developers of TPLs do not know if breaking changes occur and affect client programs, and developers of client programs may hesitate to upgrade dependencies in terms of breaking changes. To bridge this gap, we conduct the first large-scale empirical study in the Go ecosystem to study SemVer compliance in terms of breaking changes and their impact. In detail, we purpose GoSVI (Go Semantic Versioning Insight) to detect breaking changes and analyze their impact by resolving identifiers in client programs and comparing their types with breaking changes. Moreover, we collect the first large-scale Go dataset with a dependency graph from GitHub, including 124K TPLs and 532K client programs. Based on the dataset, our results show that 86.3% of library upgrades follow SemVer compliance and 28.6% of no-major upgrades introduce breaking changes. Furthermore, the tendency to comply with SemVer has improved over time from 63.7% in 2018/09 to 92.2% in 2023/03. Finally, we find 33.3% of downstream client programs may be affected by breaking changes. These findings provide developers and users of TPLs with valuable insights to help make decisions related to SemVer.']"
Evolution,"Knowledge-seeking 
Evaluation","-1 
(detaillierteres Wissen zu Python notwendig?)",0,TRUE,B7,Hard to Read and Understand Pythonic Idioms? DeIdiom and Explain Them in Non-Idiomatic Equivalent Code,"RQ1: Whatchallenges do pythonic idioms present to Python users in terms of understanding? 
RQ2: Howare pythonic idioms used in real projects? 
RQ3: Howare conciseness of pythonic idioms manifested? 
RQ4: What are the potential negative effects of using pythonic idioms?

RQ1(Accuracy): How accurate is our approach when transforming idiomatic code of nine pythonic idioms into non-idiomatic code? 
RQ2(Usefulness): Is the generated non-idiomatic code useful for understanding pythonic idiom usage?",https://dl.acm.org/doi/abs/10.1145/3597503.3639101,"['The Python community strives to design pythonic idioms so that Python users can achieve their intent in a more concise and efficient way. According to our analysis of 154 questions about challenges of understanding pythonic idioms on Stack Overflow, we find that Python users face various challenges in comprehending pythonic idioms. And the usage of pythonic idioms in 7,577 GitHub projects reveals the prevalence of pythonic idioms. By using a statistical sampling method, we find pythonic idioms result in not only lexical conciseness but also the creation of variables and functions, which indicates it is not straightforward to map back to non-idiomatic code. And usage of pythonic idioms may even cause potential negative effects such as code redundancy, bugs and performance degradation. To alleviate such readability issues and negative effects, we develop a transforming tool, DeIdiom, to automatically transform idiomatic code into equivalent non-idiomatic code. We test and review over 7,572 idiomatic code instances of nine pythonic idioms (list/set/dict-comprehension, chain-comparison, truth-value-test, loop-else, assign-multi-targets, for-multi-targets, star), the result shows the high accuracy of DeIdiom. Our user study with 20 participants demonstrates that explanatory non-idiomatic code generated by DeIdiom is useful for Python users to understand pythonic idioms correctly and efficiently, and leads to a more positive appreciation of pythonic idioms.']"
Evolution,Evaluation,0,0,FALSE,B8,Semantic GUI Scene Learning and Video Alignment for Detecting Duplicate Video-based Bug Reports,"RQ1: What is Janus𝑣𝑖𝑠’s duplicate detection performance? 
RQ2: What is Janus𝑡𝑥𝑡’s duplicate detection performance? 
RQ3: What is Janus𝑠𝑒𝑞’s duplicate detection performance? 
RQ4: What is the performance of Janus’s component combinations?",https://dl.acm.org/doi/abs/10.1145/3597503.3639163,"['Video-based bug reports are increasingly being used to document bugs for programs centered around a graphical user interface (GUI). However, developing automated techniques to manage video-based reports is challenging as it requires identifying and understanding often nuanced visual patterns that capture key information about a reported bug. In this paper, we aim to overcome these challenges by advancing the bug report management task ofduplicate detectionfor video-based reports. To this end, we introduce a new approach, called Janus, that adapts the scene-learning capabilities of vision transformers to capture subtle visual and textual patterns that manifest on app UI screens --- which is key to differentiating between similar screens for accurate duplicate report detection. Janus also makes use of a video alignment technique capable of adaptive weighting of video frames to account for typical bug manifestation patterns. In a comprehensive evaluation on a benchmark containing 7,290 duplicate detection tasks derived from 270 video-based bug reports from 90 Android app bugs, the best configuration of our approach achieves an overall mRR/mAP of 89.8%/84.7%, and for the large majority of duplicate detection tasks, outperforms prior work by ≈9% to a statistically significant degree. Finally, we qualitatively illustrate how the scene-learning capabilities provided by Janus benefits its performance.']"
Evolution,Evaluation,0,0,FALSE,B8,Developer-Intent Driven Code Comment Generation,"RQ1 : How does the DOME perform compared to the stateof-the-art comment generation baselines?
RQ2: How does each individual component in DOME
contribute to the overall performance?
RQ3: What is the perceived quality of intent-aware comments generated by DOME?",https://arxiv.org/abs/2302.07055,"['Existing automatic code comment generators mainly focus on producing a general description of functionality for a given code snippet without considering developer intentions. However, in real-world practice, comments are complicated, which often contain information reflecting various intentions of developers, e.g., functionality summarization, design rationale, implementation details, code properties, etc. To bridge the gap between automatic code comment generation and real-world comment practice, we define Developer-Intent Driven Code Comment Generation, which can generate intent-aware comments for the same source code with different intents. To tackle this challenging task, we propose DOME, an approach that utilizes Intent-guided Selective Attention to explicitly select intent-relevant information from the source code, and produces various comments reflecting different intents. Our approach is evaluated on two real-world Java datasets, and the experimental results show that our approach outperforms the state-of-the-art baselines. A human evaluation also confirms the significant potential of applying DOME in practical usage, enabling developers to comment code effectively according to their own needs.']"
Evolution,Evaluation,0,0,TRUE,B8,Has My Release Disobeyed Semantic Versioning? Static Detection Based On Semantic Differencing,"RQ1:WhatistheaccuracyofSembidintermsofSemBdetection? 
RQ2:HowistheeffectivenessofSembidagainstunittests? 
RQ3:HowdotopJavalibrariescomplywithSemVerrules?",https://dl.acm.org/doi/abs/10.1145/3551349.3556956,"['To enhance the compatibility in the version control of Java Third-party Libraries (TPLs), Maven adopts Semantic Versioning (SemVer) to standardize the underlying meaning of versions, but users could still confront abnormal execution and crash after upgrades even if compilation and linkage succeed. It is caused by semantic breaking (SemB) issues, such that APIs directly used by users have identical signatures but inconsistent semantics across upgrades. To strengthen compliance with SemVer rules, developers and users should be alerted of such issues. Unfortunately, it is challenging to detect them statically, because semantic changes in the internal methods of APIs are difficult to capture. Dynamic testing can confirmingly uncover some, but it is limited by inadequate coverage.', 'To detect SemB issues over compatible upgrades (Patch and Minor) by SemVer rules, we conduct an empirical study on 180 SemB issues to understand the root causes, inspired by which, we propose Sembid (Semantic Breaking Issue Detector) to statically detect such issues of TPLs for developers and users. Since APIs are directly used by users, Sembid detects and reports SemB issues based on APIs. For a pair of APIs, Sembid walks through the call chains originating from the API to locate breaking changes by measuring semantic diff. Then, Sembid checks if the breaking changes can affect API’s output along call chains. The evaluation showed Sembid achieved recall and precision and outperformed other API checkers on SemB API detection. We also revealed Sembid detected over 3 times more SemB APIs with better coverage than unit tests, the commonly used solution. Furthermore, we carried out an empirical study on 1,629,589 APIs from 546 version pairs of top Java libraries and found there were 2 ∼ 4 times more SemB APIs than those with signature-based issues. Due to various version release strategies, of Patch version pairs and of Minor version pairs had at least one API affected by any breaking.']"
Human and social aspects,Evaluation,"-1
(bin mir unsicher, ob da nicht Wissen zum Gehirn usw. benötigt sind)",-1,TRUE,-,Causal Relationships and Programming Outcomes: A Transcranial Magnetic Stimulation Experiment,"RQ1: Can wereplicate prior findings that neurostimulation of the SMA reduces mental rotation completion times? 
RQ2: Is there a direct causal relationship between activity in the SMA(or M1) brain region alone and performance? 
RQ3: Does neurostimulation of the SMA or M1 brain regions affect objective computing performance outcomes? 
RQ4: Does neurostimulation in the SMA or M1 brain region affect self-perceived problem difficulty?",https://dl.acm.org/doi/abs/10.1145/3597503.3639096,"['Understanding the relationship between cognition and programming outcomes is important: it can inform interventions that help novices become experts faster. Neuroimaging techniques can measure brain activity, but prior studies of programming report only correlations. We present the first causal neurological investigation of the cognition of programming by usingTranscranial Magnetic Stimulation(TMS). TMS permits temporary and noninvasive disruption of specific brain regions. By disrupting brain regions and then measuring programming outcomes, we discover whether a true causal relationship exists. To the best of our knowledge, this is the first use of TMS to study software engineering.', 'Where multiple previous studies reported correlations, we find no direct causal relationships between implicated brain regions and programming. Using a protocol that follows TMS best practices and mitigates for biases, we replicate psychology findings that TMS affects spatial tasks. We then find that neurostimulation can affect programming outcomes. Multi-level regression analysis shows that TMS stimulation of different regions significantly accounts for 2.2% of the variance in task completion time. Our results have implications for interventions in education and training as well as research into causal cognitive relationships.']"
Human and social aspects,Knowledge-seeking,0,0,FALSE,B11,How does Simulation-based Testing for Self-driving Cars match Human Perception?,"RQ1: To what extent does the OOB safety metric for simulation-based test cases of SDCs align with human safety assessment?
RQ2: To what extent does the safety assessment of simulation-based SDC test cases vary when humans can interact with the SDC?
RQ3: What are the main reality-gap characteristics perceived by humans in SDC test cases?",https://dl.acm.org/doi/abs/10.1145/3643768,"[""Software metrics such as coverage or mutation scores have been investigated for the automated quality assessment of test suites. While traditional tools rely on software metrics, the field of self-driving cars (SDCs) has primarily focused on simulation-based test case generation using quality metrics such as the out-of-bound (OOB) parameter to determine if a test case fails or passes. However, it remains unclear to what extent this quality metric aligns with the human perception of the safety and realism of SDCs. To address this (reality) gap, we conducted an empirical study involving 50 participants to investigate the factors that determine how humans perceive SDC test cases as safe, unsafe, realistic, or unrealistic. To this aim, we developed a framework leveraging virtual reality (VR) technologies, called SDC-Alabaster, to immerse the study participants into the virtual environment of SDC simulators. Our findings indicate that the human assessment of safety and realism of failing/passing test cases can vary based on different factors, such as the test's complexity and the possibility of interacting with the SDC. Especially for the assessment of realism, the participants' age leads to a different perception. This study highlights the need for more research on simulation testing quality metrics and the importance of human perception in evaluating SDCs.""]"
Human and social aspects,Knowledge-seeking,0,0,FALSE,B11,"A Case Study of Developer Bots: Motivations, Perceptions, and Challenges","RQ1: What processes and needs motivate developers to instrument bots in a large software engineering organization? What challenges do they face in developing the bots? 
RQ2: What is the experience of developers when using various developer bots (bene ts and challenges)? 
RQ3: Howdodevelopersengagewithdi erenttypesofdeveloper bots?",https://2023.esec-fse.org/details/fse-2023-research-papers/7/A-Case-Study-of-Developer-Bots-Motivations-Perceptions-and-Challenges,"[Continuous integration and deployment (CI/CD) is now a widely adopted development model in practice as it reduces the time from ideas to customers. This adoption has also revived the idea of “shifting left” during software development – a practice intended to find and prevent defects early in the software delivery process. To assist with that, engineering systems integrate developer bots in the development workflow to improve developer productivity and help developers identify issues early in the software delivery process.

In this paper, we present a case study of developer bots in ABC company. We identify and analyze 23 developer bots that are deployed across 13,000 repositories and assist about 6,000 developers daily in their CI/CD software development workflow. We classify these bots across five major categories: ConfigViolation, Security, Compliance, Developer Productivity, and Code Quality. By conducting interviews and surveys with bot developers and bot users and by analyzing more than half a million historical bot actions over a period of one and a half years, we present the motivations and use-cases behind these bots, factors impacting their usefulness as perceived by the bot users, and the challenges associated with their use. We discuss these findings against existing literature on developer bots, and highlight characteristics of efficient developer bots.]"
Human and social aspects,Knowledge-seeking,0,0,TRUE,B11,“STILL AROUND”: Experiences and Survival Strategies of Veteran Women Software Developers,"RQ1. What age- and gender-specific experiences have
veteran software developers of marginalized genders had
in their careers?
• RQ2. What strategies have veteran software developers
of marginalized genders adopted that they perceive as
contributing to their survival in software engineering?",https://arxiv.org/abs/2302.03723,"['The intersection of ageism and sexism can create a hostile environment for veteran software developers belonging to marginalized genders. In this study, we conducted 14 interviews to examine the experiences of people at this intersection, primarily women, in order to discover the strategies they employed in order to successfully remain in the field. We identified 283 codes, which fell into three main categories: Strategies, Experiences, and Perception. Several strategies we identified, such as (Deliberately) Not Trying to Look Younger, were not previously described in the software engineering literature. We found that, in some companies, older women developers are recognized as having particular value, further strengthening the known benefits of diversity in the workforce. Based on the experiences and strategies, we suggest organizations employing software developers to consider the benefits of hiring veteran women software developers. For example, companies can draw upon the life experiences of older women developers in order to better understand the needs of customers from a similar demographic. While we recognize that many of the strategies employed by our study participants are a response to systemic issues, we still consider that, in the short-term, there is benefit in describing these strategies for developers who are experiencing such issues today.']"
Human and social aspects,Evaluation,0,0,FALSE,B12,"Semi-Automatic, Inline and Collaborative Web Page Code Curations","RQ1: Can our approach help developers identify and curate relevant implicit links between web pages and specific source code locations? 
RQ2: Can developers successfully leverage previouslycurated links on their own change tasks?",https://ieeexplore.ieee.org/abstract/document/10172862,"[Software developers spend about a quarter of their workday using the web to fulfill various information needs. Searching for relevant information online can be time-consuming, yet acquired information is rarely systematically persisted for later reference. In this work, we introduce SALI, an approach for semi-automated inline linking of web pages to source code locations. SALI helps developers naturally capture high-quality, explicit links between web pages and specific source code lo-cations by recommending links for curation within the IDE. Through two laboratory studies, we examined the developer's ability to both curate and consume links between web pages and specific source code locations while performing software development tasks. The studies were performed with 20 subjects working on realistic software change tasks from widely-used open-source projects. Results show that developers continuously and concisely curate web pages at meaningful locations in the code with little effort. Additionally, we found that other developers could use these curations while performing new and different change tasks to speed up relevant information gathering within unfamiliar codebases by a factor of 2.4.]"
Human and social aspects,Evaluation,0,0,FALSE,B12,"AI-assisted Code Authoring at Scale: Fine-tuning, deploying, and mixed methods evaluation","RQ1. Model Evaluation: How well does CodeCompose generate one hidden line of code from existing code snippets?
RQ2. Adoption: How many suggestions are accepted by engineers and what proportion of the code is written by CodeCompose?
RQ3. Developer Feedback: How do developers perceive CodeCompose in their daily work?",https://dl.acm.org/doi/abs/10.1145/3643774,"['Generative LLMs have been shown to effectively power AI-based code authoring tools that can suggest entire statements or blocks of code during code authoring. In this paper we present CodeCompose, an AI-assisted code authoring tool developed and deployed at Meta internally. CodeCompose is based on the InCoder LLM that merges generative capabilities with bi-directionality. We have scaled up CodeCompose to serve tens of thousands of developers at Meta, across 9 programming languages and several coding surfaces. We present our experience in making design decisions about the model and system architecture for CodeCompose that addresses these challenges. To release a LLM model at this scale, we needed to first ensure that it is sufficiently accurate. In a random sample of 20K source code files, depending on the language, we are able to reproduce hidden lines between 40% and 58% of the time, an improvement of 1.4× and 4.1× over a model trained only on public data. We gradually rolled CodeCompose out to developers. At the time of this writing, 16K developers have used it with 8% of their code coming directly from CodeCompose. To triangulate our numerical findings, we conduct a thematic analysis on the feedback from 70 developers. We find that 91.5% of the feedback is positive, with the most common themes being discovering APIs, dealing with boilerplate code, and accelerating coding. Meta continues to integrate this feedback into CodeCompose.']"
Human and social aspects,Evaluation,0,0,TRUE,B12,"GenderMag Improves Discoverability in the Field, Especially for Women","Hypothesis1:Intheversion of Critique that was not designed using GenderMag, discoverability is significantly higher for men than for women.
Hypothesis 2: In the version of Critique that was redesigned using GenderMag,discoverability of Suggest Edit increased, especially for women.",https://dl.acm.org/doi/abs/10.1145/3597503.3639097,"[Prior research shows that the GenderMag method can help identify and address usability barriers that are more likely to affect women software users than men. However, the evidence for the effectiveness of GenderMag is limited to small lab studies. In this case study, by combining self-reported gender data from tens of thousands of users of an internal code review tool with software logs data gathered over a five-year period, we quantitatively show that GenderMag helped a team at Google (a) correctly identify discoverability as a usability barrier more likely to affect women than men, and (b) increase discoverability by 2.4x while also achieving gender parity. That is, compared to men using the original code review tool, women and men using the system redesigned with GenderMag were both 2.4x more likely to discover the ""Suggest Edit"" feature at any given time. Thus, this paper contributes the first large-scale evidence of the effectiveness of GenderMag in the field.]"
Requirements and modeling,Evaluation,0,1,FALSE,-,SmartCoCo: Checking Comment-code Inconsistency in Smart Contracts via Constraint Propagation and Binding,"RQ1: What is the prevalence of security-related
comment-code inconsistencies in smart contracts?
• RQ2: What is the effectiveness of SmartCoCo in detecting comment-code inconsistencies?
• RQ3: What is the performance in checking a smart
contract with proposed constraints?
• RQ4: Can large language models check CCIs identified
by SmartCoCo?",https://doi.org/10.1109/ASE56229.2023.00142,"[Smart contracts are programs running on the blockchain. Comments in source code provide meaningful information for developers to facilitate code writing and understanding. Given various kinds of token standards in smart contracts (e.g., ERC-20, ERC-721), developers often copy&paste code from other projects as templates, and then implement their own logic as add-ons to such templates. In many cases, the consistency between code and comment is not well-aligned, leading to comment-code inconsistencies (as we call CCIs). Such inconsistencies can mislead developers and users, and even introduce vulnerabilities to the contracts. In this paper, we present SmartCoCo, a novel framework to detect comment-code inconsistencies in smart contracts. In particular, our research focuses on comments related to roles, parameters, and events that may lead to security implications. To achieve this, SmartCoCo takes the original smart contract source code as input and automatically analyzes the comment and code to find potential inconsistencies. SmartCoCo associates comment constraints and code facts via a set of propagation and binding strategies, allowing it to effectively discover inconsistencies with more contextual information. We evaluated SmartCoCo on 101,780 unique smart contracts on Ethereum. The evaluation result shows that SmartCoCo achieves good effectiveness and efficiency. In particular, SmartCoCo reports 4,732 inconsistencies from 1,745 smart contracts, with a precision of over 79% on 439 manual-labeled comment-code inconsistencies. Meanwhile, it only takes 2.64 seconds to check a smart contract on average.]"
Requirements and modeling,Evaluation,-1 (RQ2 Begriffe?),1,FALSE,-,TRIAD: Automated Traceability Recovery based on Biterm-enhanced Deduction of Transitive Links among Artifacts,"RQ1: To what extent does TRIAD exceed the performance of baseline approaches? 
RQ2: What is the individual impact of biterms, outer- and inner-transitive on performance?",https://arxiv.org/pdf/2312.16854.pdf,"[Traceability allows stakeholders to extract and comprehend the trace links among software artifacts introduced across the software life cycle, to provide significant support for software engineering tasks. Despite its proven benefits, software traceability is challenging to recover and maintain manually. Hence, plenty of approaches for automated traceability have been proposed. Most rely on textual similarities among software artifacts, such as those based on Information Retrieval (IR). However, artifacts in different abstraction levels usually have different textual descriptions, which can greatly hinder the performance of IR-based approaches (e.g., a requirement in natural language may have a small textual similarity to a Java class). In this work, we leverage the consensual biterms and transitive relationships (i.e., inner- and outer-transitive links) based on intermediate artifacts to improve IR-based traceability recovery. We first extract and filter biterms from all source, intermediate, and target artifacts. We then use the consensual biterms from the intermediate artifacts to enrich the texts of both source and target artifacts, and finally deduce outer and inner-transitive links to adjust text similarities between source and target artifacts. We conducted a comprehensive empirical evaluation based on five systems widely used in other literature to show that our approach can outperform four state-of-the-art approaches in Average Precision over 15% and Mean Average Precision over 10% on average.]"
Requirements and modeling,Knowledge-seeking,0,0,FALSE,B9,A Comprehensive Study on Code Clones in Automated Driving Software,"RQ1: To what extent do code clones occur in automated
driving software?
–If a large number of code clones exist in automated driving
software, meaning code cloning deserves more attention in
such software systems. Hence, we examined the existence of
code clones by calculating their amounts and lines of code
(LOC).
RQ2. Do code clones have a tendency to introduce bugs in
automated driving software?
–In this question, we analyzed whether and to what extent
code clones bring bugs into automated driving software.
RQ3. Do code clones in automated driving software have
co-modifications?
RQ4. How do code clones in autonomous driving software
distribute over different modules? Which modules have more
bug-prone and co-modified clones?",https://ieeexplore.ieee.org/abstract/document/10298437,"[With the continuous improvement of artificial intelligence technology, autonomous driving technology has been greatly developed. Hence automated driving software has drawn more and more attention from both researchers and practitioners. Code clone is a commonly used to speed up the development cycle in software development, but many studies have shown that code clones may affect software maintainability. Currently, there is little research investigating code clones in automated driving software. To bridge this gap, we conduct a comprehensive experience study on the code clones in automated driving software. Through the analysis of Apollo and Autoware, we have presented that code clones are prevalent in automated driving software. about 30% of code lines are involved in code clones and more than 50% of files contain code clones. Moreover, a notable portion of these code clones has caused bugs and co-modifications. Due to the high complexity of autonomous driving, the automated driving software is often designed to be modular, with each module responsible for a single task. When considering each module individually, we have found that Perception, Planning, Canbus, and Sensing modules are more likely to encounter code clones, and more likely to have bug-prone and co-modified clones. Finally, we have shown that there exist cross-module clones to propagate bugs and co-modifications in different modules, which undermine the software's modularity.]"
Requirements and modeling,Knowledge-seeking,0,0,FALSE,B9,Too Much Accessibility is Harmful! Automated Detection and Analysis of Overly Accessible Elements in Mobile Apps,"RQ1. Howaccurate is OverSight in detecting OA elements? 
RQ2. How prevalent are over-access problems in securityconcerned apps? 
RQ3. What are the potential impacts of OA elements on different apps and communities? 
RQ4. What is the performance of OverSight?",https://dl.acm.org/doi/abs/10.1145/3551349.3560424,"['Mobile apps, an essential technology in today’s world, should provide equal access to all, including 15% of the world population with disabilities. Assistive Technologies\xa0(AT), with the help of Accessibility APIs, provide alternative ways of interaction with apps for disabled users who cannot see or touch the screen. Prior studies have shown that mobile apps are prone to the under-access problem, i.e., a condition in which functionalities in an app are not accessible to disabled users, even with the use of ATs. We study the dual of this problem, called the over-access problem, and defined as a condition in which an AT can be used to gain access to functionalities in an app that are inaccessible otherwise. Over-access has severe security and privacy implications, allowing one to bypass protected functionalities using ATs, e.g., using VoiceOver to read notes on a locked phone. Over-access also degrades the accessibility of apps by presenting to disabled users information that is actually not intended to be available on a screen, thereby confusing and hindering their ability to effectively navigate. In this work, we first empirically study overly accessible elements in Android apps and define a set of conditions that can result in over-access problem. We then present OverSight, an automated framework that leverages these conditions to detect overly accessible elements and verifies their accessibility dynamically using an AT. Our empirical evaluation of OverSight on real-world apps demonstrates OverSight’s effectiveness in detecting previously unknown security threats, workflow violations, and accessibility issues.']"
Requirements and modeling,Knowledge-seeking,0,0,TRUE,B9,A Qualitative Study on the Implementation Design Decisions of Developers,"RQ1: What implementation design decisions do software
developers make?
• RQ2: What considerations do software developers have
while making implementation design decisions?
• RQ3: What process do software developers follow to
make implementation design decisions?
• RQ4: Which types of developer expertise are described
in the implementation decision-making process?",https://arxiv.org/abs/2301.09789,"['Decision-making is a key software engineering skill. Developers constantly make choices throughout the software development process, from requirements to implementation. While prior work has studied developer decision-making, the choices made while choosing what solution to write in code remain understudied. In this mixed-methods study, we examine the phenomenon where developers select one specific way to implement a behavior in code, given many potential alternatives. We call these decisions implementation design decisions. Our mixed-methods study includes 46 survey responses and 14 semi-structured interviews with professional developers about their decision types, considerations, processes, and expertise for implementation design decisions. We find that implementation design decisions, rather than being a natural outcome from higher levels of design, require constant monitoring of higher level design choices, such as requirements and architecture. We also show that developers have a consistent general structure to their implementation decision-making process, but no single process is exactly the same. We discuss the implications of our findings on research, education, and practice, including insights on teaching developers how to make implementation design decisions.']"
Requirements and modeling,Evaluation,0,0,FALSE,B10,Groundhog: An Automated Accessibility Crawler for Mobile Apps,"RQ1.HoweffectiveisGroundhogindetectingaccessibilityissues? 
RQ2.HowdoesGroundhogcomparetoGoogleAccessibility Scanner(theofficialaccessibilitytestingtoolinAndroid)? 
RQ3.Whatarethecharacteristicsof thedetectedaccessibility issues?Howdotheyimpactappusageforuserswithdisabilities? 
RQ4.WhatistheperformanceofGroundhog?Towhatextent optimizationimprovesitsperformance?",https://dl.acm.org/doi/abs/10.1145/3551349.3556905,"['Accessibility is a critical software quality affecting more than 15% of the world’s population with some form of disabilities. Modern mobile platforms, i.e., iOS and Android, provide guidelines and testing tools for developers to assess the accessibility of their apps. The main focus of the testing tools is on examining a particular screen’s compliance with some predefined rules derived from accessibility guidelines. Unfortunately, these tools cannot detect accessibility issues that manifest themselves in interactions with apps using assistive services, e.g., screen readers. A few recent studies have proposed assistive-service driven testing; however, they require manually constructed inputs from developers to evaluate a specific screen or presume availability of UI test cases. In this work, we propose an automated accessibility crawler for mobile apps, Groundhog, that explores an app with the purpose of finding accessibility issues without any manual effort from developers. Groundhog assesses the functionality of UI elements in an app with and without assistive services and pinpoints accessibility issues with an intuitive video of how to replicate them. Our experiments show Groundhog is highly effective in detecting accessibility barriers that existing techniques cannot discover. Powered by Groundhog, we conducted an empirical study on a large set of real-world apps and found new classes of critical accessibility issues that should be the focus of future work in this area.']"
Requirements and modeling,Evaluation,0,0,FALSE,B10,Generating Critical Test Scenarios for Autonomous Driving Systems via Influential Behavior Patterns,"RQ1:Howeffective is CRISCO in finding safety violations of ADS? 
RQ2: How effective and efficient is CRISCO to expose safety violations compared to existing state-of-the-art technique? 
RQ3: Is CRISCO able to generate more types of safety-violation scenarios which are different from the traffic collisions occurred in the selected datasets inD and Stanford Drone?",https://dl.acm.org/doi/abs/10.1145/3551349.3560430,"['Autonomous Driving Systems (ADSs) are safety-critical, and must be fully tested before being deployed on real-world roads. To comprehensively evaluate the performance of ADSs, it is essential to generate various safety-critical scenarios. Most of existing studies assess ADSs either by searching high-dimensional input space, or using simple and pre-defined test scenarios, which are not efficient or not adequate. To better test ADSs, this paper proposes to automatically generate safety-critical test scenarios for ADSs by influential behavior patterns, which are mined from real traffic trajectories. Based on influential behavior patterns, a novel scenario generation technique, CRISCO, is presented to generate safety-critical scenarios for ADSs testing. CRISCO assigns participants to perform influential behaviors to challenge the ADS. It generates different test scenarios by solving trajectory constraints, and improves the challenge of those non-critical scenarios by adding participants’ behavior from influential behavior patterns incrementally. We demonstrate CRISCO on an industrial-grade ADS platform, Baidu Apollo. The experiment results show that our approach can effectively and efficiently generate critical scenarios to crash ADS, and it exposes 13 distinct types of safety violations in 12 hours. It also outperforms two state-of-art ADS testing techniques by exposing more 5 distinct types of safety violations on the same roads.']"
Requirements and modeling,Evaluation,0,0,TRUE,B10,Analyzing and Debugging Normative Requirements via Satisfiability Checking,Howeffective is LEGOS-SLEEC in detecting WFIs?,https://dl.acm.org/doi/abs/10.1145/3597503.3639093,"['As software systems increasingly interact with humans in application domains such as transportation and healthcare, they raise concerns related to the social, legal, ethical, empathetic, and cultural (SLEEC) norms and values of their stakeholders.Normative non-functional requirements(N-NFRs) are used to capture these concerns by setting SLEEC-relevant boundaries for system behavior. Since N-NFRs need to be specified by multiple stakeholders with widely different, non-technical expertise (ethicists, lawyers, regulators, end users, etc.), N-NFR elicitation is very challenging. To address this difficult task, we introduce N-Check, a novel tool-supported formal approach to N-NFR analysis and debugging. N-Check employs satisfiability checking to identify a broad spectrum of N-NFR well-formedness issues, such as conflicts, redundancy, restrictiveness, and insufficiency, yielding diagnostics that pinpoint their causes in a user-friendly way that enables non-technical stakeholders to understand and fix them. We show the effectiveness and usability of our approach through nine case studies in which teams of ethicists, lawyers, philosophers, psychologists, safety analysts, and engineers used N-Check to analyse and debug 233 N-NFRs, comprising 62 issues for the software underpinning the operation of systems, such as, assistive-care robots and tree-disease detection drones to manufacturing collaborative robots.']"
Testing and analysis,Knowledge-seeking,-1,1,FALSE,-,AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models,"–RQ1:CantheAST-Probelearn to parse on top of any informative code representation 4?
– RQ2: Which pre-trained language model best encodes the AST in its hidden representations? Wecompare a total of six pre-trained language models for three programming languages and assess which one best encodes the AST of input codes.
– RQ3: What layers of the pre-trained language models encode the AST better? Weapply our probe to specific hidden representation spaces of intermediate layers of the models and compare the probe effectiveness over the layers.
– RQ4: What is the dimension of the syntactic subspace S? To end our experiments, we are interested in how compact is the syntactic subspace in the hidden representation spaces of the pre-trained language models.",https://arxiv.org/pdf/2206.11719.pdf,"[Software systems introduce an increasing number of configuration options to provide flexibility, and support updating the options on the fly to provide persistent services. This mechanism, however, may affect the system reliability, leading to unexpected results like software crashes or functional errors. In this paper, we refer to the bugs caused by on-the-fly configuration updates as on-the-fly configuration bugs, or OCBugs for short. In this paper, we conducted the first in-depth study on 75 real-world OCBugs from 5 widely used systems to understand the symptoms, root causes, and triggering conditions of OCBugs. Based on our study, we designed and implemented PARACHUTE, an automated testing framework to detect OCBugs. Our key insight is that the value of one configuration option, either loaded at the startup phase or updated on the fly, should have the same effects on the target program. PARACHUTE generates tests for onthe-fly configuration updates by mutating the existing tests and conducts differential analysis to identify OCBugs. We evaluated PARACHUTE on 7 real-world software systems. The results show that PARACHUTE detected 75% (42/56) of the known OCBugs, and reported 13 unknown bugs, 11 of which have been confirmed or fixed by developers until the time of writing.]"
Testing and analysis,Evaluation,"-1
(end-to end fault loc.?, mutation selection konnte ich mir einigermaßen herleiten --> hab aber lein tieferes Verständnis, was man aber auch nicht braucht zum Beantworten der RQ fidne ich)",1,TRUE,-,Mutation-based Fault Localization of Deep Neural Networks,"• RQ1 (Effectiveness):
1) How does deepmufl compare to state-of-the-art tools
in terms of the number of bugs detected?
2) How many bugs does deepmufl detect from each subcategory of model bugs in our dataset and how does
that compare to state-of-the-art tools?
3) What are the overlap of detected bugs among deepmufl and other fault localization techniques?
• RQ2 (Efficiency):
1) What is the impact of mutation selection on the
effectiveness and efficiency of deepmufl?
2) How does deepmufl compare to state-of-the-art tools
in terms of end-to-end fault localization time?",https://arxiv.org/abs/2309.05067,"['Deep neural networks (DNNs) are susceptible to bugs, just like other types of software systems. A significant uptick in using DNN, and its applications in wide-ranging areas, including safety-critical systems, warrant extensive research on software engineering tools for improving the reliability of DNN-based systems. One such tool that has gained significant attention in the recent years is DNN fault localization. This paper revisits mutation-based fault localization in the context of DNN models and proposes a novel technique, named deepmufl, applicable to a wide range of DNN models. We have implemented deepmufl and have evaluated its effectiveness using 109 bugs obtained from StackOverflow. Our results show that deepmufl detects 53/109 of the bugs by ranking the buggy layer in top-1 position, outperforming state-of-the-art static and dynamic DNN fault localization systems that are also designed to target the class of bugs supported by deepmufl. Moreover, we observed that we can halve the fault localization time for a pre-trained model using mutation selection, yet losing only 7.55% of the bugs localized in top-1 position.']"
Testing and analysis,Knowledge-seeking,0,0,FALSE,B3,Out of Context: How important is Local Context in Neural Program Repair?,"RQ1. How important is local context for repair success? We study multiple context sizes, ranging from a single line up to 28 lines on both sides (56 lines) on three datasets (MegaDiff [21], TSSB [29], and ManySStuBs4J [10], see Table 1), totalling several hundreds of thousands of bugs. 
RQ2. Howdodifferentbugtypesandcomplexity(numberofchanges) respond to different context sizes and context window positions? Both, MaySStuBs4J [10] andTSSM-3M[29]classifybugsinto several bug types or bug patterns. We use this bug type labels to analyze how context size affects repair success for bugs of different types. We perform a similar analysis also for the number of changes of a bugfix. 
RQ3. What is the optimal context window position? In other words, given a fixed context budget, how should it be divided among pre-context and post-context? We experiment with six different context window positions (for four different context window sizes), from only pre-context over several combinations to only post-context. 
RQ4. Is there a connection between the model size (number of parameters), the number of sampled fix candidates and context? With more context, the amount of fix ingredients increases. Wehypothesize that in order to fully exploit context, model size should increase, as should the number of samples. We investigate if this is indeed the case.",https://dl.acm.org/doi/abs/10.1145/3597503.3639086,"['Deep learning source code models have been applied very successfully to the problem of automated program repair. One of the standing issues is the small input window of current models which often cannot fully fit the context code required for a bug fix (e.g., method or class declarations of a project). Instead, input is often restricted to the local context, that is, the lines below and above the bug location. In this work we study the importance of this local context on repair success: how much local context is needed?; is context before or after the bug location more important? how is local context tied to the bug type? To answer these questions we train and evaluate Transformer models in many different local context configurations on three datasets and two programming languages. Our results indicate that overall repair success increases with the size of the local context (albeit not for all bug types) and confirm the common practice that roughly 50--60% of the input window should be used for context leading the bug. Our results are not only relevant for researchers working on Transformer-based APR tools but also for benchmark and dataset creators who must decide what and how much context to include in their datasets.']"
Testing and analysis,Knowledge-seeking,0,0,FALSE,B3,Exploring Experiences with Automated Program Repair in Practice,"RQ1 What factors influence the awareness and adoption or use of APR in practice? 
RQ2 To what extent are APR tool(s) being used in practice compared to other forms of support? 
RQ3 Whatare the human-centric challenges faced by developers that can prevent the widespread use of APR tools?",https://dl.acm.org/doi/abs/10.1145/3597503.3639182,"[""Automated program repair, also known as APR, is an approach for automatically repairing software faults. There is a large amount of research on automated program repair, but very little offers in-depth insights into how practitioners think about and employ APR in practice. To learn more about practitioners' perspectives and experiences with current APR tools and techniques, we administered a survey, which received valid responses from 331 software practitioners. We analyzed survey responses to gain insights regarding factors that correlate with APR awareness, experience, and use. We established a strong correlation between APR awareness and tool use and attributes including job position, company size, total coding experience, and preferred language of software practitioners. We also found that practitioners are using other forms of support, such as co-workers and ChatGPT, more frequently than APR tools when fixing software defects. We learned about the drawbacks that practitioners encounter while utilizing existing APR tools and the impact that each drawback has on their practice. Our findings provide implications for research and practice centered on development, adoption, and use of APR.""]"
Testing and analysis,Knowledge-seeking,0,0,TRUE,B3,Understanding and Detecting On-the-Fly Configuration Bugs,"RQ1: What are the common symptoms of OCBugs? 
 RQ2: What are the root causes of OCBugs?
RQ3: What are the triggering conditions of OCBugs?
bzw. (aus den oberen RQs wurde ein Tool entiwckelt und mit den folgenden RQs untersucht)
• RQ1: How effective is PARACHUTE in detecting
known OCBugs? This question examines the recall of
PARACHUTE by calculating the percentage of bugs that
can be detected among all known bugs.
• RQ2: How effective is PARACHUTE in detecting unknown OCBugs? This question evaluates the precision
of PARACHUTE by calculating the percentage of true
positives among all reported bugs.
• RQ3: Can PARACHUTE outperform the state-of-the-art
tool for detecting configuration update bugs? This question compares PARACHUTE with Staccato, the most related work for detecting OCBugs.",https://leopard-lab.github.io/paper/icse23-Parachute.pdf,"[Software systems introduce an increasing number of configuration options to provide flexibility, and support updating the options on the fly to provide persistent services. This mechanism, however, may affect the system reliability, leading to unexpected results like software crashes or functional errors. In this paper, we refer to the bugs caused by on-the-fly configuration updates as on-the-fly configuration bugs, or OCBugs for short. In this paper, we conducted the first in-depth study on 75 real-world OCBugs from 5 widely used systems to understand the symptoms, root causes, and triggering conditions of OCBugs. Based on our study, we designed and implemented PARACHUTE, an automated testing framework to detect OCBugs. Our key insight is that the value of one configuration option, either loaded at the startup phase or updated on the fly, should have the same effects on the target program. PARACHUTE generates tests for onthe-fly configuration updates by mutating the existing tests and conducts differential analysis to identify OCBugs. We evaluated PARACHUTE on 7 real-world software systems. The results show that PARACHUTE detected 75% (42/56) of the known OCBugs, and reported 13 unknown bugs, 11 of which have been confirmed or fixed by developers until the time of writing.]"
Testing and analysis,Evaluation,0,0,FALSE,B4,Accelerating Continuous Integration with Parallel Batch Testing,"RQ1:Howdoesparallelization affect the feedback time performance of TestAll with varying numbers of machines? 
RQ2: How effective is ConstantBatching in terms of feedback time and execution reduction when executed in parallel with varying numbers of machines? 
RQ3: How effective is BatchAll in terms of feedback time and execution reduction when executed in parallel with varying numbers of machines? 
RQ4: How effective is TestCaseBatching in terms of feedback time and execution reduction when executed in parallel with varying numbers of machines? Our major contributions to this study are as follows.",https://arxiv.org/pdf/2308.13129,"[Continuous integration at scale is costly but essential to software development. Various test optimization techniques including test selection and prioritization aim to reduce the cost. Test batching is an effective alternative, but overlooked technique. This study evaluates parallelization’s effect by adjusting machine count for test batching and introduces two novel approaches.
We establish TestAll as a baseline to study the impact of parallelism and machine count on feedback time. We re-evaluate ConstantBatching and introduce DynamicBatching, which adapts batch size based on the remaining changes in the queue. We also propose TestCaseBatching, enabling new builds to join a batch before full test execution, thus speeding up continuous integration. Our evaluations utilize Ericsson’s results and 276 million test outcomes from open-source Chrome, assessing feedback time, execution reduction, and providing access to Chrome project scripts and data.
The results reveal a non-linear impact of test parallelization on feedback time, as each test delay compounds across the entire test queue. ConstantBatching, with a batch size of 4, utilizes up to 72% fewer machines to maintain the actual average feedback time and provides a constant execution reduction of up to 75%. Similarly, DynamicBatching maintains the actual average feedback time with up to 91% fewer machines and exhibits variable execution reduction of up to 99%. TestCaseBatching holds the line of the actual average feedback time with up to 81% fewer machines and demonstrates variable execution reduction of up to 67%. We recommend practitioners use DynamicBatching and TestCaseBatching to reduce the required testing machines efficiently. Analyzing historical data to find the threshold where adding more machines has minimal impact on feedback time is also crucial for resource-effective testing.]"
Testing and analysis,Evaluation,0,0,FALSE,B4,PyTy: Repairing Static Type Errors in Python,"RQ1 Howeffective is our automated data gathering at producing minimal code changes that fix type errors? 
RQ2 Howeffective is PyTy at fixing type errors? 
RQ3 Howdovariants of PyTy compare to the full approach? 
RQ4 Howdoes PyTy compare to state-of-the-art APR techniques?",https://dl.acm.org/doi/10.1145/3597503.3639184,"['Gradual typing enables developers to annotate types of their own choosing, offering a flexible middle ground between no type annotations and a fully statically typed language. As more and more code bases get type-annotated, static type checkers detect an increasingly large number of type errors. Unfortunately, fixing these errors requires manual effort, hampering the adoption of gradual typing in practice. This paper presents PyTy, an automated program repair approach targeted at statically detectable type errors in Python. The problem of repairing type errors deserves specific attention because it exposes particular repair patterns, offers a warning message with hints about where and how to apply a fix, and because gradual type checking serves as an automatic way to validate fixes. We addresses this problem through three contributions: (i) an empirical study that investigates how developers fix Python type errors, showing a diverse set of fixing strategies with some recurring patterns; (ii) an approach to automatically extract type error fixes, which enables us to create a dataset of 2,766 error-fix pairs from 176 GitHub repositories, named PyTyDefects; (iii) the first learning-based repair technique for fixing type errors in Python. Motivated by the relative data scarcity of the problem, the neural model at the core of PyTy is trained via cross-lingual transfer learning. Our evaluation shows that PyTy offers fixes for ten frequent categories of type errors, successfully addressing 85.4% of 281 real-world errors. This effectiveness outperforms state-of-the-art large language models asked to repair type errors (by 2.1x) and complements a previous technique aimed at type errors that manifest at runtime. Finally, 20 out of 30 pull requests with PyTy-suggested fixes have been merged by developers, showing the usefulness of PyTy in practice.']"
Testing and analysis,Evaluation,0,0,TRUE,B4,EndWatch: A Practical Method for Detecting Non-Termination in Real-World Software,"• RQ1: How effective is EndWatch on existing benchmark
programs compared with the state-of-the-art tools?
• RQ2: How effective is EndWatch on detecting CVEs in
real world programs?
• RQ3: How useful is EndWatch in finding zero-day nontermination bugs?",https://personal.ntu.edu.sg/yi_li/files/Zhang2023EAP.pdf,"[Detecting non-termination is crucial for ensuring program correctness and security, such as preventing denial-ofservice attacks. While termination analysis has been studied for manyyears, existing methods have limited scalability and are only effective on small programs. To address this issue, we propose a practical termination checking technique, called EndWatch, for detecting non-termination caused by infinite loops through testing. Specifically, we introduce two methods to generate nontermination oracles based on checking state revisits, i.e., if the program returns to a previously visited state at the same program location, it does not terminate. The non-termination oracles can be incorporated into testing tools (e.g., AFL used in this paper) to detect non-termination in large programs. For linear loops, we perform symbolic execution on individual loops to infer State Revisit Conditions (SRCs) and instrument SRCs into target loops. For non-linear loops, we instrument target loops for checking concrete state revisits during execution. We evaluated EndWatch on standard benchmarks with small-sized programs and realworld projects with large-sized programs. The evaluation results show that EndWatch is more effective than the state-of-the-art tools on standard benchmarks (detecting 87% of non-terminating programs while the best baseline detects only 67%), and useful in detecting non-termination in real-world projects (detecting 90% of known non-termination CVEs and 4 unknown bugs).]"