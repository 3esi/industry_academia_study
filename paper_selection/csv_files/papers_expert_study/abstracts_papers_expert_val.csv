Paper Name,Research Questions,URL,Abstract
Towards Understanding Fairness and its Composition in Ensemble Machine Learning,"RQ1: What are the fairness measures of various ensemble techniques? 
• RQ2: How does fairness compose in ensemble models? 
• RQ3: Can ensemble-related hyperparameters be chosen to design fair ensemble models?",https://arxiv.org/abs/2212.04593,"['Machine Learning (ML) software has been widely adopted in modern society, with reported fairness implications for minority groups based on race, sex, age, etc. Many recent works have proposed methods to measure and mitigate algorithmic bias in ML models. The existing approaches focus on single classifier-based ML models. However, real-world ML models are often composed of multiple independent or dependent learners in an ensemble (e.g., Random Forest), where the fairness composes in a non-trivial way. How does fairness compose in ensembles? What are the fairness impacts of the learners on the ultimate fairness of the ensemble? Can fair learners result in an unfair ensemble? Furthermore, studies have shown that hyperparameters influence the fairness of ML models. Ensemble hyperparameters are more complex since they affect how learners are combined in different categories of ensembles. Understanding the impact of ensemble hyperparameters on fairness will help programmers design fair ensembles. Today, we do not understand these fully for different ensemble algorithms. In this paper, we comprehensively study popular real-world ensembles: bagging, boosting, stacking and voting. We have developed a benchmark of 168 ensemble models collected from Kaggle on four popular fairness datasets. We use existing fairness metrics to understand the composition of fairness. Our results show that ensembles can be designed to be fairer without using mitigation techniques. We also identify the interplay between fairness composition and data characteristics to guide fair ensemble design. Finally, our benchmark can be leveraged for further research on fair ensembles. To the best of our knowledge, this is one of the first and largest studies on fairness composition in ensembles yet presented in the literature.']"
An Empirical Study on Noisy Label Learning for Program Understanding,"RQ1:Howdodifferenttypes of synthetic label noises in program classification affect the performance of deep learning models when NLL is not introduced? • 
RQ2:Howdoexisting NLL approaches perform on different synthetic noises in program classification? 
• RQ3:HowdoNLLapproaches perform on program understanding tasks with real-world noises?",https://arxiv.org/abs/2307.08990,"['Recently, deep learning models have been widely applied in program understanding tasks, and these models achieve state-of-the-art results on many benchmark datasets. A major challenge of deep learning for program understanding is that the effectiveness of these approaches depends on the quality of their datasets, and these datasets often contain noisy data samples. A typical kind of noise in program understanding datasets is label noise, which means that the target outputs for some inputs are incorrect. Researchers have proposed various approaches to alleviate the negative impact of noisy labels, and formed a new research topic: noisy label learning (NLL). In this paper, we conduct an empirical study on the effectiveness of noisy label learning on deep learning for program understanding datasets. We evaluate various NLL approaches and deep learning models on three tasks: program classification, vulnerability detection, and code summarization. From the evaluation results, we come to the following findings: 1) small trained-from-scratch models are prone to label noises in program understanding, while large pre-trained models are highly robust against them. 2) NLL approaches significantly improve the program classification accuracies for small models on noisy training sets, but they only slightly benefit large pre-trained models in classification accuracies. 3) NLL can effectively detect synthetic noises in program understanding, but struggle in detecting real-world noises. We believe our findings can provide insights on the abilities of NLL in program understanding, and shed light on future works in tackling noises in software engineering datasets. We have released our code at this https URL .']"
Development in times of hype: How freelancers explore Generative AI?,RQ: What challenges do freelancers experience when developing solutions based on generative AI?,https://doi.org/10.1145/3597503.3639111,"['The rise of generative AI has led many companies to hire freelancers to harness its potential. However, this technology presents unique challenges to developers who have not previously engaged with it. Freelancers may find these challenges daunting due to the absence of organizational support and their reliance on positive client feedback. In a study involving 52 freelance developers, we identified multiple challenges associated with developing solutions based on generative AI. Freelancers often struggle with aspects they perceive as unique to generative AI such as unpredictability of its output, the occurrence of hallucinations, and the inconsistent effort required due to trial-and-error prompting cycles. Further, the limitations of specific frameworks, such as token limits and long response times, add to the complexity. Hype-related issues, such as inflated client expectations and a rapidly evolving technological ecosystem, further exacerbate the difficulties. To address these issues, we propose Software Engineering for Generative AI (SE4GenAI) and Hype-Induced Software Engineering (HypeSE) as areas where the software engineering community can provide effective guidance. This support is essential for freelancers working with generative AI and other emerging technologies.']"
Using an LLM to Help With Code Understanding,"RQ1: To what extent does GILT affect developers’ understanding, task completion time, and task completion rates when faced with unfamiliar code? 
• RQ2: How do developers interact with GILT, and to what extent does that differ between the participants? 
• RQ3: Howdodevelopers perceive the usefulness of GILT?",https://dl.acm.org/doi/abs/10.1145/3597503.3639187,"[""Understanding code is challenging, especially when working in new and complex development environments. Code comments and documentation can help, but are typically scarce or hard to navigate. Large language models (LLMs) are revolutionizing the process of writing code. Can they do the same for helping understand it? In this study, we provide a first investigation of an LLM-based conversational UI built directly in the IDE that is geared towards code understanding. Our IDE plugin queries OpenAI's GPT-3.5-turbo model with four high-level requestswithoutthe user having to write explicit prompts: to explain a highlighted section of code, provide details of API calls used in the code, explain key domain-specific terms, and provide usage examples for an API. The plugin also allows for open-ended prompts, which are automatically contextualized to the LLM with the program being edited. We evaluate this system in a user study with 32 participants, which confirms that using our plugin can aid task completion more than web search. We additionally provide a thorough analysis of the ways developers use, and perceive the usefulness of, our system, among others finding that the usage and benefits differ between students and professionals. We conclude that in-IDE prompt-less interaction with LLMs is a promising future direction for tool builders.""]"
Can Machine Learning Pipelines Be Better Configured?,"• RQ1 (Common ML Library Combinations): What combinations of ML libraries do developers commonly use? To answer
RQ1, we collect 11,363 published ML pipelines and analyze their
library usage to identify common library combinations.
• RQ2 (Impacts of Different ML Library Version Combinations): Do different version combinations of ML libraries
affect pipelines’ performances? To answer RQ2, for each ML
pipeline, we generate a series of variants with different ML library
version combinations to inspect their performance inconsistencies. In particular, we define the generation rules of ML library
version combinations, to systematically explore their impacts on
pipelines’ performances.
• RQ3 (Root Causes of Performance Inconsistencies): What
are the root causes of pipelines’ performance inconsistencies when adopting different version combinations of ML
libraries? To answer RQ3, we consider the pipeline variants that
induce (1) significant performance inconsistencies, (2) crashes
and (3) NaN bugs as subjects, to analyze the corresponding root
causes and triggering conditions.",https://dl.acm.org/doi/abs/10.1145/3611643.3616352,"['A Machine Learning (ML) pipeline configures the workflow of a learning task using the APIs provided by ML libraries. However, a pipeline’s performance can vary significantly across different configurations of ML library versions. Misconfigured pipelines can result in inferior performance, such as poor execution time and memory usage, numeric errors and even crashes. A pipeline is subject to misconfiguration if it exhibits significantly inconsistent performance upon changes in the versions of its configured libraries or the combination of these libraries. We refer to such performance inconsistency as a pipeline configuration (PLC) issue.', 'There is no prior systematic study on the pervasiveness, impact and root causes of PLC issues. A systematic understanding of these issues helps configure effective ML pipelines and identify misconfigured ones. In this paper, we conduct the first empirical study of PLC issues. To better dig into the problem, we propose Piecer, an infrastructure that automatically generates a set of pipeline variants by varying different version combinations of ML libraries and compares their performance inconsistencies. We apply Piecer to the 3,380 pipelines that can be deployed out of the 11,363 ML pipelines collected from multiple ML competitions at Kaggle platform. The empirical study results show that 1,092 (32.3']"
Reusing Deep Neural Network Models through Model Re-engineering,"RQ1: How effective is our model re-engineering approach
in reusing trained models?
• RQ2: Does reusing a re-engineered model incur less overhead than reusing the original model?
• RQ3: Does reusing the re-engineered model mitigate the
defect inheritance?",https://ieeexplore.ieee.org/abstract/document/10172769,[]
Flexible and Optimal Dependency Management via Max-SMT,"RQ1: Can MAXNPM find better solutions than NPM when
given different optimization objectives?
RQ2: Do MAXNPM’s solutions pass existing test suites?
RQ3: Does MAXNPM successfully solve packages that NPM
solves?
RQ4: Does using MAXNPM substantially increase solving
time?",https://arxiv.org/abs/2203.13737,"[""Package managers such as NPM have become essential for software development. The NPM repository hosts over 2 million packages and serves over 43 billion downloads every week. Unfortunately, the NPM dependency solver has several shortcomings. 1) NPM is greedy and often fails to install the newest versions of dependencies; 2) NPM's algorithm leads to duplicated dependencies and bloated code, which is particularly bad for web applications that need to minimize code size; 3) NPM's vulnerability fixing algorithm is also greedy, and can even introduce new vulnerabilities; and 4) NPM's ability to duplicate dependencies can break stateful frameworks and requires a lot of care to workaround. Although existing tools try to address these problems they are either brittle, rely on post hoc changes to the dependency tree, do not guarantee optimality, or are not composable. We present PacSolve, a unifying framework and implementation for dependency solving which allows for customizable constraints and optimization goals. We use PacSolve to build MaxNPM, a complete, drop-in replacement for NPM, which empowers developers to combine multiple objectives when installing dependencies. We evaluate MaxNPM with a large sample of packages from the NPM ecosystem and show that it can: 1) reduce more vulnerabilities in dependencies than NPM's auditing tool in 33% of cases; 2) chooses newer dependencies than NPM in 14% of cases; and 3) chooses fewer dependencies than NPM in 21% of cases. All our code and data is open and available.""]"
Improving the Learning of Code Review Successive Tasks with Cross-Task Knowledge Distillation,"RQ1: Accuracy on code refinement Howdoesthe coderefinementmodel,guidedbythe qualityestimationmodel, perform compared to baseline models? 
• RQ2: Accuracy on comment generation How does the comment generation model, guided by the code refinement model (and the embedding alignment), perform compared to baseline models? 
• RQ3: Effect of the embedding alignment objective Considering that the inclusion of the embedding has an effect on the training cost, do we have similar results without considering the embedding? 
• RQ4: Training time of DISCOREV What is the additional time of the joint training compared to baseline models?",https://dl.acm.org/doi/abs/10.1145/3643775,"['Code review is a fundamental process in software development that plays a pivotal role in ensuring code quality and reducing the likelihood of errors and bugs. However, code review can be complex, subjective, and time-consuming.Quality estimation,comment generation, andcode refinementconstitute the three key tasks of this process, and their automation has traditionally been addressed separately in the literature using different approaches. In particular, recent efforts have focused on fine-tuning pre-trained language models to aid in code review tasks, with each task being considered in isolation. We believe that these tasks are interconnected, and their fine-tuning should consider this interconnection. In this paper, we introduce a novel deep-learning architecture, named DISCOREV, which employs cross-task knowledge distillation to address these tasks simultaneously. In our approach, we utilize a cascade of models to enhance bothcomment generationandcode refinementmodels. The fine-tuning of thecomment generationmodel is guided by thecode refinementmodel, while the fine-tuning of thecode refinementmodel is guided by thequality estimationmodel. We implement this guidance using two strategies: a feedback-based learning objective and an embedding alignment objective. We evaluate DISCOREV by comparing it to state-of-the-art methods based on independent training and fine-tuning. Our results show that our approach generates better review comments, as measured by theBLEUscore, as well as more accuratecode refinementaccording to theCodeBLEUscore.']"
When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference,"RQ1:For different inputs, how many layers of the LCM are indispensable to yield correct predictions? 
RQ2:Is it advantageous to continue code completion after a wrong token has been generated?",https://arxiv.org/pdf/2401.09964.pdf,[]
Modularizing while Training: a New Paradigm for Modularizing DNN Models,"RQ1:HoweffectiveisMwTintrainingandmodularizingCNN models? 
•RQ2:HowefficientisMwTintrainingandmodularizingCNN models? 
•RQ3:HoweffectiveisMwTinreusingCNNmodules? 
•RQ4:Howdothemajorhyper-parametersinfluencetheperformanceofMwT?",https://arxiv.org/pdf/2306.09376.pdf,[]
The Smelly Eight: An Empirical Study on the Prevalence of Code Smells in Quantum Computing,"RQ1: How do practitioners perceive quantum-specific
code smells?
RQ2: What is the prevalence of quantum-specific code
smells in quantum programs?",https://ieeexplore.ieee.org/abstract/document/10172808,[]
CodeGen4Libs: A Two-Stage Approach for Library-Oriented Code Generation,"RQ1: How much do developers prefer specific third-party
libraries when coding?
RQ2: To what extent are developers familiar with the
contextual intricacies of third-party libraries when coding?
RQ3: How effective are current code generation models
at generating code for specific third-party libraries without
specific fine-tuning on library-related data?",https://ieeexplore.ieee.org/abstract/document/10298327,[]
SkCoder: A Sketch-based Approach for Automatic Code Generation,"RQ1: How does SKCODER perform compared to SOTA
baselines? 
RQ2: What are the contributions of different modules in
our approach? 
RQ3: What is the better design choice of the sketcher?",https://arxiv.org/abs/2302.06144,"['Recently, deep learning techniques have shown great success in automatic code generation. Inspired by the code reuse, some researchers propose copy-based approaches that can copy the content from similar code snippets to obtain better performance. Practically, human developers recognize the content in the similar code that is relevant to their needs, which can be viewed as a code sketch. The sketch is further edited to the desired code. However, existing copy-based approaches ignore the code sketches and tend to repeat the similar code without necessary modifications, which leads to generating wrong results. In this paper, we propose a sketch-based code generation approach named SkCoder to mimic developers\' code reuse behavior. Given a natural language requirement, SkCoder retrieves a similar code snippet, extracts relevant parts as a code sketch, and edits the sketch into the desired code. Our motivations are that the extracted sketch provides a well-formed pattern for telling models ""how to write"". The post-editing further adds requirement-specific details to the sketch and outputs the complete code. We conduct experiments on two public datasets and a new dataset collected by this work. We compare our approach to 20 baselines using 5 widely used metrics. Experimental results show that (1) SkCoder can generate more correct programs, and outperforms the state-of-the-art - CodeT5-base by 30.30%, 35.39%, and 29.62% on three datasets. (2) Our approach is effective to multiple code generation models and improves them by up to 120.1% in Pass@1. (3) We investigate three plausible code sketches and discuss the importance of sketches. (4) We manually evaluate the generated code and prove the superiority of our SkCoder in three aspects.']"
Studying and Understanding the Tradeoffs Between Generality and Reduction in Software Debloating,"RQ1:Howdotheapproaches considered compare in terms of reduction, c-generality, and their tradeoff? 
• RQ2:Howdotheapproaches considered compare in terms of reduction, r-generality, and their tradeoff? 
• RQ3: How do the approaches considered perform when an increasing amount of inputs is used for debloating? 
• RQ4:Howefficient are the approaches?",https://dl.acm.org/doi/abs/10.1145/3551349.3556970,"['Existing approaches for program debloating often use a usage profile, typically provided as a set of inputs, for identifying the features of a program to be preserved. Specifically, given a program and a set of inputs, these techniques produce a reduced program that behaves correctly for these inputs. Focusing only on reduction, however, would typically result in programs that are overfitted to the inputs used for debloating. For this reason, another important factor to consider in the context of debloating is generality, which measures the extent to which a debloated program behaves correctly also for inputs that were not in the initial usage profile. Unfortunately, most evaluations of existing debloating approaches only consider reduction, thus providing partial information on the effectiveness of these approaches. To address this limitation, we perform an empirical evaluation of the reduction and generality of 4 debloating techniques, 3 state-of-the-art ones, and a baseline, on a set of 25 programs and different sets of inputs for these programs. Our results show that these approaches can indeed produce programs that are overfitted to the inputs used and have low generality. Based on these results, we also propose two new augmentation approaches and evaluate their effectiveness. The results of this additional evaluation show that these two approaches can help improve program generality without significantly affecting size reduction. Finally, because different approaches have different strengths and weaknesses, we also provide guidelines to help users choose the most suitable approach based on their specific needs and context.']"
Generative Type Inference for Python,"RQ1: How effective is TYPEGEN in type inference
compared with existing approaches?
• RQ2: How capable is TYPEGEN in language models with
different parameter sizes?
• RQ3: What are the impacts of different parts in the
prompt design of TYPEGEN?
• RQ4: What are the impacts of different examples in
TYPEGEN?",https://arxiv.org/abs/2307.09163,"['Python is a popular dynamic programming language, evidenced by its ranking as the second most commonly used language on GitHub. However, its dynamic type system can lead to potential type errors, leading researchers to explore automatic type inference approaches for Python programs. The rule-based type inference approaches can ensure the accuracy of predicted variable types, but they suffer from low coverage problems. Supervised type inference approaches, while feature-agnostic, require large, high-quality annotated datasets and are limited to pre-defined types. As zero-shot approaches, the cloze-style approaches reformulate the type inference problem into a fill-in-the-blank problem. However, their performance is limited. This paper introduces TypeGen, a few-shot generative type inference approach that incorporates static domain knowledge from static analysis. TypeGen creates chain-of-thought (COT) prompts by translating the type inference steps of static analysis into prompts based on the type dependency graphs (TDGs), enabling language models to learn from how static analysis infers types. By combining COT prompts with code slices and type hints, TypeGen constructs example prompts from human annotations. TypeGen only requires very few annotated examples to teach language models to generate similar COT prompts via in-context learning. Moreover, TypeGen enhances the interpretability of results through the use of the input-explanation-output strategy. Experiments show that TypeGen outperforms the best baseline Type4Py by 10.0% for argument type prediction and 22.5% in return value type prediction in terms of top-1 Exact Match by using only five examples. Furthermore, TypeGen achieves substantial improvements of 27% to 84% compared to the zero-shot performance of large language models with parameter sizes ranging from 1.3B to 175B in terms of top-1 Exact Match.']"
Static Application Security Testing (SAST) Tools for Smart Contracts: How Far Are We?,"RQ1:CoverageAnalysis. To what extent do existing SAST tools support different vulnerability types?
RQ2: Effectiveness Analysis. How effective are these SAST tools in detecting vulnerabilities on our benchmark?
RQ3: Consistency Analysis. This research question focuses on two consistency analyses: 1) Are the detection results consistent among these tools in terms of the detected vulnerability categories? 2) How e ective are these SAST tools when combining their detecting results?
RQ4: E iciency Analysis. How e cient are these SAST tools to perform an analysis?",https://arxiv.org/abs/2404.18186,"['In recent years, the importance of smart contract security has been heightened by the increasing number of attacks against them. To address this issue, a multitude of static application security testing (SAST) tools have been proposed for detecting vulnerabilities in smart contracts. However, objectively comparing these tools to determine their effectiveness remains challenging. Existing studies often fall short due to the taxonomies and benchmarks only covering a coarse and potentially outdated set of vulnerability types, which leads to evaluations that are not entirely comprehensive and may display bias. In this paper, we fill this gap by proposing an up-to-date and fine-grained taxonomy that includes 45 unique vulnerability types for smart contracts. Taking it as a baseline, we develop an extensive benchmark that covers 40 distinct types and includes a diverse range of code characteristics, vulnerability patterns, and application scenarios. Based on them, we evaluated 8 SAST tools using this benchmark, which comprises 788 smart contract files and 10,394 vulnerabilities. Our results reveal that the existing SAST tools fail to detect around 50% of vulnerabilities in our benchmark and suffer from high false positives, with precision not surpassing 10%. We also discover that by combining the results of multiple tools, the false negative rate can be reduced effectively, at the expense of flagging 36.77 percentage points more functions. Nevertheless, many vulnerabilities, especially those beyond Access Control and Reentrancy vulnerabilities, remain undetected. We finally highlight the valuable insights from our study, hoping to provide guidance on tool development, enhancement, evaluation, and selection for developers, researchers, and practitioners.']"
Lejacon: A Lightweight and Efficient Approach to Java Confidential Computing on SGX,"RQ1 Can Lejacon support Java confidential computing
effectively?
• RQ2 Can Java applications achieve competitive performance on Lejacon, compared with some state-of-the-art
runtime system(s)?
• RQ3 Can Lejacon be practically used in running Java
confidential computing tasks?",https://ieeexplore.ieee.org/abstract/document/10172889,[]
Measuring Secure Coding Practice and Culture: A Finger Pointing at the Moon is not the Moon,"Research Question 1: What are the secure-coding characteristics of our sample group? 
Research Question 2: What are the security culture characteristics of our sample group? 
Research Question 3: Do secure coding practice and culture correlate, and if not, what lessons can we learn to help support the development of secure coding?",https://ieeexplore.ieee.org/abstract/document/10172883,[]
An Empirical Study of Deep Learning Models for Vulnerability Detection,"RQ1 Do models agree on the vulnerability detection results? What are the variabilities across different runs of a model and across different models? 
• RQ2 Are certain types of vulnerabilities easier to detect? Should we build models for each type of vulnerabilities or should we build one model that can detect all the vulnerabilities? 
• RQ3 Are programs with certain code features harder to be predicted correctly by current models, and if so, what are those code features?
• RQ4 Can increasing the dataset size help improve the
model performance for vulnerability detection?",https://doi.org/10.48550/arXiv.2212.08109,"['Deep learning (DL) models of code have recently reported great progress for vulnerability detection. In some cases, DL-based models have outperformed static analysis tools. Although many great models have been proposed, we do not yet have a good understanding of these models. This limits the further advancement of model robustness, debugging, and deployment for the vulnerability detection. In this paper, we surveyed and reproduced 9 state-of-the-art (SOTA) deep learning models on 2 widely used vulnerability detection datasets: Devign and MSR. We investigated 6 research questions in three areas, namely model capabilities, training data, and model interpretation. We experimentally demonstrated the variability between different runs of a model and the low agreement among different models\' outputs. We investigated models trained for specific types of vulnerabilities compared to a model that is trained on all the vulnerabilities at once. We explored the types of programs DL may consider ""hard"" to handle. We investigated the relations of training data sizes and training data composition with model performance. Finally, we studied model interpretations and analyzed important features that the models used to make predictions. We believe that our findings can help better understand model results, provide guidance on preparing training data, and improve the robustness of the models. All of our datasets, code, and results are available at this https URL .']"
What Challenges Do Developers Face About Checked-in Secrets in Software Artifacts?,"• RQ1: What are the technical challenges faced by developers related to checked-in secrets?
RQ1.1 What are the questions developers ask about
checked-in secrets?
RQ1.2 Which questions related to checked-in secrets
exhibit more unsatisfactory answers?
• RQ1.3 Which questions are the most
• RQ2: What solutions do developers get for mitigating
checked-in secrets?
",https://arxiv.org/pdf/2301.12377.pdf,[]
Silent Vulnerable Dependency Alert Prediction with Vulnerability Key Aspect Explanation,"• RQ1: How is the accuracy of silent dependency alert classification?
• RQ2: How is the accuracy of explainable vulnerability key
aspect generation?
• RQ3: How useful is our explainable silent dependency alert
prediction compared with only binary patch classification?",https://ieeexplore.ieee.org/abstract/document/10172824,[]
Automated Black-box Testing of Mass Assignment Vulnerabilities in RESTful APIs,"RQ1: What is the accuracy of the automated identification of operations CRUD semantics, resource types, and resource-id parameters? Second, the main objective of our approach is to reveal mass assignment vulnerabilities in REST APIs, this is investigated by the following research question.             
RQ2: What is the accuracy in revealing mass assignment vulnerabilities in REST APIs? Finally, our approach should be able to deal with real-world production-ready REST services, that can be complex and large in size. Hence, the last research question investigates the scalability of the approach on large REST APIs. 
RQ3: Does the proposed approach to detect mass assignment vulnerabilities scale to large REST APIs?",https://arxiv.org/abs/2301.01261,"['Mass assignment is one of the most prominent vulnerabilities in RESTful APIs. This vulnerability originates from a misconfiguration in common web frameworks, such that naming convention and automatic binding can be exploited by an attacker to craft malicious requests writing confidential resources and (massively) overriding data, that should be read-only and/or confidential. In this paper, we adopt a black-box testing perspective to automatically detect mass assignment vulnerabilities in RESTful APIs. Execution scenarios are generated purely based on the OpenAPI specification, that lists the available operations and their message format. Clustering is used to group similar operations and reveal read-only fields, the latter are candidate for mass assignment. Then, interaction sequences are automatically generated by instantiating abstract testing templates, trying to exploit the potential vulnerabilities. Finally, test cases are run, and their execution is assessed by a specific oracle, in order to reveal whether the vulnerability could be successfully exploited. The proposed novel approach has been implemented and evaluated on a set of case studies written in different programming languages. The evaluation highlights that the approach is quite effective in detecting seeded vulnerabilities, with a remarkably high accuracy.']"
Compatible Remediation on Vulnerabilities from Third-Party Libraries for Java Projects,"RQ1: How is CORAL compared with other cutting-edge
remediation tools regarding security and compatibility?
RQ2: How effectively does CORAL resolve the challenge of
global optimization by subgraph partitioning?
RQ3: How many vulnerabilities CAN/CANNOT be fixed
without breaking the projects in the Maven ecosystem?",https://ieeexplore.ieee.org/abstract/document/10172542,[]
Sibyl: Improving Software Engineering Tools with SMT Selection,"RQ1: For each domain, what portion of queries is the overall
fastest SMT solver the fastest? Depending on the domain, the
overall fastest solver is the fastest on 2.6% to 38.9% of the
queries.
RQ2: How does Sibyl’s predictions compare to other algorithm selectors on software engineering domains? Sibyl’s
predictions are 37.6% to 159.7% better than existing selectors.
RQ3: How does Sibyl’s overhead affect its performance?
Sibyl’s overhead is non-negligible, but there is evidence to
suggest a more efficient implementation will greatly reduce it.
RQ4: How do the components of Sibyl’s GNN contribute to
its performance? When combined, the core GNN components
of Sibyl – GAT layers and a jumping knowledge layer –
improve Sibyl’s performance substantially",https://ieeexplore.ieee.org/abstract/document/10172504,[]
Repeated Builds During Code Review: An Empirical Study of the OpenStack Community,"RQ1. How often are failing CI jobs rechecked?
RQ2. How often do CI outcomes change after a recheck?
RQ3. How much overhead is generated by rechecking
builds?",https://ieeexplore.ieee.org/abstract/document/10298533,[]
A Large-Scale Empirical Study on Semantic Versioning in Golang Ecosystem,"RQ1: How are semantic versioning compliance applied
in the Go ecosystem in terms of breaking changes?
• RQ2: How much adherence to semantic versioning compliance has increased over time?
• RQ3: What about the impact of breaking changes on
client programs?",https://arxiv.org/abs/2309.02894,"['Third-party libraries (TPLs) have become an essential component of software, accelerating development and reducing maintenance costs. However, breaking changes often occur during the upgrades of TPLs and prevent client programs from moving forward. Semantic versioning (SemVer) has been applied to standardize the versions of releases according to compatibility, but not all releases follow SemVer compliance. Lots of work focuses on SemVer compliance in ecosystems such as Java and JavaScript beyond Golang (Go for short). Due to the lack of tools to detect breaking changes and dataset for Go, developers of TPLs do not know if breaking changes occur and affect client programs, and developers of client programs may hesitate to upgrade dependencies in terms of breaking changes. To bridge this gap, we conduct the first large-scale empirical study in the Go ecosystem to study SemVer compliance in terms of breaking changes and their impact. In detail, we purpose GoSVI (Go Semantic Versioning Insight) to detect breaking changes and analyze their impact by resolving identifiers in client programs and comparing their types with breaking changes. Moreover, we collect the first large-scale Go dataset with a dependency graph from GitHub, including 124K TPLs and 532K client programs. Based on the dataset, our results show that 86.3% of library upgrades follow SemVer compliance and 28.6% of no-major upgrades introduce breaking changes. Furthermore, the tendency to comply with SemVer has improved over time from 63.7% in 2018/09 to 92.2% in 2023/03. Finally, we find 33.3% of downstream client programs may be affected by breaking changes. These findings provide developers and users of TPLs with valuable insights to help make decisions related to SemVer.']"
Hard to Read and Understand Pythonic Idioms? DeIdiom and Explain Them in Non-Idiomatic Equivalent Code,"RQ1: Whatchallenges do pythonic idioms present to Python users in terms of understanding? 
RQ2: Howare pythonic idioms used in real projects? 
RQ3: Howare conciseness of pythonic idioms manifested? 
RQ4: What are the potential negative effects of using pythonic idioms?

RQ1(Accuracy): How accurate is our approach when transforming idiomatic code of nine pythonic idioms into non-idiomatic code? 
RQ2(Usefulness): Is the generated non-idiomatic code useful for understanding pythonic idiom usage?",https://dl.acm.org/doi/abs/10.1145/3597503.3639101,"['The Python community strives to design pythonic idioms so that Python users can achieve their intent in a more concise and efficient way. According to our analysis of 154 questions about challenges of understanding pythonic idioms on Stack Overflow, we find that Python users face various challenges in comprehending pythonic idioms. And the usage of pythonic idioms in 7,577 GitHub projects reveals the prevalence of pythonic idioms. By using a statistical sampling method, we find pythonic idioms result in not only lexical conciseness but also the creation of variables and functions, which indicates it is not straightforward to map back to non-idiomatic code. And usage of pythonic idioms may even cause potential negative effects such as code redundancy, bugs and performance degradation. To alleviate such readability issues and negative effects, we develop a transforming tool, DeIdiom, to automatically transform idiomatic code into equivalent non-idiomatic code. We test and review over 7,572 idiomatic code instances of nine pythonic idioms (list/set/dict-comprehension, chain-comparison, truth-value-test, loop-else, assign-multi-targets, for-multi-targets, star), the result shows the high accuracy of DeIdiom. Our user study with 20 participants demonstrates that explanatory non-idiomatic code generated by DeIdiom is useful for Python users to understand pythonic idioms correctly and efficiently, and leads to a more positive appreciation of pythonic idioms.']"
Semantic GUI Scene Learning and Video Alignment for Detecting Duplicate Video-based Bug Reports,"RQ1: What is Janus𝑣𝑖𝑠’s duplicate detection performance? 
RQ2: What is Janus𝑡𝑥𝑡’s duplicate detection performance? 
RQ3: What is Janus𝑠𝑒𝑞’s duplicate detection performance? 
RQ4: What is the performance of Janus’s component combinations?",https://dl.acm.org/doi/abs/10.1145/3597503.3639163,"['Video-based bug reports are increasingly being used to document bugs for programs centered around a graphical user interface (GUI). However, developing automated techniques to manage video-based reports is challenging as it requires identifying and understanding often nuanced visual patterns that capture key information about a reported bug. In this paper, we aim to overcome these challenges by advancing the bug report management task ofduplicate detectionfor video-based reports. To this end, we introduce a new approach, called Janus, that adapts the scene-learning capabilities of vision transformers to capture subtle visual and textual patterns that manifest on app UI screens --- which is key to differentiating between similar screens for accurate duplicate report detection. Janus also makes use of a video alignment technique capable of adaptive weighting of video frames to account for typical bug manifestation patterns. In a comprehensive evaluation on a benchmark containing 7,290 duplicate detection tasks derived from 270 video-based bug reports from 90 Android app bugs, the best configuration of our approach achieves an overall mRR/mAP of 89.8%/84.7%, and for the large majority of duplicate detection tasks, outperforms prior work by ≈9% to a statistically significant degree. Finally, we qualitatively illustrate how the scene-learning capabilities provided by Janus benefits its performance.']"
Developer-Intent Driven Code Comment Generation,"RQ1 : How does the DOME perform compared to the stateof-the-art comment generation baselines?
RQ2: How does each individual component in DOME
contribute to the overall performance?
RQ3: What is the perceived quality of intent-aware comments generated by DOME?",https://arxiv.org/abs/2302.07055,"['Existing automatic code comment generators mainly focus on producing a general description of functionality for a given code snippet without considering developer intentions. However, in real-world practice, comments are complicated, which often contain information reflecting various intentions of developers, e.g., functionality summarization, design rationale, implementation details, code properties, etc. To bridge the gap between automatic code comment generation and real-world comment practice, we define Developer-Intent Driven Code Comment Generation, which can generate intent-aware comments for the same source code with different intents. To tackle this challenging task, we propose DOME, an approach that utilizes Intent-guided Selective Attention to explicitly select intent-relevant information from the source code, and produces various comments reflecting different intents. Our approach is evaluated on two real-world Java datasets, and the experimental results show that our approach outperforms the state-of-the-art baselines. A human evaluation also confirms the significant potential of applying DOME in practical usage, enabling developers to comment code effectively according to their own needs.']"
Has My Release Disobeyed Semantic Versioning? Static Detection Based On Semantic Differencing,"RQ1:WhatistheaccuracyofSembidintermsofSemBdetection? 
RQ2:HowistheeffectivenessofSembidagainstunittests? 
RQ3:HowdotopJavalibrariescomplywithSemVerrules?",https://dl.acm.org/doi/abs/10.1145/3551349.3556956,"['To enhance the compatibility in the version control of Java Third-party Libraries (TPLs), Maven adopts Semantic Versioning (SemVer) to standardize the underlying meaning of versions, but users could still confront abnormal execution and crash after upgrades even if compilation and linkage succeed. It is caused by semantic breaking (SemB) issues, such that APIs directly used by users have identical signatures but inconsistent semantics across upgrades. To strengthen compliance with SemVer rules, developers and users should be alerted of such issues. Unfortunately, it is challenging to detect them statically, because semantic changes in the internal methods of APIs are difficult to capture. Dynamic testing can confirmingly uncover some, but it is limited by inadequate coverage.', 'To detect SemB issues over compatible upgrades (Patch and Minor) by SemVer rules, we conduct an empirical study on 180 SemB issues to understand the root causes, inspired by which, we propose Sembid (Semantic Breaking Issue Detector) to statically detect such issues of TPLs for developers and users. Since APIs are directly used by users, Sembid detects and reports SemB issues based on APIs. For a pair of APIs, Sembid walks through the call chains originating from the API to locate breaking changes by measuring semantic diff. Then, Sembid checks if the breaking changes can affect API’s output along call chains. The evaluation showed Sembid achieved recall and precision and outperformed other API checkers on SemB API detection. We also revealed Sembid detected over 3 times more SemB APIs with better coverage than unit tests, the commonly used solution. Furthermore, we carried out an empirical study on 1,629,589 APIs from 546 version pairs of top Java libraries and found there were 2 ∼ 4 times more SemB APIs than those with signature-based issues. Due to various version release strategies, of Patch version pairs and of Minor version pairs had at least one API affected by any breaking.']"
Causal Relationships and Programming Outcomes: A Transcranial Magnetic Stimulation Experiment,"RQ1: Can wereplicate prior findings that neurostimulation of the SMA reduces mental rotation completion times? 
RQ2: Is there a direct causal relationship between activity in the SMA(or M1) brain region alone and performance? 
RQ3: Does neurostimulation of the SMA or M1 brain regions affect objective computing performance outcomes? 
RQ4: Does neurostimulation in the SMA or M1 brain region affect self-perceived problem difficulty?",https://dl.acm.org/doi/abs/10.1145/3597503.3639096,"['Understanding the relationship between cognition and programming outcomes is important: it can inform interventions that help novices become experts faster. Neuroimaging techniques can measure brain activity, but prior studies of programming report only correlations. We present the first causal neurological investigation of the cognition of programming by usingTranscranial Magnetic Stimulation(TMS). TMS permits temporary and noninvasive disruption of specific brain regions. By disrupting brain regions and then measuring programming outcomes, we discover whether a true causal relationship exists. To the best of our knowledge, this is the first use of TMS to study software engineering.', 'Where multiple previous studies reported correlations, we find no direct causal relationships between implicated brain regions and programming. Using a protocol that follows TMS best practices and mitigates for biases, we replicate psychology findings that TMS affects spatial tasks. We then find that neurostimulation can affect programming outcomes. Multi-level regression analysis shows that TMS stimulation of different regions significantly accounts for 2.2% of the variance in task completion time. Our results have implications for interventions in education and training as well as research into causal cognitive relationships.']"
How does Simulation-based Testing for Self-driving Cars match Human Perception?,"RQ1: To what extent does the OOB safety metric for simulation-based test cases of SDCs align with human safety assessment?
RQ2: To what extent does the safety assessment of simulation-based SDC test cases vary when humans can interact with the SDC?
RQ3: What are the main reality-gap characteristics perceived by humans in SDC test cases?",https://dl.acm.org/doi/abs/10.1145/3643768,"[""Software metrics such as coverage or mutation scores have been investigated for the automated quality assessment of test suites. While traditional tools rely on software metrics, the field of self-driving cars (SDCs) has primarily focused on simulation-based test case generation using quality metrics such as the out-of-bound (OOB) parameter to determine if a test case fails or passes. However, it remains unclear to what extent this quality metric aligns with the human perception of the safety and realism of SDCs. To address this (reality) gap, we conducted an empirical study involving 50 participants to investigate the factors that determine how humans perceive SDC test cases as safe, unsafe, realistic, or unrealistic. To this aim, we developed a framework leveraging virtual reality (VR) technologies, called SDC-Alabaster, to immerse the study participants into the virtual environment of SDC simulators. Our findings indicate that the human assessment of safety and realism of failing/passing test cases can vary based on different factors, such as the test's complexity and the possibility of interacting with the SDC. Especially for the assessment of realism, the participants' age leads to a different perception. This study highlights the need for more research on simulation testing quality metrics and the importance of human perception in evaluating SDCs.""]"
"A Case Study of Developer Bots: Motivations, Perceptions, and Challenges","RQ1: What processes and needs motivate developers to instrument bots in a large software engineering organization? What challenges do they face in developing the bots? 
RQ2: What is the experience of developers when using various developer bots (bene ts and challenges)? 
RQ3: Howdodevelopersengagewithdi erenttypesofdeveloper bots?",https://2023.esec-fse.org/details/fse-2023-research-papers/7/A-Case-Study-of-Developer-Bots-Motivations-Perceptions-and-Challenges,[]
“STILL AROUND”: Experiences and Survival Strategies of Veteran Women Software Developers,"RQ1. What age- and gender-specific experiences have
veteran software developers of marginalized genders had
in their careers?
• RQ2. What strategies have veteran software developers
of marginalized genders adopted that they perceive as
contributing to their survival in software engineering?",https://arxiv.org/abs/2302.03723,"['The intersection of ageism and sexism can create a hostile environment for veteran software developers belonging to marginalized genders. In this study, we conducted 14 interviews to examine the experiences of people at this intersection, primarily women, in order to discover the strategies they employed in order to successfully remain in the field. We identified 283 codes, which fell into three main categories: Strategies, Experiences, and Perception. Several strategies we identified, such as (Deliberately) Not Trying to Look Younger, were not previously described in the software engineering literature. We found that, in some companies, older women developers are recognized as having particular value, further strengthening the known benefits of diversity in the workforce. Based on the experiences and strategies, we suggest organizations employing software developers to consider the benefits of hiring veteran women software developers. For example, companies can draw upon the life experiences of older women developers in order to better understand the needs of customers from a similar demographic. While we recognize that many of the strategies employed by our study participants are a response to systemic issues, we still consider that, in the short-term, there is benefit in describing these strategies for developers who are experiencing such issues today.']"
"Semi-Automatic, Inline and Collaborative Web Page Code Curations","RQ1: Can our approach help developers identify and curate relevant implicit links between web pages and specific source code locations? 
RQ2: Can developers successfully leverage previouslycurated links on their own change tasks?",https://ieeexplore.ieee.org/abstract/document/10172862,[]
"AI-assisted Code Authoring at Scale: Fine-tuning, deploying, and mixed methods evaluation","RQ1. Model Evaluation: How well does CodeCompose generate one hidden line of code from existing code snippets?
RQ2. Adoption: How many suggestions are accepted by engineers and what proportion of the code is written by CodeCompose?
RQ3. Developer Feedback: How do developers perceive CodeCompose in their daily work?",https://dl.acm.org/doi/abs/10.1145/3643774,"['Generative LLMs have been shown to effectively power AI-based code authoring tools that can suggest entire statements or blocks of code during code authoring. In this paper we present CodeCompose, an AI-assisted code authoring tool developed and deployed at Meta internally. CodeCompose is based on the InCoder LLM that merges generative capabilities with bi-directionality. We have scaled up CodeCompose to serve tens of thousands of developers at Meta, across 9 programming languages and several coding surfaces. We present our experience in making design decisions about the model and system architecture for CodeCompose that addresses these challenges. To release a LLM model at this scale, we needed to first ensure that it is sufficiently accurate. In a random sample of 20K source code files, depending on the language, we are able to reproduce hidden lines between 40% and 58% of the time, an improvement of 1.4× and 4.1× over a model trained only on public data. We gradually rolled CodeCompose out to developers. At the time of this writing, 16K developers have used it with 8% of their code coming directly from CodeCompose. To triangulate our numerical findings, we conduct a thematic analysis on the feedback from 70 developers. We find that 91.5% of the feedback is positive, with the most common themes being discovering APIs, dealing with boilerplate code, and accelerating coding. Meta continues to integrate this feedback into CodeCompose.']"
"GenderMag Improves Discoverability in the Field, Especially for Women","Hypothesis1:Intheversion of Critique that was not designed using GenderMag, discoverability is significantly higher for men than for women.
Hypothesis 2: In the version of Critique that was redesigned using GenderMag,discoverability of Suggest Edit increased, especially for women.",https://www.computer.org/csdl/proceedings-article/icse/2024/021700a973/1V5Bl3jMYVO,[]
SmartCoCo: Checking Comment-code Inconsistency in Smart Contracts via Constraint Propagation and Binding,"RQ1: What is the prevalence of security-related
comment-code inconsistencies in smart contracts?
• RQ2: What is the effectiveness of SmartCoCo in detecting comment-code inconsistencies?
• RQ3: What is the performance in checking a smart
contract with proposed constraints?
• RQ4: Can large language models check CCIs identified
by SmartCoCo?",https://doi.org/10.1109/ASE56229.2023.00142,[]
TRIAD: Automated Traceability Recovery based on Biterm-enhanced Deduction of Transitive Links among Artifacts,"RQ1: To what extent does TRIAD exceed the performance of baseline approaches? 
RQ2: What is the individual impact of biterms, outer- and inner-transitive on performance?",https://arxiv.org/pdf/2312.16854.pdf,[]
A Comprehensive Study on Code Clones in Automated Driving Software,"RQ1: To what extent do code clones occur in automated
driving software?
–If a large number of code clones exist in automated driving
software, meaning code cloning deserves more attention in
such software systems. Hence, we examined the existence of
code clones by calculating their amounts and lines of code
(LOC).
RQ2. Do code clones have a tendency to introduce bugs in
automated driving software?
–In this question, we analyzed whether and to what extent
code clones bring bugs into automated driving software.
RQ3. Do code clones in automated driving software have
co-modifications?
RQ4. How do code clones in autonomous driving software
distribute over different modules? Which modules have more
bug-prone and co-modified clones?",https://github.com/vvioletNego/ccParser/blob/master/ase23-main-575.pdf,[]
Too Much Accessibility is Harmful! Automated Detection and Analysis of Overly Accessible Elements in Mobile Apps,"RQ1. Howaccurate is OverSight in detecting OA elements? 
RQ2. How prevalent are over-access problems in securityconcerned apps? 
RQ3. What are the potential impacts of OA elements on different apps and communities? 
RQ4. What is the performance of OverSight?",https://dl.acm.org/doi/abs/10.1145/3551349.3560424,"['Mobile apps, an essential technology in today’s world, should provide equal access to all, including 15% of the world population with disabilities. Assistive Technologies\xa0(AT), with the help of Accessibility APIs, provide alternative ways of interaction with apps for disabled users who cannot see or touch the screen. Prior studies have shown that mobile apps are prone to the under-access problem, i.e., a condition in which functionalities in an app are not accessible to disabled users, even with the use of ATs. We study the dual of this problem, called the over-access problem, and defined as a condition in which an AT can be used to gain access to functionalities in an app that are inaccessible otherwise. Over-access has severe security and privacy implications, allowing one to bypass protected functionalities using ATs, e.g., using VoiceOver to read notes on a locked phone. Over-access also degrades the accessibility of apps by presenting to disabled users information that is actually not intended to be available on a screen, thereby confusing and hindering their ability to effectively navigate. In this work, we first empirically study overly accessible elements in Android apps and define a set of conditions that can result in over-access problem. We then present OverSight, an automated framework that leverages these conditions to detect overly accessible elements and verifies their accessibility dynamically using an AT. Our empirical evaluation of OverSight on real-world apps demonstrates OverSight’s effectiveness in detecting previously unknown security threats, workflow violations, and accessibility issues.']"
A Qualitative Study on the Implementation Design Decisions of Developers,"RQ1: What implementation design decisions do software
developers make?
• RQ2: What considerations do software developers have
while making implementation design decisions?
• RQ3: What process do software developers follow to
make implementation design decisions?
• RQ4: Which types of developer expertise are described
in the implementation decision-making process?",https://arxiv.org/abs/2301.09789,"['Decision-making is a key software engineering skill. Developers constantly make choices throughout the software development process, from requirements to implementation. While prior work has studied developer decision-making, the choices made while choosing what solution to write in code remain understudied. In this mixed-methods study, we examine the phenomenon where developers select one specific way to implement a behavior in code, given many potential alternatives. We call these decisions implementation design decisions. Our mixed-methods study includes 46 survey responses and 14 semi-structured interviews with professional developers about their decision types, considerations, processes, and expertise for implementation design decisions. We find that implementation design decisions, rather than being a natural outcome from higher levels of design, require constant monitoring of higher level design choices, such as requirements and architecture. We also show that developers have a consistent general structure to their implementation decision-making process, but no single process is exactly the same. We discuss the implications of our findings on research, education, and practice, including insights on teaching developers how to make implementation design decisions.']"
Groundhog: An Automated Accessibility Crawler for Mobile Apps,"RQ1.HoweffectiveisGroundhogindetectingaccessibilityissues? 
RQ2.HowdoesGroundhogcomparetoGoogleAccessibility Scanner(theofficialaccessibilitytestingtoolinAndroid)? 
RQ3.Whatarethecharacteristicsof thedetectedaccessibility issues?Howdotheyimpactappusageforuserswithdisabilities? 
RQ4.WhatistheperformanceofGroundhog?Towhatextent optimizationimprovesitsperformance?",https://dl.acm.org/doi/abs/10.1145/3551349.3556905,"['Accessibility is a critical software quality affecting more than 15% of the world’s population with some form of disabilities. Modern mobile platforms, i.e., iOS and Android, provide guidelines and testing tools for developers to assess the accessibility of their apps. The main focus of the testing tools is on examining a particular screen’s compliance with some predefined rules derived from accessibility guidelines. Unfortunately, these tools cannot detect accessibility issues that manifest themselves in interactions with apps using assistive services, e.g., screen readers. A few recent studies have proposed assistive-service driven testing; however, they require manually constructed inputs from developers to evaluate a specific screen or presume availability of UI test cases. In this work, we propose an automated accessibility crawler for mobile apps, Groundhog, that explores an app with the purpose of finding accessibility issues without any manual effort from developers. Groundhog assesses the functionality of UI elements in an app with and without assistive services and pinpoints accessibility issues with an intuitive video of how to replicate them. Our experiments show Groundhog is highly effective in detecting accessibility barriers that existing techniques cannot discover. Powered by Groundhog, we conducted an empirical study on a large set of real-world apps and found new classes of critical accessibility issues that should be the focus of future work in this area.']"
Generating Critical Test Scenarios for Autonomous Driving Systems via Influential Behavior Patterns,"RQ1:Howeffective is CRISCO in finding safety violations of ADS? 
RQ2: How effective and efficient is CRISCO to expose safety violations compared to existing state-of-the-art technique? 
RQ3: Is CRISCO able to generate more types of safety-violation scenarios which are different from the traffic collisions occurred in the selected datasets inD and Stanford Drone?",https://dl.acm.org/doi/abs/10.1145/3551349.3560430,"['Autonomous Driving Systems (ADSs) are safety-critical, and must be fully tested before being deployed on real-world roads. To comprehensively evaluate the performance of ADSs, it is essential to generate various safety-critical scenarios. Most of existing studies assess ADSs either by searching high-dimensional input space, or using simple and pre-defined test scenarios, which are not efficient or not adequate. To better test ADSs, this paper proposes to automatically generate safety-critical test scenarios for ADSs by influential behavior patterns, which are mined from real traffic trajectories. Based on influential behavior patterns, a novel scenario generation technique, CRISCO, is presented to generate safety-critical scenarios for ADSs testing. CRISCO assigns participants to perform influential behaviors to challenge the ADS. It generates different test scenarios by solving trajectory constraints, and improves the challenge of those non-critical scenarios by adding participants’ behavior from influential behavior patterns incrementally. We demonstrate CRISCO on an industrial-grade ADS platform, Baidu Apollo. The experiment results show that our approach can effectively and efficiently generate critical scenarios to crash ADS, and it exposes 13 distinct types of safety violations in 12 hours. It also outperforms two state-of-art ADS testing techniques by exposing more 5 distinct types of safety violations on the same roads.']"
Analyzing and Debugging Normative Requirements via Satisfiability Checking,Howeffective is LEGOS-SLEEC in detecting WFIs?,https://dl.acm.org/doi/abs/10.1145/3597503.3639093,"['As software systems increasingly interact with humans in application domains such as transportation and healthcare, they raise concerns related to the social, legal, ethical, empathetic, and cultural (SLEEC) norms and values of their stakeholders.Normative non-functional requirements(N-NFRs) are used to capture these concerns by setting SLEEC-relevant boundaries for system behavior. Since N-NFRs need to be specified by multiple stakeholders with widely different, non-technical expertise (ethicists, lawyers, regulators, end users, etc.), N-NFR elicitation is very challenging. To address this difficult task, we introduce N-Check, a novel tool-supported formal approach to N-NFR analysis and debugging. N-Check employs satisfiability checking to identify a broad spectrum of N-NFR well-formedness issues, such as conflicts, redundancy, restrictiveness, and insufficiency, yielding diagnostics that pinpoint their causes in a user-friendly way that enables non-technical stakeholders to understand and fix them. We show the effectiveness and usability of our approach through nine case studies in which teams of ethicists, lawyers, philosophers, psychologists, safety analysts, and engineers used N-Check to analyse and debug 233 N-NFRs, comprising 62 issues for the software underpinning the operation of systems, such as, assistive-care robots and tree-disease detection drones to manufacturing collaborative robots.']"
AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models,"–RQ1:CantheAST-Probelearn to parse on top of any informative code representation 4?
– RQ2: Which pre-trained language model best encodes the AST in its hidden representations? Wecompare a total of six pre-trained language models for three programming languages and assess which one best encodes the AST of input codes.
– RQ3: What layers of the pre-trained language models encode the AST better? Weapply our probe to specific hidden representation spaces of intermediate layers of the models and compare the probe effectiveness over the layers.
– RQ4: What is the dimension of the syntactic subspace S? To end our experiments, we are interested in how compact is the syntactic subspace in the hidden representation spaces of the pre-trained language models.",https://arxiv.org/pdf/2206.11719.pdf,[]
Mutation-based Fault Localization of Deep Neural Networks,"• RQ1 (Effectiveness):
1) How does deepmufl compare to state-of-the-art tools
in terms of the number of bugs detected?
2) How many bugs does deepmufl detect from each subcategory of model bugs in our dataset and how does
that compare to state-of-the-art tools?
3) What are the overlap of detected bugs among deepmufl and other fault localization techniques?
• RQ2 (Efficiency):
1) What is the impact of mutation selection on the
effectiveness and efficiency of deepmufl?
2) How does deepmufl compare to state-of-the-art tools
in terms of end-to-end fault localization time?",https://arxiv.org/abs/2309.05067,"['Deep neural networks (DNNs) are susceptible to bugs, just like other types of software systems. A significant uptick in using DNN, and its applications in wide-ranging areas, including safety-critical systems, warrant extensive research on software engineering tools for improving the reliability of DNN-based systems. One such tool that has gained significant attention in the recent years is DNN fault localization. This paper revisits mutation-based fault localization in the context of DNN models and proposes a novel technique, named deepmufl, applicable to a wide range of DNN models. We have implemented deepmufl and have evaluated its effectiveness using 109 bugs obtained from StackOverflow. Our results show that deepmufl detects 53/109 of the bugs by ranking the buggy layer in top-1 position, outperforming state-of-the-art static and dynamic DNN fault localization systems that are also designed to target the class of bugs supported by deepmufl. Moreover, we observed that we can halve the fault localization time for a pre-trained model using mutation selection, yet losing only 7.55% of the bugs localized in top-1 position.']"
Out of Context: How important is Local Context in Neural Program Repair?,"RQ1. How important is local context for repair success? We study multiple context sizes, ranging from a single line up to 28 lines on both sides (56 lines) on three datasets (MegaDiff [21], TSSB [29], and ManySStuBs4J [10], see Table 1), totalling several hundreds of thousands of bugs. 
RQ2. Howdodifferentbugtypesandcomplexity(numberofchanges) respond to different context sizes and context window positions? Both, MaySStuBs4J [10] andTSSM-3M[29]classifybugsinto several bug types or bug patterns. We use this bug type labels to analyze how context size affects repair success for bugs of different types. We perform a similar analysis also for the number of changes of a bugfix. 
RQ3. What is the optimal context window position? In other words, given a fixed context budget, how should it be divided among pre-context and post-context? We experiment with six different context window positions (for four different context window sizes), from only pre-context over several combinations to only post-context. 
RQ4. Is there a connection between the model size (number of parameters), the number of sampled fix candidates and context? With more context, the amount of fix ingredients increases. Wehypothesize that in order to fully exploit context, model size should increase, as should the number of samples. We investigate if this is indeed the case.",https://dl.acm.org/doi/abs/10.1145/3597503.3639086,"['Deep learning source code models have been applied very successfully to the problem of automated program repair. One of the standing issues is the small input window of current models which often cannot fully fit the context code required for a bug fix (e.g., method or class declarations of a project). Instead, input is often restricted to the local context, that is, the lines below and above the bug location. In this work we study the importance of this local context on repair success: how much local context is needed?; is context before or after the bug location more important? how is local context tied to the bug type? To answer these questions we train and evaluate Transformer models in many different local context configurations on three datasets and two programming languages. Our results indicate that overall repair success increases with the size of the local context (albeit not for all bug types) and confirm the common practice that roughly 50--60% of the input window should be used for context leading the bug. Our results are not only relevant for researchers working on Transformer-based APR tools but also for benchmark and dataset creators who must decide what and how much context to include in their datasets.']"
Exploring Experiences with Automated Program Repair in Practice,"RQ1 What factors influence the awareness and adoption or use of APR in practice? 
RQ2 To what extent are APR tool(s) being used in practice compared to other forms of support? 
RQ3 Whatare the human-centric challenges faced by developers that can prevent the widespread use of APR tools?",https://dl.acm.org/doi/abs/10.1145/3597503.3639182,"[""Automated program repair, also known as APR, is an approach for automatically repairing software faults. There is a large amount of research on automated program repair, but very little offers in-depth insights into how practitioners think about and employ APR in practice. To learn more about practitioners' perspectives and experiences with current APR tools and techniques, we administered a survey, which received valid responses from 331 software practitioners. We analyzed survey responses to gain insights regarding factors that correlate with APR awareness, experience, and use. We established a strong correlation between APR awareness and tool use and attributes including job position, company size, total coding experience, and preferred language of software practitioners. We also found that practitioners are using other forms of support, such as co-workers and ChatGPT, more frequently than APR tools when fixing software defects. We learned about the drawbacks that practitioners encounter while utilizing existing APR tools and the impact that each drawback has on their practice. Our findings provide implications for research and practice centered on development, adoption, and use of APR.""]"
Understanding and Detecting On-the-Fly Configuration Bugs,"RQ1: What are the common symptoms of OCBugs? 
 RQ2: What are the root causes of OCBugs?
RQ3: What are the triggering conditions of OCBugs?
bzw. (aus den oberen RQs wurde ein Tool entiwckelt und mit den folgenden RQs untersucht)
• RQ1: How effective is PARACHUTE in detecting
known OCBugs? This question examines the recall of
PARACHUTE by calculating the percentage of bugs that
can be detected among all known bugs.
• RQ2: How effective is PARACHUTE in detecting unknown OCBugs? This question evaluates the precision
of PARACHUTE by calculating the percentage of true
positives among all reported bugs.
• RQ3: Can PARACHUTE outperform the state-of-the-art
tool for detecting configuration update bugs? This question compares PARACHUTE with Staccato, the most related work for detecting OCBugs.",https://leopard-lab.github.io/paper/icse23-Parachute.pdf,[]
Accelerating Continuous Integration with Parallel Batch Testing,"RQ1:Howdoesparallelization affect the feedback time performance of TestAll with varying numbers of machines? 
RQ2: How effective is ConstantBatching in terms of feedback time and execution reduction when executed in parallel with varying numbers of machines? 
RQ3: How effective is BatchAll in terms of feedback time and execution reduction when executed in parallel with varying numbers of machines? 
RQ4: How effective is TestCaseBatching in terms of feedback time and execution reduction when executed in parallel with varying numbers of machines? Our major contributions to this study are as follows.",https://arxiv.org/pdf/2308.13129,[]
PyTy: Repairing Static Type Errors in Python,"RQ1 Howeffective is our automated data gathering at producing minimal code changes that fix type errors? 
RQ2 Howeffective is PyTy at fixing type errors? 
RQ3 Howdovariants of PyTy compare to the full approach? 
RQ4 Howdoes PyTy compare to state-of-the-art APR techniques?",https://dl.acm.org/doi/10.1145/3597503.3639184,"['Gradual typing enables developers to annotate types of their own choosing, offering a flexible middle ground between no type annotations and a fully statically typed language. As more and more code bases get type-annotated, static type checkers detect an increasingly large number of type errors. Unfortunately, fixing these errors requires manual effort, hampering the adoption of gradual typing in practice. This paper presents PyTy, an automated program repair approach targeted at statically detectable type errors in Python. The problem of repairing type errors deserves specific attention because it exposes particular repair patterns, offers a warning message with hints about where and how to apply a fix, and because gradual type checking serves as an automatic way to validate fixes. We addresses this problem through three contributions: (i) an empirical study that investigates how developers fix Python type errors, showing a diverse set of fixing strategies with some recurring patterns; (ii) an approach to automatically extract type error fixes, which enables us to create a dataset of 2,766 error-fix pairs from 176 GitHub repositories, named PyTyDefects; (iii) the first learning-based repair technique for fixing type errors in Python. Motivated by the relative data scarcity of the problem, the neural model at the core of PyTy is trained via cross-lingual transfer learning. Our evaluation shows that PyTy offers fixes for ten frequent categories of type errors, successfully addressing 85.4% of 281 real-world errors. This effectiveness outperforms state-of-the-art large language models asked to repair type errors (by 2.1x) and complements a previous technique aimed at type errors that manifest at runtime. Finally, 20 out of 30 pull requests with PyTy-suggested fixes have been merged by developers, showing the usefulness of PyTy in practice.']"
EndWatch: A Practical Method for Detecting Non-Termination in Real-World Software,"• RQ1: How effective is EndWatch on existing benchmark
programs compared with the state-of-the-art tools?
• RQ2: How effective is EndWatch on detecting CVEs in
real world programs?
• RQ3: How useful is EndWatch in finding zero-day nontermination bugs?",https://personal.ntu.edu.sg/yi_li/files/Zhang2023EAP.pdf,[]
