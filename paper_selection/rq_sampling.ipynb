{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling of RQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "import pprint\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_papers_csv(csv_file):\n",
    "    \"\"\"\n",
    "    Parses a CSV file containing information about research papers and organizes it into a structured dictionary.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): The path to the CSV file containing the paper data. The CSV file should have the following columns:\n",
    "            - 'Domain': The domain or category of the paper.\n",
    "            - 'Paper Name': The title of the paper.\n",
    "            - 'Authors': A comma-separated list of authors.\n",
    "            - 'Distinguished': A string indicating whether the paper is distinguished ('True' or 'False').\n",
    "\n",
    "    Returns:\n",
    "        defaultdict: A nested dictionary where each domain key maps to a dictionary containing:\n",
    "            - 'papers': A list of dictionaries, each representing a paper with the following keys:\n",
    "                - 'name' (str): The title of the paper.\n",
    "                - 'authors' (str): A comma-separated string of author names.\n",
    "                - 'distinguished' (bool): A boolean indicating if the paper is distinguished.\n",
    "                - 'url' (str): A placeholder string ('No URL available') for the paper's URL.\n",
    "    \"\"\"\n",
    "    merged_dict_all = defaultdict(lambda: {'papers': []})\n",
    "\n",
    "    with open(csv_file, mode='r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            domain = row['Domain']\n",
    "            paper_name = row['Paper Name']\n",
    "            \n",
    "            authors = [author.strip() for author in row['Authors'].split(',')]\n",
    "            authors = ', '.join(authors)\n",
    "            \n",
    "            distinguished = row['Distinguished'] == 'True'\n",
    "\n",
    "            paper_entry = {\n",
    "                'name': paper_name,\n",
    "                'authors': authors,\n",
    "                'distinguished': distinguished,\n",
    "                'url': url\n",
    "            }\n",
    "\n",
    "            merged_dict_all[domain]['papers'].append(paper_entry)\n",
    "\n",
    "    return merged_dict_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_papers_by_domain(merged_dict_all):\n",
    "    \"\"\"\n",
    "    Prints a list of papers organized by domain in a readable format.\n",
    "\n",
    "    Args:\n",
    "        merged_dict_all (dict): A dictionary where each key is a domain, and the value is a dictionary containing:\n",
    "            - 'papers': A list of dictionaries, each representing a paper with the following keys:\n",
    "                - 'name' (str): The title of the paper.\n",
    "                - 'authors' (str): A comma-separated string of author names.\n",
    "                - 'distinguished' (bool): A boolean indicating if the paper is distinguished.\n",
    "                - 'url' (str): A placeholder string for the paper's URL.\n",
    "    \"\"\"\n",
    "    # Iterate over each domain in merged_dict_all\n",
    "    for domain, domain_data in merged_dict_all.items():\n",
    "        print(f\"Domain: {domain}\")\n",
    "        # Iterate over the list of papers in each domain\n",
    "        for paper in domain_data['papers']:\n",
    "            print(f\"  Paper Name: {paper['name']}\")\n",
    "            #print(f\"    Authors: {paper['authors']}\")\n",
    "            print(f\"    Distinguished: {paper['distinguished']}\")\n",
    "            #print(f\"    URL: {paper['url']}\")\n",
    "        print(\"\\n\")  # Print a newline between domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'csv_files/papers_by_domain.csv'\n",
    "merged_dict_all = parse_papers_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_papers_by_domain(merged_dict_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_papers(paper_dict):\n",
    "    \"\"\"\n",
    "    Counts the total number of papers in the dictionary.\n",
    "\n",
    "    Args:\n",
    "        paper_dict (defaultdict): A dictionary with paper data, including \"papers\" as a key.\n",
    "\n",
    "    Returns:\n",
    "        int: The total number of papers in the dictionary.\n",
    "    \"\"\"\n",
    "    total_papers = 0\n",
    "    for domain, data in paper_dict.items():\n",
    "        if \"papers\" in data:\n",
    "            total_papers += len(data[\"papers\"])\n",
    "    return total_papers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of papers\n",
    "total_papers = count_papers(merged_dict_all)\n",
    "print(total_papers)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_to_remove = [\n",
    "    \"Improving the Learning of Code Review Successive Tasks with Cross-Task Knowledge Distillation\",\n",
    "    \"Can Machine Learning Pipelines Be Better Configured?\",\n",
    "    \"Benchmarking Robustness of AI-enabled Multi-sensor Fusion Systems: Challenges and Opportunities\",\n",
    "    \"An Empirical Study on Noisy Label Learning for Program Understanding\",\n",
    "    \"EGFE: End-to-end Grouping of Fragmented Elements in UI Designs with Multimodal Learning\",\n",
    "    \"Harnessing Neuron Stability to Improve DNN Verification\",\n",
    "    \"Towards Finding Accounting Errors in Smart Contracts\",\n",
    "    \"Detecting Blocking Errors in Go Programs using Localized Abstract Interpretation\",\n",
    "    \"NeuRI: Diversifying DNN Generation via Inductive Rule Inference\",\n",
    "    \"Is unsafe an Achilles' Heel? A Comprehensive Study of Safety Requirements in Unsafe Rust Programming\",\n",
    "    \"Generating Realistic and Diverse Tests for LiDAR-Based Perception Systems\",\n",
    "    \"Provably Tightest Linear Approximation for Robustness Verification of Sigmoid-like Neural Networks\",\n",
    "    \"Modularizing while Training: a New Paradigm for Modularizing DNN Models\",\n",
    "    \"A Highly Scalable, Hybrid, Cross-Platform Timing Analysis Framework Providing Accurate Differential Throughput Estimation via Instruction-Level Tracing\",\n",
    "    \"Code Search is All You Need? Improving Code Suggestions with Code Search\",\n",
    "    \"GrACE: Language Models Meet Code Edits\",\n",
    "    \"Using Deep Learning to Automatically Improve Code Readability\",\n",
    "    \"When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference\",\n",
    "    \"Sibyl: Improving Software Engineering Tools with SMT Selection\",\n",
    "    \"HyperAST: Enabling Efficient Analysis of Software Histories at Scale\",\n",
    "    \"DeepScaler: Holistic Autoscaling for Microservices Based on Spatiotemporal GNN with Adaptive Graph Learning\",\n",
    "    \"On Using GUI Interaction Data to Improve Text Retrieval-based Bug Localization\",\n",
    "    \"Repeated Builds During Code Review: An Empirical Study of the OpenStack Community\",\n",
    "    \"A Large-Scale Empirical Study on Semantic Versioning in Golang Ecosystem\",\n",
    "    \"A Qualitative Study on the Implementation Design Decisions of Developers\",\n",
    "    \"Analyzing and Debugging Normative Requirements via Satisfiability Checking\",\n",
    "    \"Compiler Testing using Template Java Programs\",\n",
    "    \"ReFAIR: Toward a Context-Aware Recommender for Fairness Requirements Engineering\",\n",
    "    \"TRIAD: Automated Traceability Recovery based on Biterm-enhanced Deduction of Transitive Links among Artifacts\",\n",
    "    \"BSHUNTER: Detecting and Tracing Defects of Bitcoin Scripts\",\n",
    "    \"A Longitudinal Study of Student Contributions to OSS vs. OSS4SG with a Lightweight Intervention\",\n",
    "    \"Do I Belong? Modeling Sense of Virtual Community Among Linux Kernel Contributors\",\n",
    "    \"GenderMag Improves Discoverability in the Field, Especially for Women\",\n",
    "    \"[Remote] Understanding the topics and challenges of GPU programming by classifying and analyzing Stack Overflow posts\",\n",
    "    \"Towards Greener Yet Powerful Code Generation via Quantization: An Empirical Study\",\n",
    "    \"How Are Paid and Volunteer Open Source Developers Different? A Study of the Rust Project\",\n",
    "    \"Mate! Are You Really Aware? An Explainability-Guided Testing Framework for Robustness of Malware Detectors\",\n",
    "    \"Lejacon: A Lightweight and Efficient Approach to Java Confidential Computing on SGX\",\n",
    "    \"Static Application Security Testing (SAST) Tools for Smart Contracts: How Far Are We?\",\n",
    "    \"Leveraging Practitioners' Feedback to Improve a Security Linter\",\n",
    "    \"Automated Black-box Testing of Mass Assignment Vulnerabilities in RESTful APIs\",\n",
    "    \"ViaLin: Path-Aware Dynamic Taint Analysis for Android\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improving the Learning of Code Review Successive Tasks with Cross-Task Knowledge Distillation: Found\n",
      "\n",
      "Can Machine Learning Pipelines Be Better Configured?: Found\n",
      "\n",
      "Benchmarking Robustness of AI-enabled Multi-sensor Fusion Systems: Challenges and Opportunities: Found\n",
      "\n",
      "An Empirical Study on Noisy Label Learning for Program Understanding: Found\n",
      "\n",
      "EGFE: End-to-end Grouping of Fragmented Elements in UI Designs with Multimodal Learning: Found\n",
      "\n",
      "Harnessing Neuron Stability to Improve DNN Verification: Found\n",
      "\n",
      "Towards Finding Accounting Errors in Smart Contracts: Found\n",
      "\n",
      "Detecting Blocking Errors in Go Programs using Localized Abstract Interpretation: Found\n",
      "\n",
      "NeuRI: Diversifying DNN Generation via Inductive Rule Inference: Found\n",
      "\n",
      "Is unsafe an Achilles' Heel? A Comprehensive Study of Safety Requirements in Unsafe Rust Programming: Found\n",
      "\n",
      "Generating Realistic and Diverse Tests for LiDAR-Based Perception Systems: Found\n",
      "\n",
      "Provably Tightest Linear Approximation for Robustness Verification of Sigmoid-like Neural Networks: Found\n",
      "\n",
      "Modularizing while Training: a New Paradigm for Modularizing DNN Models: Found\n",
      "\n",
      "A Highly Scalable, Hybrid, Cross-Platform Timing Analysis Framework Providing Accurate Differential Throughput Estimation via Instruction-Level Tracing: Found\n",
      "\n",
      "Code Search is All You Need? Improving Code Suggestions with Code Search: Found\n",
      "\n",
      "GrACE: Language Models Meet Code Edits: Found\n",
      "\n",
      "Using Deep Learning to Automatically Improve Code Readability: Found\n",
      "\n",
      "When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference: Found\n",
      "\n",
      "Sibyl: Improving Software Engineering Tools with SMT Selection: Found\n",
      "\n",
      "HyperAST: Enabling Efficient Analysis of Software Histories at Scale: Found\n",
      "\n",
      "DeepScaler: Holistic Autoscaling for Microservices Based on Spatiotemporal GNN with Adaptive Graph Learning: Found\n",
      "\n",
      "On Using GUI Interaction Data to Improve Text Retrieval-based Bug Localization: Found\n",
      "\n",
      "Repeated Builds During Code Review: An Empirical Study of the OpenStack Community: Found\n",
      "\n",
      "A Large-Scale Empirical Study on Semantic Versioning in Golang Ecosystem: Found\n",
      "\n",
      "A Qualitative Study on the Implementation Design Decisions of Developers: Found\n",
      "\n",
      "Analyzing and Debugging Normative Requirements via Satisfiability Checking: Found\n",
      "\n",
      "Compiler Testing using Template Java Programs: Found\n",
      "\n",
      "ReFAIR: Toward a Context-Aware Recommender for Fairness Requirements Engineering: Found\n",
      "\n",
      "TRIAD: Automated Traceability Recovery based on Biterm-enhanced Deduction of Transitive Links among Artifacts: Found\n",
      "\n",
      "BSHUNTER: Detecting and Tracing Defects of Bitcoin Scripts: Found\n",
      "\n",
      "A Longitudinal Study of Student Contributions to OSS vs. OSS4SG with a Lightweight Intervention: Found\n",
      "\n",
      "Do I Belong? Modeling Sense of Virtual Community Among Linux Kernel Contributors: Found\n",
      "\n",
      "GenderMag Improves Discoverability in the Field, Especially for Women: Found\n",
      "\n",
      "[Remote] Understanding the topics and challenges of GPU programming by classifying and analyzing Stack Overflow posts: Found\n",
      "\n",
      "Towards Greener Yet Powerful Code Generation via Quantization: An Empirical Study: Found\n",
      "\n",
      "How Are Paid and Volunteer Open Source Developers Different? A Study of the Rust Project: Found\n",
      "\n",
      "Mate! Are You Really Aware? An Explainability-Guided Testing Framework for Robustness of Malware Detectors: Found\n",
      "\n",
      "Lejacon: A Lightweight and Efficient Approach to Java Confidential Computing on SGX: Found\n",
      "\n",
      "Static Application Security Testing (SAST) Tools for Smart Contracts: How Far Are We?: Found\n",
      "\n",
      "Leveraging Practitioners' Feedback to Improve a Security Linter: Found\n",
      "\n",
      "Automated Black-box Testing of Mass Assignment Vulnerabilities in RESTful APIs: Found\n",
      "\n",
      "ViaLin: Path-Aware Dynamic Taint Analysis for Android: Found\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Improving the Learning of Code Review Successive Tasks with Cross-Task Knowledge Distillation': True,\n",
       " 'Can Machine Learning Pipelines Be Better Configured?': True,\n",
       " 'Benchmarking Robustness of AI-enabled Multi-sensor Fusion Systems: Challenges and Opportunities': True,\n",
       " 'An Empirical Study on Noisy Label Learning for Program Understanding': True,\n",
       " 'EGFE: End-to-end Grouping of Fragmented Elements in UI Designs with Multimodal Learning': True,\n",
       " 'Harnessing Neuron Stability to Improve DNN Verification': True,\n",
       " 'Towards Finding Accounting Errors in Smart Contracts': True,\n",
       " 'Detecting Blocking Errors in Go Programs using Localized Abstract Interpretation': True,\n",
       " 'NeuRI: Diversifying DNN Generation via Inductive Rule Inference': True,\n",
       " \"Is unsafe an Achilles' Heel? A Comprehensive Study of Safety Requirements in Unsafe Rust Programming\": True,\n",
       " 'Generating Realistic and Diverse Tests for LiDAR-Based Perception Systems': True,\n",
       " 'Provably Tightest Linear Approximation for Robustness Verification of Sigmoid-like Neural Networks': True,\n",
       " 'Modularizing while Training: a New Paradigm for Modularizing DNN Models': True,\n",
       " 'A Highly Scalable, Hybrid, Cross-Platform Timing Analysis Framework Providing Accurate Differential Throughput Estimation via Instruction-Level Tracing': True,\n",
       " 'Code Search is All You Need? Improving Code Suggestions with Code Search': True,\n",
       " 'GrACE: Language Models Meet Code Edits': True,\n",
       " 'Using Deep Learning to Automatically Improve Code Readability': True,\n",
       " 'When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference': True,\n",
       " 'Sibyl: Improving Software Engineering Tools with SMT Selection': True,\n",
       " 'HyperAST: Enabling Efficient Analysis of Software Histories at Scale': True,\n",
       " 'DeepScaler: Holistic Autoscaling for Microservices Based on Spatiotemporal GNN with Adaptive Graph Learning': True,\n",
       " 'On Using GUI Interaction Data to Improve Text Retrieval-based Bug Localization': True,\n",
       " 'Repeated Builds During Code Review: An Empirical Study of the OpenStack Community': True,\n",
       " 'A Large-Scale Empirical Study on Semantic Versioning in Golang Ecosystem': True,\n",
       " 'A Qualitative Study on the Implementation Design Decisions of Developers': True,\n",
       " 'Analyzing and Debugging Normative Requirements via Satisfiability Checking': True,\n",
       " 'Compiler Testing using Template Java Programs': True,\n",
       " 'ReFAIR: Toward a Context-Aware Recommender for Fairness Requirements Engineering': True,\n",
       " 'TRIAD: Automated Traceability Recovery based on Biterm-enhanced Deduction of Transitive Links among Artifacts': True,\n",
       " 'BSHUNTER: Detecting and Tracing Defects of Bitcoin Scripts': True,\n",
       " 'A Longitudinal Study of Student Contributions to OSS vs. OSS4SG with a Lightweight Intervention': True,\n",
       " 'Do I Belong? Modeling Sense of Virtual Community Among Linux Kernel Contributors': True,\n",
       " 'GenderMag Improves Discoverability in the Field, Especially for Women': True,\n",
       " '[Remote] Understanding the topics and challenges of GPU programming by classifying and analyzing Stack Overflow posts': True,\n",
       " 'Towards Greener Yet Powerful Code Generation via Quantization: An Empirical Study': True,\n",
       " 'How Are Paid and Volunteer Open Source Developers Different? A Study of the Rust Project': True,\n",
       " 'Mate! Are You Really Aware? An Explainability-Guided Testing Framework for Robustness of Malware Detectors': True,\n",
       " 'Lejacon: A Lightweight and Efficient Approach to Java Confidential Computing on SGX': True,\n",
       " 'Static Application Security Testing (SAST) Tools for Smart Contracts: How Far Are We?': True,\n",
       " \"Leveraging Practitioners' Feedback to Improve a Security Linter\": True,\n",
       " 'Automated Black-box Testing of Mass Assignment Vulnerabilities in RESTful APIs': True,\n",
       " 'ViaLin: Path-Aware Dynamic Taint Analysis for Android': True}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def are_papers_in_dict(paper_dict, paper_names):\n",
    "    \"\"\"\n",
    "    Checks whether papers with the given names exist in the dictionary.\n",
    "\n",
    "    Args:\n",
    "        paper_dict (defaultdict): A dictionary with paper data, including \"papers\" as a key.\n",
    "        paper_names (list): A list of paper names to search for.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each paper name to True if it exists, False otherwise.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    # Flatten the list of all paper names in the dictionary for quick lookup\n",
    "    existing_papers = set(\n",
    "        paper[\"name\"] for domain, data in paper_dict.items() if \"papers\" in data\n",
    "        for paper in data[\"papers\"]\n",
    "    )\n",
    "\n",
    "    # Check if each paper name is in the existing papers set\n",
    "    for paper_name in paper_names:\n",
    "        result[paper_name] = paper_name in existing_papers\n",
    "\n",
    "    # Print each entry with a line break\n",
    "    for paper_name, exists in result.items():\n",
    "        print(f\"{paper_name}: {'Found' if exists else 'Not Found'}\\n\")\n",
    "\n",
    "    return result\n",
    "\n",
    "are_papers_in_dict(merged_dict_all, papers_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling of the Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 6 papers per domain (3 distinguished and 3 not distinguished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_papers(merged_dict_all):\n",
    "    \"\"\"\n",
    "    Randomly samples up to 6 papers per domain from the given dataset.\n",
    "\n",
    "    Args:\n",
    "        merged_dict_all (dict): A dictionary where each domain maps to a dictionary containing a list of papers.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the same domain keys, where each domain maps to a list of up to 6 sampled papers \n",
    "              (3 distinguished and 3 non-distinguished, if available).\n",
    "    \"\"\"\n",
    "    sampled_papers = {}\n",
    "\n",
    "    # Iterate over each domain in merged_dict_all\n",
    "    for domain, domain_data in merged_dict_all.items():\n",
    "        # Separate papers into distinguished and non-distinguished lists\n",
    "        distinguished_papers = [paper for paper in domain_data['papers'] if paper['distinguished']]\n",
    "        non_distinguished_papers = [paper for paper in domain_data['papers'] if not paper['distinguished']]\n",
    "\n",
    "        # Sample 3 distinguished papers and 3 non-distinguished papers (if possible)\n",
    "        sampled_distinguished = random.sample(distinguished_papers, min(3, len(distinguished_papers)))\n",
    "        sampled_non_distinguished = random.sample(non_distinguished_papers, min(3, len(non_distinguished_papers)))\n",
    "\n",
    "        # Combine the sampled papers\n",
    "        sampled_papers[domain] = sampled_distinguished + sampled_non_distinguished\n",
    "\n",
    "    return sampled_papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sampled_papers_to_csv(sampled_papers, filename='sampled_papers.csv'):\n",
    "    \"\"\"\n",
    "    Writes sampled papers data to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        sampled_papers (dict): A dictionary where each key is a domain, and the value is a list of sampled paper dictionaries.\n",
    "        filename (str, optional): The name of the CSV file to write to. Defaults to 'sampled_papers.csv'.\n",
    "\n",
    "    Behavior:\n",
    "        - Writes a header row to the CSV file.\n",
    "        - Iterates through the sampled papers and writes each paper's details (domain, name, authors, distinguished status, and URL) as a row.\n",
    "    \"\"\"\n",
    "    os.makedirs('csv_files', exist_ok=True)\n",
    "    filepath = os.path.join('csv_files', filename)\n",
    "\n",
    "    # Define the header for the CSV\n",
    "    header = ['Domain', 'Paper Name', 'Authors', 'Distinguished', 'URL']\n",
    "\n",
    "    # Open the CSV file in write mode\n",
    "    with open(filepath, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # Iterate over the sampled papers and write each one to the CSV\n",
    "        for domain, papers in sampled_papers.items():\n",
    "            for paper in papers:\n",
    "                writer.writerow([domain, paper['name'], paper['authors'], paper['distinguished'], paper['url']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sampled_papers = sample_papers(merged_dict_all)\\nfor domain, papers in sampled_papers.items():\\n    print(f\"Domain: {domain}\")\\n    for paper in papers:\\n        print(f\"  Paper Name: {paper[\\'name\\']}\")\\n        #print(f\"    Authors: {paper[\\'authors\\']}\")\\n        print(f\"    Distinguished: {paper[\\'distinguished\\']}\")\\n        #print(f\"    URL: {paper[\\'url\\']}\")\\n    print(\"\\n\")  '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This has become obsolete since the current Sampling strategy is the one that includes the removal of the already sampled papers\"\"\"\n",
    "\n",
    "\"\"\"sampled_papers = sample_papers(merged_dict_all)\n",
    "for domain, papers in sampled_papers.items():\n",
    "    print(f\"Domain: {domain}\")\n",
    "    for paper in papers:\n",
    "        print(f\"  Paper Name: {paper['name']}\")\n",
    "        #print(f\"    Authors: {paper['authors']}\")\n",
    "        print(f\"    Distinguished: {paper['distinguished']}\")\n",
    "        #print(f\"    URL: {paper['url']}\")\n",
    "    print(\"\\n\")  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"write_sampled_papers_to_csv(sampled_papers, 'sampled_papers.csv')\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"write_sampled_papers_to_csv(sampled_papers, 'sampled_papers.csv')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling including the removal of the already sampled papers\n",
    "Remove the papers that have already been sampled and labeled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following papers have been removed from the pool to avoid the sampling of duplicate papers (after Sampling Round 3). The remaining papers are saved inside current_papers_pool.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_to_remove = [\n",
    "    \"Improving the Learning of Code Review Successive Tasks with Cross-Task Knowledge Distillation\",\n",
    "    \"Can Machine Learning Pipelines Be Better Configured?\",\n",
    "    \"Benchmarking Robustness of AI-enabled Multi-sensor Fusion Systems: Challenges and Opportunities\",\n",
    "    \"An Empirical Study on Noisy Label Learning for Program Understanding\",\n",
    "    \"EGFE: End-to-end Grouping of Fragmented Elements in UI Designs with Multimodal Learning\",\n",
    "    \"Harnessing Neuron Stability to Improve DNN Verification\",\n",
    "    \"Towards Finding Accounting Errors in Smart Contracts\",\n",
    "    \"Detecting Blocking Errors in Go Programs using Localized Abstract Interpretation\",\n",
    "    \"NeuRI: Diversifying DNN Generation via Inductive Rule Inference\",\n",
    "    \"Is unsafe an Achilles' Heel? A Comprehensive Study of Safety Requirements in Unsafe Rust Programming\",\n",
    "    \"Generating Realistic and Diverse Tests for LiDAR-Based Perception Systems\",\n",
    "    \"Provably Tightest Linear Approximation for Robustness Verification of Sigmoid-like Neural Networks\",\n",
    "    \"Modularizing while Training: a New Paradigm for Modularizing DNN Models\",\n",
    "    \"A Highly Scalable, Hybrid, Cross-Platform Timing Analysis Framework Providing Accurate Differential Throughput Estimation via Instruction-Level Tracing\",\n",
    "    \"Code Search is All You Need? Improving Code Suggestions with Code Search\",\n",
    "    \"GrACE: Language Models Meet Code Edits\",\n",
    "    \"Using Deep Learning to Automatically Improve Code Readability\",\n",
    "    \"When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference\",\n",
    "    \"Sibyl: Improving Software Engineering Tools with SMT Selection\",\n",
    "    \"HyperAST: Enabling Efficient Analysis of Software Histories at Scale\",\n",
    "    \"DeepScaler: Holistic Autoscaling for Microservices Based on Spatiotemporal GNN with Adaptive Graph Learning\",\n",
    "    \"On Using GUI Interaction Data to Improve Text Retrieval-based Bug Localization\",\n",
    "    \"Repeated Builds During Code Review: An Empirical Study of the OpenStack Community\",\n",
    "    \"A Large-Scale Empirical Study on Semantic Versioning in Golang Ecosystem\",\n",
    "    \"A Qualitative Study on the Implementation Design Decisions of Developers\",\n",
    "    \"Analyzing and Debugging Normative Requirements via Satisfiability Checking\",\n",
    "    \"Compiler Testing using Template Java Programs\",\n",
    "    \"ReFAIR: Toward a Context-Aware Recommender for Fairness Requirements Engineering\",\n",
    "    \"TRIAD: Automated Traceability Recovery based on Biterm-enhanced Deduction of Transitive Links among Artifacts\",\n",
    "    \"BSHUNTER: Detecting and Tracing Defects of Bitcoin Scripts\",\n",
    "    \"AFour-Year Study of Student Contributions to OSS vs. OSS4SG with a Lightweight Intervention\",\n",
    "    \"Do I Belong? Modeling Sense of Virtual Community Among Linux Kernel Contributors\",\n",
    "    \"GenderMag Improves Discoverability in the Field, Especially for Women\",\n",
    "    \"[Remote] Understanding the topics and challenges of GPU programming by classifying and analyzing Stack Overflow posts\",\n",
    "    \"Towards Greener Yet Powerful Code Generation via Quantization: An Empirical Study\",\n",
    "    \"How Are Paid and Volunteer Open Source Developers Different? A Study of the Rust Project\",\n",
    "    \"Mate! Are You Really Aware? An Explainability-Guided Testing Framework for Robustness of Malware Detectors\",\n",
    "    \"Lejacon: A Lightweight and Efficient Approach to Java Confidential Computing on SGX\",\n",
    "    \"Static Application Security Testing (SAST) Tools for Smart Contracts: How Far Are We?\",\n",
    "    \"Leveraging Practitioners' Feedback to Improve a Security Linter\",\n",
    "    \"Automated Black-box Testing of Mass Assignment Vulnerabilities in RESTful APIs\",\n",
    "    \"ViaLin: Path-Aware Dynamic Taint Analysis for Android\",\n",
    "    \"Improving the Learning of Code Review Successive Tasks with Cross-Task Knowledge Distillation\",\n",
    "    \"Boosting the Revealing of Detected Violations in Deep Learning Testing: A Diversity-Guided Method\",\n",
    "    \"CARGO: AI-Guided Dependency Analysis for Migrating Monolithic Applications to Microservices Architecture\",\n",
    "    \"Reusing Deep Neural Network Models through Model Re-engineering\",\n",
    "    \"Using an LLM to Help With Code Understanding\",\n",
    "    \"Flexible and Optimal Dependency Management via Max-SMT\",\n",
    "    \"Speeding up SMT Solving via Compiler Optimization\",\n",
    "    \"Mutation-based Fault Localization of Deep Neural Networks\",\n",
    "    \"EndWatch: A Practical Method for Detecting Non-Termination in Real-World Software\",\n",
    "    \"The Plastic Surgery Hypothesis in the Era of Large Language Models\",\n",
    "    \"Accelerating Continuous Integration with Parallel Batch Testing\",\n",
    "    \"Testing the Limits: Unusual Text Inputs Generation for Mobile App Crash Detection with Large Language Model\",\n",
    "    \"Generative Type Inference for Python\",\n",
    "    \"TraStrainer: Adaptive Sampling for Distributed Traces with System Runtime State\",\n",
    "    \"FAIR: Flow Type-Aware Pre-Training of Compiler Intermediate Representations\",\n",
    "    \"The Smelly Eight: An Empirical Study on the Prevalence of Code Smells in Quantum Computing\",\n",
    "    \"SkCoder: A Sketch-based Approach for Automatic Code Generation\",\n",
    "    \"CodeGen4Libs: A Two-Stage Approach for Library-Oriented Code Generation\",\n",
    "    \"DeepScaler: Holistic Autoscaling for Microservices Based on Spatiotemporal GNN with Adaptive Graph Learning\",\n",
    "    \"Hard to Read and Understand Pythonic Idioms? DeIdiom and Explain Them in Non-Idiomatic Equivalent Code\",\n",
    "    \"Has My Release Disobeyed Semantic Versioning? Static Detection Based On Semantic Differencing\",\n",
    "    \"Semantic GUI Scene Learning and Video Alignment for Detecting Duplicate Video-based Bug Reports\",\n",
    "    \"Developer-Intent Driven Code Comment Generation\",\n",
    "    \"Dependency-Induced Waste in Continuous Integration: An Empirical Study of Unused Dependencies in the NPM Ecosystem\",\n",
    "    \"Analyzing and Debugging Normative Requirements via Satisfiability Checking\",\n",
    "    \"Compiler Testing using Template Java Programs\",\n",
    "    \"Detecting Smart Home Automation Application Interferences with Domain Knowledge\",\n",
    "    \"Groundhog: An Automated Accessibility Crawler for Mobile Apps\",\n",
    "    \"Automatically Detecting Visual Bugs in HTML5 <canvas> Games\",\n",
    "    \"Generating Critical Test Scenarios for Autonomous Driving Systems via Influential Behavior Patterns\",\n",
    "    \"Causal Relationships and Programming Outcomes: A Transcranial Magnetic Stimulation Experiment\",\n",
    "    \"GenderMag Improves Discoverability in the Field, Especially for Women\",\n",
    "    \"Property-Based Testing in Practice\",\n",
    "    \"Understanding the Impact of APIs Behavioral Breaking Changes on Client Applications\",\n",
    "    \"How does Simulation-based Testing for Self-driving Cars match Human Perception?\",\n",
    "    \"A Case Study of Developer Bots: Motivations, Perceptions, and Challenges\",\n",
    "    \"Compatible Remediation on Vulnerabilities from Third-Party Libraries for Java Projects\",\n",
    "    \"[Remote] TransRacer: Function Dependence-Guided Transaction Race Detection for Smart Contracts\",\n",
    "    \"Lejacon: A Lightweight and Efficient Approach to Java Confidential Computing on SGX\",\n",
    "    \"FuzzSlice: Pruning False Positives in Static Analysis Warnings through Function-Level Fuzzing\",\n",
    "    \"Does data sampling improve deep learning-based vulnerability detection? Yeas! and Nays!\",\n",
    "    \"Fine-grained Commit-level Vulnerability Type Prediction By CWE Tree Structure\",\n",
    "    \"LExecutor: Learning-Guided Execution\",\n",
    "    \"Improving the Learning of Code Review Successive Tasks with Cross-Task Knowledge Distillation\",\n",
    "    \"Benchmarking Robustness of AI-enabled Multi-sensor Fusion Systems: Challenges and Opportunities\",\n",
    "    \"Compressing Pre-trained Models of Code into 3 MB\",\n",
    "    \"PyEvolve: Automating Frequent Code Changes in Python ML Systems\",\n",
    "    \"[Remote] CodeMark: Imperceptible Watermarking for Code Datasets against Neural Code Completion Models\",\n",
    "    \"Baldur: Whole-Proof Generation and Repair with Large Language Models\",\n",
    "    \"Understanding and Detecting On-the-Fly Configuration Bugs\",\n",
    "    \"ProveNFix: Temporal Property guided Program Repair\",\n",
    "    \"PyTy: Repairing Static Type Errors in Python\",\n",
    "    \"MuAkka: Mutation Testing for Actor Concurrency in Akka Using Real-World Bugs\",\n",
    "    \"HTFuzz: Heap Operation Sequence Sensitive Fuzzing\",\n",
    "    \"Generative Type Inference for Python\",\n",
    "    \"FAIR: Flow Type-Aware Pre-Training of Compiler Intermediate Representations\",\n",
    "    \"Recommending Analogical APIs via Knowledge Graph Embedding\",\n",
    "    \"On the Usage of Continual Learning for Out-of-Distribution Generalization in Pre-trained Language Models of Code\",\n",
    "    \"Studying and Understanding the Tradeoffs Between Generality and Reduction in Software Debloating\",\n",
    "    \"Decomposing Software Verification Using Distributed Summary Synthesis\",\n",
    "    \"Only diff is Not Enough: Generating Commit Messages Leveraging Reasoning and Action of Large Language Model\",\n",
    "    \"Sibyl: Improving Software Engineering Tools with SMT Selection\",\n",
    "    \"HyperAST: Enabling Efficient Analysis of Software Histories at Scale\",\n",
    "    \"On Using GUI Interaction Data to Improve Text Retrieval-based Bug Localization\",\n",
    "    \"UpCy: Safely Updating Outdated Dependencies\",\n",
    "    \"OSSFP: Precise and Scalable C/C++ Third-Party Library Detection using Fingerprinting Functions\",\n",
    "    \"Compiler Testing using Template Java Programs\",\n",
    "    \"Analyzing and Debugging Normative Requirements via Satisfiability Checking\",\n",
    "    \"Detecting Smart Home Automation Application Interferences with Domain Knowledge\",\n",
    "    \"SmartCoCo: Checking Comment-code Inconsistency in Smart Contracts via Constraint Propagation and Binding\",\n",
    "    \"TRIAD: Automated Traceability Recovery based on Biterm-enhanced Deduction of Transitive Links among Artifacts\",\n",
    "    \"Testability Refactoring in Pull Requests: Patterns and Trends\",\n",
    "    \"“STILL AROUND”: Experiences and Survival Strategies of Veteran Women Software Developers\",\n",
    "    \"A Longitudinal Study of Student Contributions to OSS vs. OSS4SG with a Lightweight Intervention\",\n",
    "    \"Property-Based Testing in Practice\",\n",
    "    \"Semi-Automatic, Inline and Collaborative Web Page Code Curations\",\n",
    "    \"AI-assisted Code Authoring at Scale: Fine-tuning, deploying, and mixed methods evaluation\",\n",
    "    \"\\\"We Feel Like We're Winging It\\\": A Study on Navigating Open-Source Dependency Abandonment\",\n",
    "    \"Static Application Security Testing (SAST) Tools for Smart Contracts: How Far Are We?\",\n",
    "    \"Attention! Your Copied Data is Under Monitoring: A Systematic Study of Clipboard Usage in Android Apps\",\n",
    "    \"Compatible Remediation on Vulnerabilities from Third-Party Libraries for Java Projects\",\n",
    "    \"Silent Vulnerable Dependency Alert Prediction with Vulnerability Key Aspect Explanation\",\n",
    "    \"An Empirical Study of Data Disruption by Ransomware Attacks\",\n",
    "    \"TAINTMINI: Detecting Flow of Sensitive Data in Mini-Programs with Static Taint Analysis\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sampled_papers_from_pool(current_pool_dict, sampled_papers_dict):\n",
    "    \"\"\"\n",
    "    Remove papers from current_pool_dict that exist in sampled_papers_debug.\n",
    "    Papers are matched based on their names.\n",
    "    \n",
    "    Args:\n",
    "        current_pool_dict (defaultdict): Dictionary containing papers organized by domain\n",
    "        sampled_papers_dict (dict): Dictionary containing papers to check against\n",
    "    \"\"\"\n",
    "    # For each domain in sampled_papers_dict\n",
    "    for domain, sampled_papers in sampled_papers_dict.items():\n",
    "        # Skip if domain doesn't exist in current_pool_dict\n",
    "        if domain not in current_pool_dict:\n",
    "            continue\n",
    "            \n",
    "        # Create a set of paper names from sampled papers for efficient lookup\n",
    "        sampled_paper_names = {paper['name'] for paper in sampled_papers}\n",
    "        \n",
    "        # Filter out papers that exist in sampled_papers_dict\n",
    "        current_pool_dict[domain]['papers'] = [\n",
    "            paper for paper in current_pool_dict[domain]['papers']\n",
    "            if paper['name'] not in sampled_paper_names\n",
    "        ]\n",
    "        \n",
    "        # If no papers left in domain, you might want to remove the domain entirely\n",
    "        if not current_pool_dict[domain]['papers']:\n",
    "            del current_pool_dict[domain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_remaining_papers(merged_dict_all):\n",
    "    \"\"\"\n",
    "    Returns randomly sampled papers from domain_ai_se and domain_testing_analysis. These two domains are the only domains where papers still need to be sampled.\n",
    "\n",
    "    Args:\n",
    "        merged_dict_all (dict): A dictionary where each domain maps to a dictionary containing a list of papers.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing only domain_ai_se and domain_testing_analysis, where each domain maps \n",
    "             to a list of papers with distinguished papers first, followed by non-distinguished papers.\n",
    "    \"\"\"\n",
    "    sampled_papers = {}\n",
    "    target_domains = ['domain_dependability_security']\n",
    "\n",
    "    for domain in target_domains:\n",
    "        if domain in merged_dict_all:\n",
    "            # Separate papers into distinguished and non-distinguished\n",
    "            all_papers = merged_dict_all[domain]['papers']\n",
    "            distinguished_papers = [paper for paper in all_papers if paper['distinguished']]\n",
    "            non_distinguished_papers = [paper for paper in all_papers if not paper['distinguished']]\n",
    "            \n",
    "            # Randomly shuffle each group separately\n",
    "            random.shuffle(distinguished_papers)\n",
    "            random.shuffle(non_distinguished_papers)\n",
    "            \n",
    "            # Combine the papers with distinguished papers first\n",
    "            sampled_papers[domain] = distinguished_papers + non_distinguished_papers\n",
    "\n",
    "    return sampled_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dict_to_csv(paper_dict, filename):\n",
    "    \"\"\"\n",
    "    Writes the dictionary containing papers to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        paper_dict (defaultdict): A dictionary containing paper data.\n",
    "        filename (str): The name of the CSV file to write to.\n",
    "    \"\"\"\n",
    "    # Open the file for writing\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Write the header row (assuming papers are under \"papers\" key)\n",
    "        writer.writerow(['Domain', 'Paper Name', 'Authors', 'Distinguished', 'URL'])\n",
    "\n",
    "        # Loop through the dictionary and write the paper details to the file\n",
    "        for domain, data in paper_dict.items():\n",
    "            if \"papers\" in data:\n",
    "                for paper in data[\"papers\"]:\n",
    "                    writer.writerow([domain, paper[\"name\"], paper[\"authors\"], paper[\"distinguished\"], paper[\"url\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_papers_in_dict(paper_dict):\n",
    "    \"\"\"\n",
    "    Counts the number of papers in the dictionary.\n",
    "\n",
    "    Args:\n",
    "        paper_dict (dict): A dictionary containing domains and their respective papers.\n",
    "\n",
    "    Returns:\n",
    "        int: The total number of papers in the dictionary.\n",
    "    \"\"\"\n",
    "    total_papers = 0\n",
    "    for domain, papers in paper_dict.items():\n",
    "        total_papers += len(papers)  # Count the number of papers in each domain\n",
    "    return total_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_pool_csv_file = 'csv_files/current_papers_pool.csv'\n",
    "current_papers_pool = parse_papers_csv(current_pool_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_papers(current_papers_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampled_papers = sample_papers(current_papers_pool)\n",
    "#write_sampled_papers_to_csv(sampled_papers, 'sampled_papers.csv')\n",
    "\n",
    "## After multiple rounds of sampling only two domains are missing papers. Sample only papers out of these two domains:\n",
    "sampled_papers = sample_remaining_papers(current_papers_pool)\n",
    "write_sampled_papers_to_csv(sampled_papers, 'sampled_papers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging (count the number of the sampled papers which are to be removed from the pool):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_papers_in_dict(sampled_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_sampled_papers_from_pool(current_papers_pool, sampled_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging (count the number of papers that remain in the pool after the removal of the sampled papers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_papers(current_papers_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict_to_csv(current_papers_pool, 'csv_files/current_papers_pool.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conference_scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
