{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling of the rated research for the expert validation of the research questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rated_papers_dict(folder_path):\n",
    "    data_dict = {}\n",
    "    \n",
    "    # List of columns to include in the output\n",
    "    columns_to_include = [\n",
    "        'Domain', \n",
    "        'Knowledge-seeking vs. Eval', \n",
    "        'Nerd factor/zu spezifisch', \n",
    "        'Validation Nerd Factor', \n",
    "        'Distinguished', \n",
    "        'Bucket ID', \n",
    "        'Paper Name', \n",
    "        'Research Questions (max. 4)',\n",
    "        'URL'\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # Loop through each file in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "                csvreader = csv.DictReader(csvfile)\n",
    "                \n",
    "                # Loop through each row in the CSV file\n",
    "                for row in csvreader:\n",
    "                    # Extract the relevant columns only\n",
    "                    filtered_row = {col: row[col] for col in columns_to_include if col in row}\n",
    "                    \n",
    "                    # Extract domain from the row\n",
    "                    domain = row['Domain']\n",
    "                    \n",
    "                    # Add the filtered row to the dictionary under the corresponding domain\n",
    "                    if domain not in data_dict:\n",
    "                        data_dict[domain] = []\n",
    "                    data_dict[domain].append(filtered_row)\n",
    "    \n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_papers_per_domain(data_dict):\n",
    "    for domain, papers in data_dict.items():\n",
    "        print(f\"Domain: {domain}\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Loop through each paper and print its details in one line\n",
    "        for paper in papers:\n",
    "            paper_details = f\"Paper Name: {paper['Paper Name']}\"\n",
    "            \n",
    "            # Append all the other relevant information\n",
    "            for key, value in paper.items():\n",
    "                if key != 'Paper Name':  # Skip the Paper Name itself\n",
    "                    paper_details += f\", {key}: {value}\"\n",
    "            \n",
    "            # Print the paper details in one line\n",
    "            print(paper_details)\n",
    "        \n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_non_specific_papers(data_dict):\n",
    "    filtered_data_dict = {}\n",
    "    \n",
    "    # Loop through each domain in the existing data dictionary\n",
    "    for domain, papers in data_dict.items():\n",
    "        # Filter the papers for this domain by checking if Bucket ID starts with 'B'\n",
    "        filtered_papers = [\n",
    "            paper for paper in papers if paper.get('Bucket ID', '').startswith('B')\n",
    "        ]\n",
    "        \n",
    "        # If there are any filtered papers, add them to the new dictionary\n",
    "        if filtered_papers:\n",
    "            filtered_data_dict[domain] = filtered_papers\n",
    "    \n",
    "    return filtered_data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_specific_papers(data_dict):\n",
    "    filtered_data_dict = {}\n",
    "    \n",
    "    # Loop through each domain in the existing data dictionary\n",
    "    for domain, papers in data_dict.items():\n",
    "        # Filter the papers for this domain where the \"Validation Nerd Factor\" is either 1 or -1\n",
    "        filtered_papers = [\n",
    "            paper for paper in papers \n",
    "            if paper.get('Validation Nerd Factor') in ['1', '-1']\n",
    "        ]\n",
    "        \n",
    "        # If there are any filtered papers, add them to the new dictionary\n",
    "        if filtered_papers:\n",
    "            filtered_data_dict[domain] = filtered_papers\n",
    "    \n",
    "    return filtered_data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debugging\n",
    "def print_paper_info(data):\n",
    "    for domain, papers in data.items():\n",
    "        print(f\"Domain: {domain}\")\n",
    "        for paper in papers:\n",
    "            paper_name = paper.get('Paper Name', 'N/A')\n",
    "            distinguished = paper.get('Distinguished', 'N/A')\n",
    "            bucket_id = paper.get('Bucket ID', 'N/A')\n",
    "            print(f\"  Paper Name: {paper_name}\")\n",
    "            print(f\"  Distinguished: {distinguished}\")\n",
    "            print(f\"  Bucket ID: {bucket_id}\")\n",
    "        print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging\n",
    "def print_sampled_papers(sampled_papers):\n",
    "    \"\"\"\n",
    "    Prints the list of sampled papers in a single-line, readable format without the research questions.\n",
    "\n",
    "    Parameters:\n",
    "    sampled_papers (list): A list of sampled papers where each paper is a dictionary.\n",
    "    \"\"\"\n",
    "    # Check if there are any sampled papers\n",
    "    if not sampled_papers:\n",
    "        print(\"No papers were sampled.\")\n",
    "        return\n",
    "    \n",
    "    # Iterate over each sampled paper and print the details in a single line without research questions\n",
    "    for idx, paper in enumerate(sampled_papers, 1):\n",
    "        paper_info = (\n",
    "            f\"Paper {idx}: \"\n",
    "            f\"Domain: {paper['Domain']}, \"\n",
    "            f\"Paper Name: {paper['Paper Name']}, \"\n",
    "            f\"Bucket ID: {paper['Bucket ID']}, \"\n",
    "            f\"Distinguished: {paper['Distinguished']}, \"\n",
    "            f\"Nerd factor/zu spezifisch: {paper['Nerd factor/zu spezifisch']}, \"\n",
    "            f\"Validation Nerd Factor: {paper['Validation Nerd Factor']}\"\n",
    "        )\n",
    "        print(paper_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'csv_files/rated_papers'\n",
    "all_rated_papers = create_rated_papers_dict(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_specific_papers = filter_non_specific_papers(all_rated_papers)\n",
    "specific_papers = filter_specific_papers(all_rated_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling of non specific papers\n",
    "\n",
    "Non specific papers are papers which contain topics and research questions which are considered easy to understand by the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_non_specific_papers(non_specific_papers):\n",
    "    \"\"\"\n",
    "    Samples papers from each bucket with specific selection criteria:\n",
    "    - 2 non-distinguished papers: first from agreed papers, then from disagreed papers\n",
    "    - 1 distinguished paper: prioritizing agreed papers, then disagreed papers, \n",
    "      then non-distinguished agreed papers, finally non-distinguished disagreed papers\n",
    "    \n",
    "    Parameters:\n",
    "    non_specific_papers (dict): Dictionary of domain names to paper lists\n",
    "    \n",
    "    Returns:\n",
    "    list: Flat list of sampled papers (3 papers per bucket if available)\n",
    "    \"\"\"\n",
    "    def get_papers_for_bucket(bucket_id):\n",
    "        return [\n",
    "            paper for papers in non_specific_papers.values() \n",
    "            for paper in papers if paper['Bucket ID'] == bucket_id\n",
    "        ]\n",
    "    \n",
    "    def has_agreed_ratings(paper):\n",
    "        return (paper.get(\"Nerd factor/zu spezifisch\", \"\").strip() == \"0\" and \n",
    "                paper.get(\"Validation Nerd Factor\", \"\").strip() == \"0\")\n",
    "    \n",
    "    def remove_paper(paper):\n",
    "        for papers in non_specific_papers.values():\n",
    "            if paper in papers:\n",
    "                papers.remove(paper)\n",
    "                break\n",
    "    \n",
    "    sampled_papers = []\n",
    "    \n",
    "    for bucket_id in [f\"B{i}\" for i in range(1, 15)]:\n",
    "        bucket_papers = get_papers_for_bucket(bucket_id)\n",
    "        if not bucket_papers:\n",
    "            continue\n",
    "            \n",
    "        # Split papers by agreement and distinguished status\n",
    "        papers_agreed = [p for p in bucket_papers if has_agreed_ratings(p)]\n",
    "        papers_disagreed = [p for p in bucket_papers if not has_agreed_ratings(p)]\n",
    "        \n",
    "        # Sample 2 non-distinguished papers\n",
    "        non_dist_papers = []\n",
    "        \n",
    "        # First try from agreed papers\n",
    "        non_dist_agreed = [p for p in papers_agreed if p['Distinguished'] == 'FALSE']\n",
    "        non_dist_papers.extend(non_dist_agreed[:2])\n",
    "        \n",
    "        # If needed, try from disagreed papers\n",
    "        if len(non_dist_papers) < 2:\n",
    "            needed = 2 - len(non_dist_papers)\n",
    "            non_dist_disagreed = [p for p in papers_disagreed if p['Distinguished'] == 'FALSE']\n",
    "            non_dist_papers.extend(non_dist_disagreed[:needed])\n",
    "            \n",
    "            if len(non_dist_papers) < 2:\n",
    "                print(f\"Error: Could not find enough non-distinguished papers for bucket {bucket_id}\")\n",
    "        \n",
    "        # Sample 1 distinguished paper (following priority order)\n",
    "        dist_paper = None\n",
    "        \n",
    "        # 1. Try distinguished paper from agreed papers\n",
    "        dist_agreed = [p for p in papers_agreed if p['Distinguished'] == 'TRUE']\n",
    "        if dist_agreed:\n",
    "            dist_paper = dist_agreed[0]\n",
    "        \n",
    "        # 2. Try distinguished paper from disagreed papers\n",
    "        if not dist_paper:\n",
    "            dist_disagreed = [p for p in papers_disagreed if p['Distinguished'] == 'TRUE']\n",
    "            if dist_disagreed:\n",
    "                dist_paper = dist_disagreed[0]\n",
    "        \n",
    "        # 3. Try non-distinguished paper from agreed papers\n",
    "        if not dist_paper:\n",
    "            remaining_non_dist_agreed = [p for p in papers_agreed if p['Distinguished'] == 'FALSE' \n",
    "                                       and p not in non_dist_papers]\n",
    "            if remaining_non_dist_agreed:\n",
    "                dist_paper = remaining_non_dist_agreed[0]\n",
    "        \n",
    "        # 4. Try non-distinguished paper from disagreed papers\n",
    "        if not dist_paper:\n",
    "            remaining_non_dist_disagreed = [p for p in papers_disagreed if p['Distinguished'] == 'FALSE' \n",
    "                                          and p not in non_dist_papers]\n",
    "            if remaining_non_dist_disagreed:\n",
    "                dist_paper = remaining_non_dist_disagreed[0]\n",
    "        \n",
    "        if not dist_paper:\n",
    "            print(f\"Error: Could not find a suitable third paper for bucket {bucket_id}\")\n",
    "        \n",
    "        # Remove sampled papers from the pool\n",
    "        for paper in non_dist_papers:\n",
    "            remove_paper(paper)\n",
    "        \n",
    "        if dist_paper:\n",
    "            remove_paper(dist_paper)\n",
    "            non_dist_papers.append(dist_paper)\n",
    "        \n",
    "        sampled_papers.extend(non_dist_papers)\n",
    "    \n",
    "    return sampled_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not find a suitable third paper for bucket B5\n"
     ]
    }
   ],
   "source": [
    "sampled_non_specific_papers = sample_non_specific_papers(non_specific_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling of specific papers\n",
    "\n",
    "Specific papers are papers which contain topics and research questions labeled as too specific by the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_papers_by_domain(specific_papers):\n",
    "    \"\"\"\n",
    "    Randomly selects one \"Knowledge-seeking\" and one \"Evaluation\" paper per domain, \n",
    "    removing them from the original dataset.\n",
    "\n",
    "    Parameters:\n",
    "    specific_papers (dict): A dictionary where keys are domain names and values are lists of paper dictionaries. The papers have been labeled as too specific.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of sampled papers, each represented as a dictionary.\n",
    "    \"\"\" \n",
    "\n",
    "    sampled_papers = []\n",
    "    \n",
    "    for domain, papers in list(specific_papers.items()):\n",
    "        knowledge_seeking_papers = [paper for paper in papers if paper.get(\"Knowledge-seeking vs. Eval\") == \"Knowledge-seeking\"]\n",
    "        evaluation_papers = [paper for paper in papers if paper.get(\"Knowledge-seeking vs. Eval\") == \"Evaluation\"]\n",
    "\n",
    "        if not knowledge_seeking_papers:\n",
    "            print(f\"No knowledge-seeking papers for domain: {domain}\")\n",
    "        if not evaluation_papers:\n",
    "            print(f\"No evaluation papers for domain: {domain}\")\n",
    "\n",
    "        sampled_knowledge = random.choice(knowledge_seeking_papers).copy() if knowledge_seeking_papers else None\n",
    "        sampled_evaluation = random.choice(evaluation_papers).copy() if evaluation_papers else None\n",
    "\n",
    "        if sampled_knowledge:\n",
    "            sampled_papers.append(sampled_knowledge)\n",
    "            specific_papers[domain].remove(sampled_knowledge)  # This modifies original data\n",
    "        elif len(evaluation_papers) > 1:\n",
    "            sampled_papers.append(sampled_evaluation)\n",
    "            specific_papers[domain].remove(sampled_evaluation)\n",
    "            evaluation_papers.remove(sampled_evaluation)\n",
    "            sampled_evaluation = random.choice(evaluation_papers).copy() if evaluation_papers else None\n",
    "            print(f\"Knowledge-seeking paper replaced with evaluation paper for domain: {domain}\")\n",
    "\n",
    "        if sampled_evaluation:\n",
    "            sampled_papers.append(sampled_evaluation)\n",
    "            specific_papers[domain].remove(sampled_evaluation)  # This modifies original data\n",
    "    \n",
    "    return sampled_papers  # Returns a flat list of sampled papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No knowledge-seeking papers for domain: Evolution\n",
      "No knowledge-seeking papers for domain: Requirements and modeling\n",
      "Knowledge-seeking paper replaced with evaluation paper for domain: Requirements and modeling\n",
      "No knowledge-seeking papers for domain: Human and social aspects\n"
     ]
    }
   ],
   "source": [
    "sampled_specific_papers = sample_papers_by_domain(specific_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 1: Domain: AI and software engineering, Auto-coding, Paper Name: Towards Understanding Fairness and its Composition in Ensemble Machine Learning, Bucket ID: -, Distinguished: FALSE, Nerd factor/zu spezifisch: -1, Validation Nerd Factor: 1\n",
      "Paper 2: Domain: AI and software engineering, Auto-coding, Paper Name: An Empirical Study on Noisy Label Learning for Program Understanding, Bucket ID: -, Distinguished: FALSE, Nerd factor/zu spezifisch: -1 (NLL?), Validation Nerd Factor: 1\n",
      "Paper 3: Domain: Testing and analysis, Paper Name: AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models, Bucket ID: -, Distinguished: FALSE, Nerd factor/zu spezifisch: -1, Validation Nerd Factor: 1\n",
      "Paper 4: Domain: Testing and analysis, Paper Name: Mutation-based Fault Localization of Deep Neural Networks, Bucket ID: -, Distinguished: TRUE, Nerd factor/zu spezifisch: -1\n",
      "(end-to end fault loc.?, mutation selection konnte ich mir einigermaßen herleiten --> hab aber lein tieferes Verständnis, was man aber auch nicht braucht zum Beantworten der RQ fidne ich), Validation Nerd Factor: 1\n",
      "Paper 5: Domain: Analytics, Paper Name: When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference, Bucket ID: -, Distinguished: FALSE, Nerd factor/zu spezifisch: -1, Validation Nerd Factor: 1\n",
      "Paper 6: Domain: Analytics, Paper Name: Modularizing while Training: a New Paradigm for Modularizing DNN Models, Bucket ID: -, Distinguished: TRUE, Nerd factor/zu spezifisch: -1 (CNN spezifisch; ich kann mir ungefährt vorstellen, was mit Modularisierung gemeint ist, aber nicht genug um eine RQ zu benatworten), Validation Nerd Factor: 1\n",
      "Paper 7: Domain: Evolution, Paper Name: Sibyl: Improving Software Engineering Tools with SMT Selection, Bucket ID: -, Distinguished: TRUE, Nerd factor/zu spezifisch: -1 (SMT Selection?, Vorwissen zu GNNs notwendig?), Validation Nerd Factor: -1\n",
      "Paper 8: Domain: Requirements and modeling, Paper Name: SmartCoCo: Checking Comment-code Inconsistency in Smart Contracts via Constraint Propagation and Binding, Bucket ID: -, Distinguished: FALSE, Nerd factor/zu spezifisch: 0, Validation Nerd Factor: 1\n",
      "Paper 9: Domain: Requirements and modeling, Paper Name: TRIAD: Automated Traceability Recovery based on Biterm-enhanced Deduction of Transitive Links among Artifacts, Bucket ID: -, Distinguished: FALSE, Nerd factor/zu spezifisch: -1 (RQ2 Begriffe?), Validation Nerd Factor: 1\n",
      "Paper 10: Domain: Human and social aspects, Paper Name: Causal Relationships and Programming Outcomes: A Transcranial Magnetic Stimulation Experiment, Bucket ID: -, Distinguished: TRUE, Nerd factor/zu spezifisch: -1\n",
      "(bin mir unsicher, ob da nicht Wissen zum Gehirn usw. benötigt sind), Validation Nerd Factor: -1\n",
      "Paper 11: Domain: Dependability and Security, Paper Name: Static Application Security Testing (SAST) Tools for Smart Contracts: How Far Are We?, Bucket ID: -, Distinguished: TRUE, Nerd factor/zu spezifisch: -1 (SAST Hintergrundwissen notwendig?), Validation Nerd Factor: 1\n",
      "Paper 12: Domain: Dependability and Security, Paper Name: Lejacon: A Lightweight and Efficient Approach to Java Confidential Computing on SGX, Bucket ID: -, Distinguished: TRUE, Nerd factor/zu spezifisch: 0, Validation Nerd Factor: 1\n"
     ]
    }
   ],
   "source": [
    "#Debugging\n",
    "print_sampled_papers(sampled_specific_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the sampled papers in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_paper_urls(sampled_non_specific_papers, csv_file_path=\"csv_files/session and papers all conferences.csv\"):\n",
    "    \"\"\"\n",
    "    Updates the 'URL' field in the `sampled_non_specific_papers` list with the URLs from the provided CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    sampled_non_specific_papers (list): A list of sampled papers. Each paper is a dictionary containing various fields, including 'Paper Name' as the identifier.\n",
    "    csv_file_path (str): The path to the CSV file that contains paper names and corresponding URLs.\n",
    "    \n",
    "    Returns:\n",
    "    list: The `sampled_non_specific_papers` list with updated 'URL' fields for the corresponding papers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a dictionary to store paper names and their corresponding URLs from the CSV file\n",
    "    paper_urls = {}\n",
    "    \n",
    "    with open(csv_file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        for row in csvreader:\n",
    "            paper_name = row.get('name')  # Paper name as identifier\n",
    "            url = row.get('url')  # URL column in the CSV\n",
    "            if paper_name and url:\n",
    "                paper_urls[paper_name] = url\n",
    "    \n",
    "    # Iterate through the sampled_non_specific_papers list and update URLs\n",
    "    for paper in sampled_non_specific_papers:\n",
    "        paper_name = paper.get('Paper Name')  # The identifier used for matching\n",
    "        \n",
    "        if not paper_name:  # Skip papers without a valid 'Paper Name'\n",
    "            print(f\"Warning: Skipping paper with missing or invalid 'Paper Name'.\")\n",
    "            continue\n",
    "\n",
    "        if paper_name in paper_urls:\n",
    "            paper['URL'] = paper_urls[paper_name]  # Update the 'URL' field with the corresponding URL\n",
    "        else:\n",
    "            print(f\"Warning: URL not found for paper with name '{paper_name}'\")\n",
    "    \n",
    "    return sampled_non_specific_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_sort_papers(sampled_specific_papers, sampled_non_specific_papers):\n",
    "    \"\"\"\n",
    "    Combines the lists of sampled specific and non-specific papers and sorts them by domain.\n",
    "\n",
    "    Parameters:\n",
    "    sampled_specific_papers (list): A list of sampled specific papers.\n",
    "    sampled_non_specific_papers (list): A list of sampled non-specific papers.\n",
    "\n",
    "    Returns:\n",
    "    list: A combined and sorted list of papers by domain.\n",
    "    \"\"\"\n",
    "    combined_papers = sampled_specific_papers + sampled_non_specific_papers\n",
    "    sorted_papers = sorted(combined_papers, key=lambda paper: paper.get(\"Domain\", \"\"))\n",
    "    \n",
    "    return sorted_papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_papers_to_csv(papers_list, output_csv_file=\"csv_files/papers_expert_study/sampled_papers_expert_val.csv\"):\n",
    "    \"\"\"\n",
    "    Writes the list of sampled papers into a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    papers_list (list): A list of sampled papers. Each paper is a dictionary containing various fields.\n",
    "    output_csv_file (str): The path where the CSV file will be saved. Default is 'csv_files/papers_expert_val.csv'.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Check if the list is empty\n",
    "    if not papers_list:\n",
    "        print(\"Warning: The list of papers is empty. No CSV file will be created.\")\n",
    "        return\n",
    "    \n",
    "    # Define the header based on the keys of the first paper (assuming all papers have the same structure)\n",
    "    fieldnames = papers_list[0].keys()\n",
    "\n",
    "    # Open the CSV file in write mode\n",
    "    with open(output_csv_file, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        # Write the header (column names) to the CSV file\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Write the rows of data\n",
    "        for paper in papers_list:\n",
    "            writer.writerow(paper)\n",
    "    \n",
    "    print(f\"CSV file '{output_csv_file}' has been created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include all sampled papers in one list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sampled_papers = combine_and_sort_papers(sampled_specific_papers, sampled_non_specific_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add urls to the sampled papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sampled_papers = update_paper_urls(all_sampled_papers, \"csv_files/session and papers all conferences.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the papers into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'csv_files/papers_expert_val.csv' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "write_papers_to_csv(all_sampled_papers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conference_scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
