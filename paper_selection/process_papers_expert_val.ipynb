{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the papers for the expert validation study\n",
    "\n",
    "Automatic creation of short introduction texts for the papers.\n",
    "\n",
    "## Add missing URLs\n",
    "\n",
    "Start by importing a file containing missing URLs for the papers. The missing URLs were manually added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_csv_to_list(folderpath, filename):\n",
    "    \"\"\"\n",
    "    Extracts data from a CSV file and returns a list of dictionaries.\n",
    "\n",
    "    Parameters:\n",
    "    folderpath (str): The path to the folder containing the CSV file.\n",
    "    filename (str): The name of the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    list[dict]: A list of dictionaries where each dictionary represents a row in the CSV file.\n",
    "                Each dictionary contains the keys:\n",
    "                - \"Paper Name\"\n",
    "                - \"Research Questions\"\n",
    "                - \"URL\"\n",
    "                - \"Abstract\" (if present in the CSV file)\n",
    "    \"\"\"\n",
    "    csv_file_path = os.path.join(folderpath, filename)\n",
    "    extracted_data = []\n",
    "    \n",
    "    with open(csv_file_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        for row in reader:\n",
    "            subset = {\n",
    "                \"Paper Name\": row[\"Paper Name\"],\n",
    "                \"Research Questions\": row[\"Research Questions (max. 4)\"],\n",
    "                \"URL\": row[\"URL\"]\n",
    "            }\n",
    "            # Check if \"Abstract\" column exists and add it\n",
    "            if \"Abstract\" in row:\n",
    "                subset[\"Abstract\"] = row[\"Abstract\"]\n",
    "            \n",
    "            extracted_data.append(subset)\n",
    "    \n",
    "    return extracted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath = \"csv_files/papers_expert_study\"\n",
    "csv_file = \"updated_url_papers_expert_val.csv\"\n",
    "sampled_papers_list = extract_csv_to_list(folderpath, csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abstracts(url):\n",
    "    \"\"\"\n",
    "    Scrapes abstracts from the given URL using predefined CSS selectors.\n",
    "\n",
    "    This function sends an HTTP request to the specified URL, parses the HTML content \n",
    "    using BeautifulSoup, and extracts the abstract text based on a list of possible \n",
    "    selectors. It attempts to handle different website structures, including ArXiv, \n",
    "    IEEE Xplore, and ACM Digital Library.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the research paper or article from which to extract the abstract.\n",
    "\n",
    "    Returns:\n",
    "    list[str]: A list of extracted abstracts (as strings). If no abstracts are found,\n",
    "               an empty list is returned.\n",
    "    \"\"\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}  # Avoid blocking by the server\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page, status code: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Adjust these selectors based on the website structure\n",
    "    possible_selectors = [\n",
    "       'section[role=\"doc-abstract\"] div[role=\"paragraph\"]', # for dl.acm.org\n",
    "        'div.abstract-text div.col-12 div.u-mb-1 div[xplmathjax]', #ieeexplore.ieee.org\n",
    "        'span.abstract.mathjax',\n",
    "        'div.abstract',\n",
    "        'p.abstract',\n",
    "        'section.abstract',\n",
    "        'span.abstract',\n",
    "        'blockquote.abstract.mathjax'  # for arxiv.org\n",
    "    ]\n",
    "    \n",
    "    abstracts = []\n",
    "    for selector in possible_selectors:\n",
    "        elements = soup.select(selector)\n",
    "        for elem in elements:\n",
    "            # Find the text content inside the blockquote and remove the descriptor span\n",
    "            if selector == 'blockquote.abstract.mathjax':\n",
    "                abstract_text = \" \".join([text.strip() for text in elem.stripped_strings if text != \"Abstract:\"])\n",
    "                abstracts.append(abstract_text)\n",
    "            else:\n",
    "                abstracts.append(elem.get_text(strip=True))\n",
    "\n",
    "    if not abstracts:\n",
    "        print(\"Could not scrape abstract\")\n",
    "    \n",
    "    return abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_abstracts_to_papers(sampled_papers_list):\n",
    "    \"\"\"Adds abstracts to the list of papers.\"\"\"\n",
    "    for paper in sampled_papers_list:\n",
    "        url = paper[\"URL\"]\n",
    "        abstract = get_abstracts(url)  # Get the abstract using the URL\n",
    "        paper[\"Abstract\"] = abstract  # Add the \"Abstract\" to the dictionary\n",
    "    \n",
    "    return sampled_papers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(data, filename):\n",
    "    \"\"\"\n",
    "    Writes a list of dictionaries to a CSV file inside the 'csv_files/papers_expert_study' folder.\n",
    "\n",
    "    Parameters:\n",
    "    data (list of dict): A list of dictionaries where each dictionary represents a row in the CSV file.\n",
    "    filename (str): The name of the CSV file to be created.\n",
    "\n",
    "    Returns:\n",
    "    None: The function writes the CSV file and prints a confirmation message upon success.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the 'csv_files' directory exists\n",
    "    os.makedirs('csv_files/papers_expert_study', exist_ok=True)  # Creates the folder if it doesn't exist\n",
    "    \n",
    "    # Construct the full file path by joining the folder name with the filename\n",
    "    file_path = os.path.join('csv_files/papers_expert_study', filename)\n",
    "    \n",
    "    # Get the fieldnames from the first dictionary in the list (assumes all dicts have the same keys)\n",
    "    fieldnames = data[0].keys()\n",
    "    \n",
    "    # Open the file in write mode, create a CSV DictWriter object\n",
    "    with open(file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        \n",
    "        # Write the header (fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Write the rows (data)\n",
    "        writer.writerows(data)\n",
    "    \n",
    "    print(f\"Data successfully written to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape the abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch page, status code: 418\n",
      "Could not scrape abstract\n",
      "Could not scrape abstract\n",
      "Failed to fetch page, status code: 418\n",
      "Failed to fetch page, status code: 418\n",
      "Failed to fetch page, status code: 418\n",
      "Failed to fetch page, status code: 418\n",
      "Could not scrape abstract\n",
      "Failed to fetch page, status code: 418\n",
      "Failed to fetch page, status code: 418\n",
      "Failed to fetch page, status code: 418\n",
      "Failed to fetch page, status code: 418\n",
      "Could not scrape abstract\n",
      "Failed to fetch page, status code: 418\n",
      "Could not scrape abstract\n",
      "Failed to fetch page, status code: 418\n",
      "Could not scrape abstract\n",
      "Could not scrape abstract\n",
      "Could not scrape abstract\n",
      "Could not scrape abstract\n",
      "Could not scrape abstract\n",
      "Could not scrape abstract\n",
      "Paper Name: Towards Understanding Fairness and its Composition in Ensemble Machine Learning\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Research Questions (max. 4)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m paper \u001b[38;5;129;01min\u001b[39;00m updated_papers_list:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPaper Name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpaper[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPaper Name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResearch Questions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mpaper\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mResearch Questions (max. 4)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpaper[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbstract: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpaper[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbstract\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Research Questions (max. 4)'"
     ]
    }
   ],
   "source": [
    "updated_papers_list = add_abstracts_to_papers(sampled_papers_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an arfitfact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to csv_files/papers_expert_study/abstracts_papers_expert_val.csv\n"
     ]
    }
   ],
   "source": [
    "filename = 'abstracts_papers_expert_val.csv'\n",
    "write_to_csv(updated_papers_list, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create introductory texts for each paper\n",
    "\n",
    "First import a completed file containing the missing abstracts from a file that was manually completed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath = \"csv_files/papers_expert_study\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"completed_abstracts_expert_val.csv\"\n",
    "\n",
    "papers_list = extract_csv_to_list(folderpath, csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"Write a concise introductory text in German for a paper based on the following abstract. \n",
    "                The text should provide a brief overview of the main themes and context of the paper without delving into specific methods, \n",
    "                results, or contributions. It should be written in an impersonal, third-person perspective (avoid using first-person plural like \n",
    "                'we' or 'our'). The tone should remain formal and academic. The introduction should be no longer than 2-3 sentences and should \n",
    "                strictly avoid mentioning the paper's contributions, findings, or implications. Focus solely on the broader subject matter and \n",
    "                relevance of the research field.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(result):\n",
    "    as_dict = json.loads(result)\n",
    "    return as_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the request to the OpenAI API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_abstract(paper_abstract, SYSTEM_PROMPT):\n",
    "    client = OpenAI(base_url=\"http://172.26.92.115\")\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"gpt-4o-2024-11-20\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": paper_abstract}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    url = \"http://172.26.92.115/chat_completion\"\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "    # Send request\n",
    "    response = requests.post(\n",
    "        url, \n",
    "        headers={'Authorization': f'Bearer {api_key}', 'Content-Type': 'application/json'},\n",
    "        json=data\n",
    "    )\n",
    "\n",
    "    # Check response\n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # Return full JSON response\n",
    "    else:\n",
    "        return f\"Error {response.status_code}: {response.text}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debugging\n",
    "paper_abstract = \"\"\"[Continuous integration at scale is costly but essential to software development. Various test optimization techniques including test selection and prioritization aim to reduce the cost. Test batching is an effective alternative, but overlooked technique. This study evaluates parallelization’s effect by adjusting machine count for test batching and introduces two novel approaches.\n",
    "We establish TestAll as a baseline to study the impact of parallelism and machine count on feedback time. We re-evaluate ConstantBatching and introduce DynamicBatching, which adapts batch size based on the remaining changes in the queue. We also propose TestCaseBatching, enabling new builds to join a batch before full test execution, thus speeding up continuous integration. Our evaluations utilize Ericsson’s results and 276 million test outcomes from open-source Chrome, assessing feedback time, execution reduction, and providing access to Chrome project scripts and data.\n",
    "The results reveal a non-linear impact of test parallelization on feedback time, as each test delay compounds across the entire test queue. ConstantBatching, with a batch size of 4, utilizes up to 72% fewer machines to maintain the actual average feedback time and provides a constant execution reduction of up to 75%. Similarly, DynamicBatching maintains the actual average feedback time with up to 91% fewer machines and exhibits variable execution reduction of up to 99%. TestCaseBatching holds the line of the actual average feedback time with up to 81% fewer machines and demonstrates variable execution reduction of up to 67%. We recommend practitioners use DynamicBatching and TestCaseBatching to reduce the required testing machines efficiently. Analyzing historical data to find the threshold where adding more machines has minimal impact on feedback time is also crucial for resource-effective testing.]\"\"\"\n",
    "\n",
    "try:\n",
    "    result = create_abstract(paper_abstract, SYSTEM_PROMPT)\n",
    "except Exception as e:\n",
    "    print(\"Exception at \" + \"paper\")\n",
    "\n",
    "result = create_abstract(paper_abstract, SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die effektive Gestaltung von Continuous Integration (CI) ist ein zentraler Aspekt der Softwareentwicklung, jedoch mit hohen Kosten verbunden. Testoptimierungsmethoden wie Testauswahl, -priorisierung und -bündelung zielen darauf ab, diese Kosten zu senken, insbesondere durch den gezielten Einsatz von Parallelisierung und Ressourcennutzung. Der vorliegende Beitrag untersucht maßgebliche Strategien zur Testbündelung und deren Auswirkungen auf die Feedbackzeiten sowie den Ressourceneinsatz in groß angelegten CI-Systemen.\n"
     ]
    }
   ],
   "source": [
    "#Debugging\n",
    "content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'Die effektive Gestaltung von Continuous Integration (CI) ist ein zentraler Aspekt der Softwareentwicklung, jedoch mit hohen Kosten verbunden. Testoptimierungsmethoden wie Testauswahl, -priorisierung und -bündelung zielen darauf ab, diese Kosten zu senken, insbesondere durch den gezielten Einsatz von Parallelisierung und Ressourcennutzung. Der vorliegende Beitrag untersucht maßgebliche Strategien zur Testbündelung und deren Auswirkungen auf die Feedbackzeiten sowie den Ressourceneinsatz in groß angelegten CI-Systemen.', 'function_call': None, 'refusal': None, 'role': 'assistant', 'tool_calls': None}}], 'created': 1739792957, 'id': 'chatcmpl-B1tufWDEToqHnQ2e2ol8Uwi6TuBVP', 'model': 'gpt-4o-2024-11-20', 'object': 'chat.completion', 'service_tier': 'default', 'system_fingerprint': 'fp_a82c03666f', 'usage': {'completion_tokens': 112, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens': 472, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'total_tokens': 584}}\n"
     ]
    }
   ],
   "source": [
    "# Debugging\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-Do: Finish and use this function after the testing and debugging is finished\n",
    "def get_introductory_text(papers_list):\n",
    "    \"\"\"\n",
    "    Iterates through papers_list, adds the key 'Introductory text' to each paper,\n",
    "    and populates it using the analyze_paper function.\n",
    "    \n",
    "    :param papers_list: List of dictionaries containing paper details.\n",
    "    \"\"\"\n",
    "    for paper in papers_list:\n",
    "        if 'Abstract' in paper:\n",
    "            try:\n",
    "                paper_abstract = paper['Abstract']\n",
    "                #result = create_abstract(paper['Abstract'], SYSTEM_PROMPT)\n",
    "                introductory_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "                paper['Introductory text'] = introductory_text\n",
    "            except Exception as e:\n",
    "                print(\"Exception at \" + paper)\n",
    "            \n",
    "    return papers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove comment to call the function and create introductory texts for each entry\n",
    "# get_introductory_text(papers_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the properties for the research questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_PROPERRTIES = \"\"\"Using the provided abstract and research question, identify the key properties measured to answer each research question. \n",
    "                            Examples of such properties include accuracy, usability, reliability, performance, portability, CPU usage, and runtime. \n",
    "                            List the relevant properties for each research question separately, ensuring they align with the details in the abstract.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_properies(paper_abstract, paper_rqs, SYSTEM_PROMPT_PROPERRTIES):\n",
    "    client = OpenAI(base_url=\"http://172.26.92.115\")\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"gpt-4o-2024-11-20\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT_PROPERRTIES},\n",
    "            {\"role\": \"user\", \"content\": paper_abstract},\n",
    "            {\"role\": \"user\", \"content\": paper_rqs}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    url = \"http://172.26.92.115/chat_completion\"\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "    # Send request\n",
    "    response = requests.post(\n",
    "        url, \n",
    "        headers={'Authorization': f'Bearer {api_key}', 'Content-Type': 'application/json'},\n",
    "        json=data\n",
    "    )\n",
    "\n",
    "    # Check response\n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # Return full JSON response\n",
    "    else:\n",
    "        return f\"Error {response.status_code}: {response.text}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-Do: Finish and use this function after the testing and debugging is finished\n",
    "def get_properties(papers_list):\n",
    "    \"\"\"\n",
    "    Iterates through papers_list, adds the key 'Properties' to each paper,\n",
    "    and populates it using the extract_properies function.\n",
    "    \n",
    "    :param papers_list: List of dictionaries containing paper details.\n",
    "    \"\"\"\n",
    "    for paper in papers_list:\n",
    "        if 'Abstract' and 'Research Questions' in paper:\n",
    "            try:\n",
    "                paper_abstract = paper['Abstract']\n",
    "                paper_rqs = paper['Research Questions']\n",
    "                # To-Do: Remove comment to extract properties for all papers\n",
    "                #result = extract_properies(paper_abstract, paper_rqs, SYSTEM_PROMPT_PROPERRTIES)\n",
    "                properties = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "                paper['Properties'] = properties\n",
    "            except Exception as e:\n",
    "                print(\"Exception at \" + paper)\n",
    "            \n",
    "    return papers_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save everything in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(data, filename):\n",
    "    \"\"\"\n",
    "    Writes a list of dictionaries to a CSV file inside the 'csv_files/papers_expert_study' folder.\n",
    "\n",
    "    Parameters:\n",
    "    data (list of dict): A list of dictionaries where each dictionary represents a row in the CSV file.\n",
    "    filename (str): The name of the CSV file to be created.\n",
    "\n",
    "    Returns:\n",
    "    None: The function writes the CSV file and prints a confirmation message upon success.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the 'csv_files' directory exists\n",
    "    os.makedirs('csv_files/papers_expert_study', exist_ok=True)  # Creates the folder if it doesn't exist\n",
    "    \n",
    "    # Construct the full file path by joining the folder name with the filename\n",
    "    file_path = os.path.join('csv_files/papers_expert_study', filename)\n",
    "    \n",
    "    # Get the fieldnames from the first dictionary in the list (assumes all dicts have the same keys)\n",
    "    fieldnames = data[0].keys()\n",
    "    \n",
    "    # Open the file in write mode, create a CSV DictWriter object\n",
    "    with open(file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        \n",
    "        # Write the header (fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Write the rows (data)\n",
    "        writer.writerows(data)\n",
    "    \n",
    "    print(f\"Data successfully written to {file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conference_scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
