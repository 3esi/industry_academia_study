{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the papers for the expert validation study\n",
    "\n",
    "Automatic creation of short introduction texts for the papers.\n",
    "\n",
    "## Add missing URLs\n",
    "\n",
    "Start by importing a file containing missing URLs for the papers. The missing URLs were manually added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_csv_to_list(folderpath, filename):\n",
    "    \"\"\"\n",
    "    Extracts data from a CSV file and returns a list of dictionaries.\n",
    "\n",
    "    Parameters:\n",
    "    folderpath (str): The path to the folder containing the CSV file.\n",
    "    filename (str): The name of the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    list[dict]: A list of dictionaries where each dictionary represents a row in the CSV file.\n",
    "                Each dictionary contains the keys:\n",
    "                - \"Paper Name\"\n",
    "                - \"Research Questions\"\n",
    "                - \"URL\"\n",
    "                - \"Abstract\" (if present in the CSV file)\n",
    "    \"\"\"\n",
    "    csv_file_path = os.path.join(folderpath, filename)\n",
    "    extracted_data = []\n",
    "    \n",
    "    with open(csv_file_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        for row in reader:\n",
    "            subset = {\n",
    "                \"Paper Name\": row[\"Paper Name\"],\n",
    "                \"Research Questions\": row[\"Research Questions (max. 4)\"],\n",
    "                \"URL\": row[\"URL\"]\n",
    "            }\n",
    "            # Check if \"Abstract\" column exists and add it\n",
    "            if \"Abstract\" in row:\n",
    "                subset[\"Abstract\"] = row[\"Abstract\"]\n",
    "            \n",
    "            extracted_data.append(subset)\n",
    "    \n",
    "    return extracted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(data, filename):\n",
    "    \"\"\"\n",
    "    Writes a list of dictionaries to a CSV file inside the 'csv_files' folder.\n",
    "\n",
    "    Parameters:\n",
    "    data (list of dict): A list of dictionaries where each dictionary represents a row in the CSV file.\n",
    "    filename (str): The name of the CSV file to be created.\n",
    "\n",
    "    Returns:\n",
    "    None: The function writes the CSV file and prints a confirmation message upon success.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the 'csv_files' directory exists\n",
    "    os.makedirs('csv_files', exist_ok=True)  # Creates the folder if it doesn't exist\n",
    "    \n",
    "    # Construct the full file path by joining the folder name with the filename\n",
    "    file_path = os.path.join('csv_files', filename)\n",
    "    \n",
    "    # Get the fieldnames from the first dictionary in the list (assumes all dicts have the same keys)\n",
    "    fieldnames = data[0].keys()\n",
    "    \n",
    "    # Open the file in write mode, create a CSV DictWriter object\n",
    "    with open(file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        \n",
    "        # Write the header (fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Write the rows (data)\n",
    "        writer.writerows(data)\n",
    "    \n",
    "    print(f\"Data successfully written to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_csv_to_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m folderpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../paper_selection/csv_files/papers_expert_study\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdated_url_papers_expert_val.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m sampled_papers_list \u001b[38;5;241m=\u001b[39m \u001b[43mextract_csv_to_list\u001b[49m(folderpath, csv_file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extract_csv_to_list' is not defined"
     ]
    }
   ],
   "source": [
    "folderpath = \"../paper_selection/csv_files/papers_expert_study\"\n",
    "csv_file = \"updated_url_papers_expert_val.csv\"\n",
    "sampled_papers_list = extract_csv_to_list(folderpath, csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abstracts(url):\n",
    "    \"\"\"\n",
    "    Scrapes abstracts from the given URL using predefined CSS selectors.\n",
    "\n",
    "    This function sends an HTTP request to the specified URL, parses the HTML content \n",
    "    using BeautifulSoup, and extracts the abstract text based on a list of possible \n",
    "    selectors. It attempts to handle different website structures, including ArXiv, \n",
    "    IEEE Xplore, and ACM Digital Library.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the research paper or article from which to extract the abstract.\n",
    "\n",
    "    Returns:\n",
    "    list[str]: A list of extracted abstracts (as strings). If no abstracts are found,\n",
    "               an empty list is returned.\n",
    "    \"\"\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}  # Avoid blocking by the server\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page, status code: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Adjust these selectors based on the website structure\n",
    "    possible_selectors = [\n",
    "       'section[role=\"doc-abstract\"] div[role=\"paragraph\"]', # for dl.acm.org\n",
    "        'div.abstract-text div.col-12 div.u-mb-1 div[xplmathjax]', #ieeexplore.ieee.org\n",
    "        'span.abstract.mathjax',\n",
    "        'div.abstract',\n",
    "        'p.abstract',\n",
    "        'section.abstract',\n",
    "        'span.abstract',\n",
    "        'blockquote.abstract.mathjax'  # for arxiv.org\n",
    "    ]\n",
    "    \n",
    "    abstracts = []\n",
    "    for selector in possible_selectors:\n",
    "        elements = soup.select(selector)\n",
    "        for elem in elements:\n",
    "            # Find the text content inside the blockquote and remove the descriptor span\n",
    "            if selector == 'blockquote.abstract.mathjax':\n",
    "                abstract_text = \" \".join([text.strip() for text in elem.stripped_strings if text != \"Abstract:\"])\n",
    "                abstracts.append(abstract_text)\n",
    "            else:\n",
    "                abstracts.append(elem.get_text(strip=True))\n",
    "\n",
    "    if not abstracts:\n",
    "        print(\"Could not scrape abstract\")\n",
    "    \n",
    "    return abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_abstracts_to_papers(sampled_papers_list):\n",
    "    \"\"\"Adds abstracts to the list of papers.\"\"\"\n",
    "    for paper in sampled_papers_list:\n",
    "        url = paper[\"URL\"]\n",
    "        abstract = get_abstracts(url)  # Get the abstract using the URL\n",
    "        paper[\"Abstract\"] = abstract  # Add the \"Abstract\" to the dictionary\n",
    "    \n",
    "    return sampled_papers_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape the abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch page, status code: 418\n",
      "Could not scrape abstract\n",
      "Could not scrape abstract\n",
      "Failed to fetch page, status code: 418\n",
      "Failed to fetch page, status code: 418\n",
      "Failed to fetch page, status code: 418\n",
      "Failed to fetch page, status code: 418\n",
      "Could not scrape abstract\n",
      "Failed to fetch page, status code: 418\n",
      "Failed to fetch page, status code: 418\n",
      "Failed to fetch page, status code: 418\n",
      "Failed to fetch page, status code: 418\n",
      "Could not scrape abstract\n",
      "Failed to fetch page, status code: 418\n",
      "Could not scrape abstract\n",
      "Failed to fetch page, status code: 418\n",
      "Could not scrape abstract\n",
      "Could not scrape abstract\n",
      "Could not scrape abstract\n",
      "Could not scrape abstract\n",
      "Could not scrape abstract\n",
      "Could not scrape abstract\n",
      "Paper Name: Towards Understanding Fairness and its Composition in Ensemble Machine Learning\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Research Questions (max. 4)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m paper \u001b[38;5;129;01min\u001b[39;00m updated_papers_list:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPaper Name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpaper[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPaper Name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResearch Questions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mpaper\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mResearch Questions (max. 4)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpaper[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbstract: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpaper[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbstract\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Research Questions (max. 4)'"
     ]
    }
   ],
   "source": [
    "updated_papers_list = add_abstracts_to_papers(sampled_papers_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an arfitfact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to csv_files/papers_expert_study/abstracts_papers_expert_val.csv\n"
     ]
    }
   ],
   "source": [
    "filename = 'abstracts_papers_expert_val.csv'\n",
    "write_to_csv(updated_papers_list, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create introductory texts for each paper\n",
    "\n",
    "First import a completed file containing the missing abstracts from a file that was manually completed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(result):\n",
    "    as_dict = json.loads(result)\n",
    "    return as_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_intro_text(paper_abstract, SYSTEM_PROMPT_GER):\n",
    "    client = OpenAI(base_url=\"http://172.26.92.115\")\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"gpt-4o-2024-11-20\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT_GER},\n",
    "            {\"role\": \"user\", \"content\": paper_abstract}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    url = \"http://172.26.92.115/chat_completion\"\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "    # Send request\n",
    "    response = requests.post(\n",
    "        url, \n",
    "        headers={'Authorization': f'Bearer {api_key}', 'Content-Type': 'application/json'},\n",
    "        json=data\n",
    "    )\n",
    "\n",
    "    # Check response\n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # Return full JSON response\n",
    "    else:\n",
    "        return f\"Error {response.status_code}: {response.text}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath = \"csv_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"completed_abstracts_expert_val.csv\"\n",
    "\n",
    "papers_list = extract_csv_to_list(folderpath, csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the request to the OpenAI API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debugging: Create intro text for a single paper\n",
    "paper_abstract = \"\"\"In software engineering, interruptions during tasks can have significant implications for productivity and well-being. While previous studies have investigated the effect of interruptions on productivity, to the best of our knowledge, no prior work has yet distinguished the effectofdifferenttypesofinterruptions onsoftwareengineering activities. This study explores the impact of interruptions on software engineering tasks, analyzing in-person and on-screen interruptions with different levels of urgency and dominance. Participants completed code writing, code comprehension, and code review tasks while experiencing interruptions. We collect physiological data using the Empatica EmbracePlus wristband and self-perceived evaluations through surveys. Results show that on-screen interruptions with high dominance of requester significantly increase time spent on code comprehension. In-person and on-screen interruptions combined significantly affect the time spent on code review, with varied effects based on specific interruption combinations. Both interruption type and task significantly influence stress measures, with code comprehension and review tasks associated with lower stress measures compared to code writing. Interestingly, in-person interruptions present a positive impact on physiological measures, indicating reduced stress measures. However, participants’ selfperceived stress scores do not align with physiological data, with higher stress reported during in-person interruptions despite lower physiological stress measures. These findings shed light on and emphasize the potential importance of considering the complex relationship between interruptions, objective measures, and subjective experiences in software development. We discuss insights that we hope can inform interruption management and implications on stress among software engineers.\"\"\"\n",
    "\n",
    "try:\n",
    "    result = create_intro_text(paper_abstract, SYSTEM_PROMPT_GER_VER2)\n",
    "except Exception as e:\n",
    "    print(\"Exception at \" + \"paper\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unterbrechungen während der Arbeit können in der Softwareentwicklung erhebliche Auswirkungen auf Produktivität und Wohlbefinden haben. Diese Arbeit untersucht den Einfluss verschiedener Arten von Unterbrechungen auf softwarebezogene Aufgaben wie Codierung, Codeverständnis und Codeüberprüfung. Dabei werden physiologische Daten und subjektive Wahrnehmungen genutzt, um die komplexen Beziehungen zwischen Unterbrechungstypen, Aufgabenanforderungen und Stressindikatoren zu analysieren.\n"
     ]
    }
   ],
   "source": [
    "#Debugging: Create intro text for a single paper\n",
    "content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create intro texts in german"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_introductory_text(papers_list):\n",
    "    \"\"\"\n",
    "    Iterates through papers_list, adds the key 'Introductory text' to each paper,\n",
    "    and populates it using the create_intro_text function.\n",
    "    \n",
    "    :param papers_list: List of dictionaries containing paper details.\n",
    "    \"\"\"\n",
    "    for paper in papers_list:\n",
    "        if 'Abstract' in paper:\n",
    "            try:\n",
    "                paper_abstract = paper['Abstract']\n",
    "                result = create_intro_text(paper_abstract, SYSTEM_PROMPT_GER)\n",
    "                introductory_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "                paper['Introductory text'] = introductory_text\n",
    "            except Exception as e:\n",
    "                print(\"Exception at \" + paper)\n",
    "            \n",
    "    return papers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_GER = \"\"\"Write a concise introductory text in German for a paper based on the following abstract. \n",
    "                The text should provide a brief overview of the main themes and context of the paper without delving into specific methods, \n",
    "                results, or contributions. It should be written in an impersonal, third-person perspective (avoid using first-person plural like \n",
    "                'we' or 'our'). The tone should remain formal and academic. The introduction should be no longer than 2-3 sentences and should \n",
    "                strictly avoid mentioning the paper's contributions, findings, or implications. Focus solely on the broader subject matter and \n",
    "                relevance of the research field.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this prompt for results that are written in english instead of german\n",
    "SYSTEM_PROMPT_GER_VER2 = \"\"\"Write a concise introductory text in German for a paper based on the following abstract. \n",
    "                The text should provide a brief overview of the main themes and context of the paper without delving into specific methods, \n",
    "                results, or contributions. It should be written in an impersonal, third-person perspective (avoid using first-person plural like \n",
    "                'we' or 'our'). The tone should remain formal and academic. The introduction should be no longer than 2-3 sentences and should \n",
    "                strictly avoid mentioning the paper's contributions, findings, or implications. Focus solely on the broader subject matter and \n",
    "                relevance of the research field. The resulting text should be in german.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an introductory text for each entry\n",
    "list_intro_texts = get_introductory_text(papers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to csv_files/papers_with_intro_texts_german.csv\n"
     ]
    }
   ],
   "source": [
    "filename = \"papers_with_intro_texts_german.csv\"\n",
    "write_to_csv(list_intro_texts, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create intro texts in english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_ENG = \"\"\"Write a concise introductory text for a paper based on the following abstract. \n",
    "                The text should provide a brief overview of the main themes and context of the paper without delving into specific methods, \n",
    "                results, or contributions. It should be written in an impersonal, third-person perspective (avoid using first-person plural like \n",
    "                'we' or 'our'). The tone should remain formal and academic. The introduction should be no longer than 2-3 sentences and should \n",
    "                strictly avoid mentioning the paper's contributions, findings, or implications. Focus solely on the broader subject matter and \n",
    "                relevance of the research field.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_introductory_text_eng(papers_list):\n",
    "    \"\"\"\n",
    "    Iterates through papers_list, adds the key 'Introductory text' to each paper,\n",
    "    and populates it using the create_intro_text function.\n",
    "    \n",
    "    :param papers_list: List of dictionaries containing paper details.\n",
    "    \"\"\"\n",
    "    for paper in papers_list:\n",
    "        if 'Abstract' in paper:\n",
    "            try:\n",
    "                paper_abstract = paper['Abstract']\n",
    "                result = create_intro_text(paper_abstract, SYSTEM_PROMPT_ENG)\n",
    "                introductory_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "                paper['Introductory text'] = introductory_text\n",
    "            except Exception as e:\n",
    "                print(\"Exception at \" + paper)\n",
    "            \n",
    "    return papers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_intro_texts_eng = get_introductory_text_eng(papers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to csv_files/papers_with_intro_texts_english.csv\n"
     ]
    }
   ],
   "source": [
    "filename = \"papers_with_intro_texts_english.csv\"\n",
    "write_to_csv(list_intro_texts_eng, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debugging: Create intro text for a single paper\n",
    "paper_abstract = \"\"\"In software engineering, interruptions during tasks can have significant implications for productivity and well-being. While previous studies have investigated the effect of interruptions on productivity, to the best of our knowledge, no prior work has yet distinguished the effectofdifferenttypesofinterruptions onsoftwareengineering activities. This study explores the impact of interruptions on software engineering tasks, analyzing in-person and on-screen interruptions with different levels of urgency and dominance. Participants completed code writing, code comprehension, and code review tasks while experiencing interruptions. We collect physiological data using the Empatica EmbracePlus wristband and self-perceived evaluations through surveys. Results show that on-screen interruptions with high dominance of requester significantly increase time spent on code comprehension. In-person and on-screen interruptions combined significantly affect the time spent on code review, with varied effects based on specific interruption combinations. Both interruption type and task significantly influence stress measures, with code comprehension and review tasks associated with lower stress measures compared to code writing. Interestingly, in-person interruptions present a positive impact on physiological measures, indicating reduced stress measures. However, participants’ selfperceived stress scores do not align with physiological data, with higher stress reported during in-person interruptions despite lower physiological stress measures. These findings shed light on and emphasize the potential importance of considering the complex relationship between interruptions, objective measures, and subjective experiences in software development. We discuss insights that we hope can inform interruption management and implications on stress among software engineers.\"\"\"\n",
    "\n",
    "try:\n",
    "    result = create_intro_text(paper_abstract, SYSTEM_PROMPT_ENG)\n",
    "except Exception as e:\n",
    "    print(\"Exception at \" + \"paper\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interruptions are a pervasive aspect of modern work environments, with significant implications for productivity and well-being. In the context of software engineering, where tasks often demand high levels of concentration and cognitive effort, the impact of interruptions remains a critical area of study. This research examines the effects of different types of interruptions on software engineering activities, focusing on their influence across various tasks and physiological and self-perceived stress measures.\n"
     ]
    }
   ],
   "source": [
    "#Debugging: Create intro text for a single paper\n",
    "content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the properties for the research questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_PROPERRTIES = \"\"\"Using the provided abstract and research questions, identify the key properties measured to address each research \n",
    "                            question. These properties may include, but are not limited to, accuracy, usability, reliability, performance, \n",
    "                            portability, CPU usage, and runtime.\n",
    "                            For each research question, list the relevant properties without restating the question. Organize your answers by \n",
    "                            the research question number (e.g., RQ1, RQ2, etc.), ensuring that each property corresponds directly to the \n",
    "                            information in the abstract.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_properies(paper_abstract, paper_rqs, SYSTEM_PROMPT_PROPERRTIES):\n",
    "    client = OpenAI(base_url=\"http://172.26.92.115\")\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"gpt-4o-2024-11-20\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT_PROPERRTIES},\n",
    "            {\"role\": \"user\", \"content\": paper_abstract},\n",
    "            {\"role\": \"user\", \"content\": paper_rqs}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    url = \"http://172.26.92.115/chat_completion\"\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "    # Send request\n",
    "    response = requests.post(\n",
    "        url, \n",
    "        headers={'Authorization': f'Bearer {api_key}', 'Content-Type': 'application/json'},\n",
    "        json=data\n",
    "    )\n",
    "\n",
    "    # Check response\n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # Return full JSON response\n",
    "    else:\n",
    "        return f\"Error {response.status_code}: {response.text}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging\n",
    "paper_abstract = \"\"\"['Recently, deep learning models have been widely applied in program understanding tasks, and these models achieve state-of-the-art results on many benchmark datasets. A major challenge of deep learning for program understanding is that the effectiveness of these approaches depends on the quality of their datasets, and these datasets often contain noisy data samples. A typical kind of noise in program understanding datasets is label noise, which means that the target outputs for some inputs are incorrect. Researchers have proposed various approaches to alleviate the negative impact of noisy labels, and formed a new research topic: noisy label learning (NLL). In this paper, we conduct an empirical study on the effectiveness of noisy label learning on deep learning for program understanding datasets. We evaluate various NLL approaches and deep learning models on three tasks: program classification, vulnerability detection, and code summarization. From the evaluation results, we come to the following findings: 1) small trained-from-scratch models are prone to label noises in program understanding, while large pre-trained models are highly robust against them. 2) NLL approaches significantly improve the program classification accuracies for small models on noisy training sets, but they only slightly benefit large pre-trained models in classification accuracies. 3) NLL can effectively detect synthetic noises in program understanding, but struggle in detecting real-world noises. We believe our findings can provide insights on the abilities of NLL in program understanding, and shed light on future works in tackling noises in software engineering datasets. We have released our code at this https URL .']\"\"\"\n",
    "paper_rqs = \"\"\"RQ1:Howdodifferenttypes of synthetic label noises in program classification affect the performance of deep learning models when NLL is not introduced? • \n",
    "            RQ2:Howdoexisting NLL approaches perform on different synthetic noises in program classification? \n",
    "            • RQ3:HowdoNLLapproaches perform on program understanding tasks with real-world noises?\"\"\"\n",
    "\n",
    "try:\n",
    "    result = extract_properies(paper_abstract, paper_rqs, SYSTEM_PROMPT_PROPERRTIES)\n",
    "except Exception as e:\n",
    "    print(\"Exception at \" + \"paper\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the key properties measured to answer each research question based on the abstract:\n",
      "\n",
      "### **RQ1: How do different types of synthetic label noises in program classification affect the performance of deep learning models when NLL is not introduced?**\n",
      "- **Performance**: The effectiveness of the deep learning models in program classification tasks is analyzed under different synthetic label noise conditions.\n",
      "- **Accuracy**: The classification accuracy of the deep learning models is measured to determine the impact of synthetic label noise.\n",
      "- **Reliability**: The robustness of deep learning models against noise is considered, particularly in the absence of NLL.\n",
      "\n",
      "---\n",
      "\n",
      "### **RQ2: How do existing NLL approaches perform on different synthetic noises in program classification?**\n",
      "- **Accuracy**: Improvements in classification accuracy when NLL approaches are applied to handle synthetic label noises.\n",
      "- **Effectiveness**: The overall capability of NLL methods to mitigate the adverse effects of synthetic label noises on program classification tasks.\n",
      "\n",
      "---\n",
      "\n",
      "### **RQ3: How do NLL approaches perform on program understanding tasks with real-world noises?**\n",
      "- **Usability**: The practicality and effectiveness of NLL approaches in handling real-world noises.\n",
      "- **Performance**: The success of NLL approaches across multiple program understanding tasks (e.g., program classification, vulnerability detection, and code summarization).\n",
      "- **Noise Detection Capability**: The ability of NLL approaches to detect and handle real-world label noise effectively.\n",
      "- **Accuracy**: The measurable improvement (or lack thereof) in task outcomes when using NLL approaches on datasets with real-world noises.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Debugging\n",
    "content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-Do: Finish and use this function after the testing and debugging is finished\n",
    "def get_properties(papers_list):\n",
    "    \"\"\"\n",
    "    Iterates through papers_list, adds the key 'Properties' to each paper,\n",
    "    and populates it using the extract_properies function.\n",
    "    \n",
    "    :param papers_list: List of dictionaries containing paper details.\n",
    "    \"\"\"\n",
    "    for paper in papers_list:\n",
    "        if 'Abstract' and 'Research Questions' in paper:\n",
    "            try:\n",
    "                paper_abstract = paper['Abstract']\n",
    "                paper_rqs = paper['Research Questions']\n",
    "                # To-Do: Remove comment to extract properties for all papers\n",
    "                #result = extract_properies(paper_abstract, paper_rqs, SYSTEM_PROMPT_PROPERRTIES)\n",
    "                properties = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "                paper['Properties'] = properties\n",
    "            except Exception as e:\n",
    "                print(\"Exception at \" + paper)\n",
    "            \n",
    "    return papers_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save everything in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-Do: Reuse the old function write_to_csv instead of writing a new function??\n",
    "def write_to_csv(data, filename):\n",
    "    \"\"\"\n",
    "    Writes a list of dictionaries to a CSV file inside the 'csv_files/papers_expert_study' folder.\n",
    "\n",
    "    Parameters:\n",
    "    data (list of dict): A list of dictionaries where each dictionary represents a row in the CSV file.\n",
    "    filename (str): The name of the CSV file to be created.\n",
    "\n",
    "    Returns:\n",
    "    None: The function writes the CSV file and prints a confirmation message upon success.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the 'csv_files' directory exists\n",
    "    os.makedirs('csv_files/papers_expert_study', exist_ok=True)  # Creates the folder if it doesn't exist\n",
    "    \n",
    "    # Construct the full file path by joining the folder name with the filename\n",
    "    file_path = os.path.join('csv_files/papers_expert_study', filename)\n",
    "    \n",
    "    # Get the fieldnames from the first dictionary in the list (assumes all dicts have the same keys)\n",
    "    fieldnames = data[0].keys()\n",
    "    \n",
    "    # Open the file in write mode, create a CSV DictWriter object\n",
    "    with open(file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        \n",
    "        # Write the header (fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Write the rows (data)\n",
    "        writer.writerows(data)\n",
    "    \n",
    "    print(f\"Data successfully written to {file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conference_scraper (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
